{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR0o63R4Xa6F"
   },
   "source": [
    "# Step 1: Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kpv4u1hZX2Vv"
   },
   "source": [
    "# Step 2: Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riwFQZMNtU_4"
   },
   "source": [
    "Libraries used for the Bayesian Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4mw25s5cYPuv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import botorch\n",
    "from botorch.utils.transforms import standardize, normalize, unnormalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSAValUHtY_h"
   },
   "source": [
    "Libraries used to save checkpoints in GDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xdYx-gMYyeV"
   },
   "source": [
    "# Step 3: Define objetive function\n",
    "This will be the lower bound of the mean reward of a trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XNRXK99lY_T9"
   },
   "outputs": [],
   "source": [
    "def get_hyp_values(hyperparams_tensor):\n",
    "  '''\n",
    "  Returns a tuple of values from a tensor containing a hyperparameter configuration\n",
    "\n",
    "          Parameters:\n",
    "                  hyperparams_tensor (torch.DoubleTensor): A tensor of size 1xn (1 row, n columns) with n being the number of hyperparameters to tune\n",
    "          \n",
    "          Returns:\n",
    "                  hyperparams_tuple (tuple): A tuple with the unpacked values of the hyperparams_tensor \n",
    "\n",
    "  '''\n",
    "  hyperparams_list = [hyperparams_tensor[i].item() for i in range(len(hyperparams_tensor))]\n",
    "  hyperparams_tuple = tuple(hyperparams_list)\n",
    "  return tuple(hyperparams_list)\n",
    "\n",
    "\n",
    "def create_model(hyperparams,\n",
    "                 policy='MlpPolicy',\n",
    "                 env_name='LunarLander-v2'):\n",
    "  '''\n",
    "  Returns a PPO model given a policy, environment, and hyperparameters of PPO\n",
    "\n",
    "          Parameters:\n",
    "                  hyperparams (torch.DoubleTensor): A tensor of size 1x2 (1 row, n columns) with the learning rate and gamma to train the model with\n",
    "                  policy (str): The NN to train with PPO in the environment. Default is 'MlpPolicy'\n",
    "                  env (stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv): Specifies the gym environment to use for the training\n",
    "\n",
    "          Returns:\n",
    "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
    "  '''\n",
    "  lr, gamma  = get_hyp_values(hyperparams)\n",
    "  env = make_vec_env(env_name, n_envs=1)\n",
    "  model = PPO(policy = policy,\n",
    "              env = env,\n",
    "              learning_rate = lr,\n",
    "              n_steps = 1024,\n",
    "              batch_size = 64,\n",
    "              n_epochs = 4,\n",
    "              gamma = gamma,\n",
    "              gae_lambda = 0.98,\n",
    "              ent_coef = 0.01,\n",
    "              verbose=0)\n",
    "  \n",
    "  return model\n",
    "\n",
    "\n",
    "def train_model(model, timesteps=1000000):\n",
    "  '''\n",
    "  Trains a PPO model during a number of timesteps\n",
    "          \n",
    "          Parameters:\n",
    "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
    "                  timesteps (int): The number of timesteps used to train the model\n",
    "\n",
    "          Returns:\n",
    "                  None\n",
    "  '''\n",
    "  model.learn(total_timesteps=timesteps)\n",
    "  return\n",
    "\n",
    "\n",
    "def evaluate_model(model, \n",
    "                   rl_env_name='LunarLander-v2', \n",
    "                   n_eval_episodes=25):\n",
    "  '''\n",
    "  Evaluates the model for a number of episodes in a specified environment, this environment MUST be the same as the one the model has been trained in.\n",
    "\n",
    "          Parameters:\n",
    "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
    "                  rl_env_name (str): The name of the gym environment where the model has been trained\n",
    "                  n_eval_episodes (int): The number of episodes for which the model will be evaluated to obtain a mean and standard deviation\n",
    "\n",
    "          Returns:\n",
    "                  lower_mean_reward (float): A tensor of size 1x1 (1 row, 1 column) containing the mean_reward\n",
    "  '''\n",
    "  eval_env = gym.make(rl_env_name)\n",
    "  mean_reward, std_reward = evaluate_policy(model, \n",
    "                                            eval_env, \n",
    "                                            n_eval_episodes=n_eval_episodes, \n",
    "                                            deterministic=True)\n",
    "  \n",
    "  print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n",
    "  lower_mean_reward = mean_reward - std_reward\n",
    "  return lower_mean_reward\n",
    "\n",
    "\n",
    "def target_function(hyperparams, \n",
    "                    timesteps=1000000,\n",
    "                    rl_env_name='LunarLander-v2'):\n",
    "  '''\n",
    "  Given a hyperparameter configuration, evaluates their performance\n",
    "          Parameters:\n",
    "                  hyperparams (torch.DoubleTensor): A tensor of size 1x2 (1 row, n columns) with the learning rate and gamma to train the model with\n",
    "                  timesteps (int): timesteps (int): The number of timesteps used to train the model\n",
    "                  rl_env_name (str): The name of the gym environment where the model has been trained\n",
    "\n",
    "          Returns:\n",
    "                  lower_mean_reward (float): A tensor of size 1x1 (1 row, 1 column) containing the mean_reward\n",
    "  '''\n",
    "  model = create_model(hyperparams, env_name=rl_env_name)\n",
    "  \n",
    "  train_model(model, \n",
    "              timesteps)\n",
    "  \n",
    "  lower_mean_reward = evaluate_model(model, \n",
    "                                     rl_env_name)\n",
    "  \n",
    "  return lower_mean_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjdeNdB_jJPj"
   },
   "source": [
    "# Step 4: Define hyperparameters to tune\n",
    "First define the bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WXwDQp5oilS4"
   },
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "gamma = 0.85\n",
    "# Define here the list of parameters to tune\n",
    "hyperparams_list = [lr, gamma]\n",
    "# Define the lower bounds of the parameters\n",
    "lower_bounds = [0.0001, 0.8]\n",
    "# Define the upper bounds of the parameters\n",
    "upper_bounds = [0.1, 0.9997]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ILw3cqAjkbg"
   },
   "source": [
    "Then convert lists to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PnU1DMG8jnuc"
   },
   "outputs": [],
   "source": [
    "# Create tensors with the hyperparameters configurations and bounds for BOTorch to use\n",
    "hyperparams_tensor = torch.DoubleTensor([hyperparams_list])\n",
    "bounds_tensor = torch.DoubleTensor([lower_bounds, upper_bounds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igatCqADkIjz"
   },
   "source": [
    "# Step 5: Define functions needed for the Bayesian Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VYL5gMk_kK7m"
   },
   "outputs": [],
   "source": [
    "def generate_initial_data(bounds, \n",
    "                          n=3):\n",
    "  '''\n",
    "  Gets n values of the hyperparameter's bounded space and evaluates them\n",
    "          Parameters:\n",
    "                bounds (torch.DoubleTensor): The torch tensor containing the upper and lower bounds of the hyperparameters (lr and gamma in this case)\n",
    "                n (int): The number of initial points to get. Default is 3\n",
    "          \n",
    "          Returns:\n",
    "                train_x (torch.DoubleTensor): A tensor of size (n, 1) (n rows and 1 column) with the initial points\n",
    "                exact_obj (torch.DoubleTensor): A tensor of size (n, 1) (n rows and 1 column) containing the evaluation of the model with the sampled hyperparameters values\n",
    "                best_observed_vale: The best evaluation of the hyperparameters\n",
    "  '''\n",
    "  # Create our initial hyperparameter values\n",
    "  lower_bounds = bounds[0]\n",
    "  upper_bounds = bounds[1]\n",
    "  train_x = torch.rand(n, len(lower_bounds), dtype=torch.double) * (upper_bounds - lower_bounds) + lower_bounds\n",
    "\n",
    "  # Evaluate them and store them in a torch.Tensor\n",
    "  exact_obj = torch.tensor([[target_function(hyp)] for hyp in train_x])\n",
    "\n",
    "  # Get the best observed value\n",
    "  best_observed_value = exact_obj.max().item()\n",
    "  \n",
    "  return train_x, exact_obj, best_observed_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1MUKzEZkmAxw"
   },
   "outputs": [],
   "source": [
    "from botorch.acquisition.analytic import ExpectedImprovement, UpperConfidenceBound\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.utils.transforms import standardize, normalize, unnormalize\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "from botorch import fit_gpytorch_model\n",
    "  \n",
    "\n",
    "def get_next_points(init_x,\n",
    "                    init_y,\n",
    "                    best_init_y,\n",
    "                    normalized_bounds,\n",
    "                    n_points=1):\n",
    "  '''\n",
    "  Function that computes the next point to add to the Gaussian Process and visualizes the acquisition function and function distribution\n",
    "          Parameters:\n",
    "                  init_x (torch.Tensor): A tensor of shape {iterations}x2 containing the previous hyperparameters\n",
    "                  init_y (torch.Tensor): A tensor of shape {iterations}x1 containing the previous rewards of the models trained with the init_x hyperparameters values\n",
    "                  best_init_y (float): Best reward obtained until the moment\n",
    "                  normalized_bounds (torch.Tensor): Normalized bounds of the hyperparameter values in the form of tensors of shape 2x1 (2 rows, 1 column), first row containing lower bound, second containing upper bound\n",
    "                  n_points (int): Number of candidates to obtain for the next iteration. Default is 1\n",
    "\n",
    "          Returns:\n",
    "                  candidates (torch.Tensor): A tensor of shape 1x2 containing the value of the hyperparameters that optimizes the acquisition function\n",
    "  '''\n",
    "  # Create our probabilistic model with the points\n",
    "  single_model = SingleTaskGP(init_x, init_y)\n",
    "  mll = ExactMarginalLogLikelihood(single_model.likelihood, \n",
    "                                   single_model)\n",
    "  # Fit our model\n",
    "  fit_gpytorch_model(mll)\n",
    "\n",
    "  # Instantiate the acquisition function given our model\n",
    "  UCB = UpperConfidenceBound(model=single_model,\n",
    "                             beta=0.2,\n",
    "                             maximize=True)\n",
    "  \n",
    "  # Maximize the acquisition function to obtain our candidates \n",
    "  candidates, _ = optimize_acqf(acq_function=UCB, \n",
    "                                bounds=normalized_bounds,\n",
    "                                q=n_points, num_restarts=200,\n",
    "                                raw_samples=512,\n",
    "                                options={\"batch_limit\": 5, \"maxiter\": 200})\n",
    "\n",
    "  return candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E038wSN_MFoQ"
   },
   "source": [
    "# Step 6: Set experiments' configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWEEF07XuQPj"
   },
   "source": [
    "First let us define functions to save and load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8sde3AjauPiS"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def create_experiment_df():\n",
    "  '''\n",
    "  Creates an empty dataframe to save checkpoints\n",
    "          Parameters:\n",
    "\n",
    "          Returns:\n",
    "                  experiment_df (pandas.DataFrame): An empty Dataframe with columns specified below that will be used to save the experiment history\n",
    "  '''\n",
    "  columns = [\"method\",\n",
    "             \"experiment\",\n",
    "             \"iteration\",\n",
    "             \"learning_rate\",\n",
    "             \"gamma\",\n",
    "             \"reward_lower_bound\",\n",
    "             \"best_learning_rate\",\n",
    "             \"best_gamma\",\n",
    "             \"best_reward_lower_bound\"\n",
    "             ]\n",
    "\n",
    "  experiment_df = pd.DataFrame(columns=columns)\n",
    "  return experiment_df\n",
    "\n",
    "\n",
    "def get_filepath(experiment_name):\n",
    "  '''\n",
    "  Returns the path of the csv of the experiment specified.\n",
    "          Parameters:\n",
    "                  experiment_name (string): The name of the experiment\n",
    "\n",
    "          Returns:\n",
    "                  filepath (string): The path to the .csv file that has the data of the experiment\n",
    "  '''\n",
    "  filepath = f\"./experiments_results/{experiment_name}.csv\"\n",
    "  return filepath\n",
    "\n",
    "\n",
    "def update_experiment_history(method, \n",
    "                              experiment_number, \n",
    "                              iteration,\n",
    "                              lr,\n",
    "                              gamma,\n",
    "                              reward_lower_bound,\n",
    "                              best_lr,\n",
    "                              best_gamma,\n",
    "                              best_reward_lower_bound,\n",
    "                              experiment_df,\n",
    "                              experiment_name):\n",
    "  '''\n",
    "  Updates the experiment dataframe and saves it in GDrive\n",
    "          Parameters:\n",
    "                  method (int): 0 if Bayesian Optimization, 1 if Random Search\n",
    "                  experiment_number (int): The id of the experiment\n",
    "                  iteration (int): The current iteration of the experiment\n",
    "                  lr (float): The learning rate value selected for this iteration\n",
    "                  gamma (float): The gamma value selected for this iteration\n",
    "                  reward_lower_bound (float): The reward's lower bound obtained by the model trained this iteration\n",
    "                  best_lr (float): The learning rate value that has induced the best reward lower bound\n",
    "                  best_gamma (float): The gamma value that has induced the best reward lower bound\n",
    "                  best_reward_lower_bound (float): The best reward lower bound obtained until this iteration\n",
    "                  experiment_df (pandas.DataFrame): The dataframe containing the experiments history\n",
    "                  experiment_name (string): The name of the experiment\n",
    "\n",
    "          Returns:\n",
    "                  concatenated_df (pandas.DataFrame): The updated experiments history dataframe\n",
    "  '''\n",
    "  # Define the columns of the dataframe\n",
    "  columns = [\"method\",\n",
    "             \"experiment\",\n",
    "             \"iteration\",\n",
    "             \"learning_rate\",\n",
    "             \"gamma\",\n",
    "             \"reward_lower_bound\",\n",
    "             \"best_learning_rate\",\n",
    "             \"best_gamma\",\n",
    "             \"best_reward_lower_bound\"]\n",
    "\n",
    "  # Set the values of the columns given by the iteration configuration and result\n",
    "  iteration_results = [[method,\n",
    "                        experiment_number,\n",
    "                        iteration,\n",
    "                        lr,\n",
    "                        gamma,\n",
    "                        reward_lower_bound,\n",
    "                        best_lr,\n",
    "                        best_gamma,\n",
    "                        best_reward_lower_bound]]\n",
    "\n",
    "  # Create a one row dataframe for this experiment\n",
    "  new_iteration_df = pd.DataFrame(iteration_results, columns=columns)\n",
    "\n",
    "  # Concatenate the experiments history with this experiment's results\n",
    "  concatenated_df = pd.concat([experiment_df, new_iteration_df], ignore_index=True)\n",
    "\n",
    "  # Save the updated history in google drive\n",
    "  save_checkpoint(concatenated_df,\n",
    "                  experiment_name)\n",
    "  \n",
    "  # Return the concatenated dataframe representing the updated experiment history\n",
    "  return concatenated_df\n",
    "\n",
    "\n",
    "def save_checkpoint(experiment_df,\n",
    "                    experiment_name):\n",
    "  '''\n",
    "  Saves the experiments history dataframe in google drive\n",
    "          Parameters:\n",
    "                  experiment_df (pandas.DataFrame): The dataframe containing the experiments history\n",
    "                  experiment_name (string): The name of the experiment\n",
    "          \n",
    "          Returns:\n",
    "                  None\n",
    "  '''\n",
    "\n",
    "  filepath = get_filepath(experiment_name)\n",
    "  experiment_df.to_csv(filepath, index=False)\n",
    "\n",
    "\n",
    "def load_checkpoint(experiment_name,\n",
    "                    experiment_results,\n",
    "                    experiment_configurations):\n",
    "  '''\n",
    "  Loads a checkpoint of an experiment given its name\n",
    "\n",
    "          Parameters:\n",
    "                  experiment_name (string): The name of the experiment\n",
    "                  experiment_results (numpy.array): A numpy array of three dimensions (method, iteration, best_result)\n",
    "                  expeirment_configuration (numpy.array): A numpy array of three dimensions (method, iteration, best_learning rate)\n",
    "\n",
    "          Returns:\n",
    "                  experiment_df (pandas.DataFrame): A dataframe with the experiment history\n",
    "  '''\n",
    "  # First we retrieve the dataframe from GDrive\n",
    "  filepath = get_filepath(experiment_name)\n",
    "  experiment_df = pd.read_csv(filepath)\n",
    "\n",
    "  # Now we iterate through the rows of the dataframe to update the experiment history numpy arrays that we will use later to compare the methods and plot results\n",
    "  for index, row in experiment_df.iterrows():\n",
    "      # Unpack the columns\n",
    "      method, exp, iter, lr, gamma, rlb, best_lr, best_gamma, best_rlb = row.values\n",
    "      # Add them to the experiments arrays\n",
    "      experiment_results[int(method)][int(exp)][int(iter)] = best_rlb\n",
    "      experiment_configurations[int(method)][int(exp)][int(iter)][0] = best_lr\n",
    "      experiment_configurations[int(method)][int(exp)][int(iter)][1] = best_gamma\n",
    "  \n",
    "\n",
    "  method, exp, iter, lr, gamma, rlb, best_lr, best_gamma, best_rlb = experiment_df.iloc[-1]\n",
    "\n",
    "  if method == 0:\n",
    "    bo_done = False\n",
    "    last_bo_experiment = int(exp)\n",
    "    last_rs_experiment = 0\n",
    "\n",
    "    # Plus one because we want to start in the next one\n",
    "    last_bo_iteration = int(iter)+1\n",
    "    last_rs_iteration = 1\n",
    "\n",
    "  else:\n",
    "    bo_done = True\n",
    "    last_bo_experiment = experiment_configurations.shape[1]-1\n",
    "    last_rs_experiment = int(exp)\n",
    "\n",
    "    # Plus one because we want to start in the next one\n",
    "    last_bo_iteration = experiment_configurations.shape[2]-1\n",
    "    last_rs_iteration = int(iter)+1\n",
    "\n",
    "  # Now lets get the initial data\n",
    "  bo_experiment_df = experiment_df[(experiment_df[\"method\"]==method) & (experiment_df[\"experiment\"]==int(exp))]\n",
    "\n",
    "  init_x = torch.DoubleTensor([[float(lr), float(gamma)] for (lr, gamma) in zip(bo_experiment_df.learning_rate.values, bo_experiment_df.gamma.values)])\n",
    "\n",
    "  init_y = torch.DoubleTensor([[float(reward)] for reward in bo_experiment_df.reward_lower_bound.values])\n",
    "  best_init_y = init_y.max().item()\n",
    "\n",
    "  rs_experiment_df = experiment_df[experiment_df[\"method\"]==0]\n",
    "  if rs_experiment_df.empty:\n",
    "      best_rs_lr = 0\n",
    "      best_rs_gamma = 0\n",
    "      best_rs_r = 0\n",
    "  else:\n",
    "      best_rs_lr = rs_experiment_df.iloc[-1][\"best_learning_rate\"]\n",
    "      best_rs_gamma = rs_experiment_df.iloc[-1][\"best_gamma\"]\n",
    "      best_rs_r = rs_experiment_df.iloc[-1][\"best_reward_lower_bound\"]\n",
    "             \n",
    "  return experiment_df, last_bo_experiment, last_rs_experiment, last_bo_iteration, last_rs_iteration, init_x, init_y, best_init_y, best_rs_lr, best_rs_gamma, best_rs_r, bo_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AChh4uxu6lk"
   },
   "source": [
    "Now let's set the configuration for the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FF5-qYTvN2zZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# The name of the file (WITHOUT EXTENSION) where the history of experiments will be saved\n",
    "experiment_name = \"lunar_lander_learning_rate_gamma\"\n",
    "# If true, this will look for the experiment history .csv in google drive and continue from there\n",
    "continue_from_checkpoint = False\n",
    "\n",
    "# Number of experiments per method\n",
    "n_experiments = 15\n",
    "\n",
    "# Number of iterations per experiment after the first random point being evaluated\n",
    "n_iterations = 25\n",
    "\n",
    "# Number of methods\n",
    "n_methods = 2\n",
    "\n",
    "# Index of Bayesian Optimization method\n",
    "bo_method = 0\n",
    "\n",
    "# Index of Random Search method\n",
    "rs_method = 1\n",
    "\n",
    "# Number of Hyperparameters\n",
    "n_hyperparameters = 2\n",
    "\n",
    "# Arrays containing the results and configurations of experiments\n",
    "experiment_results = np.zeros((n_methods, n_experiments, n_iterations+1))\n",
    "experiment_configurations = np.zeros((n_methods, n_experiments, n_iterations+1, len(bounds_tensor[0])))\n",
    "\n",
    "# Now load checkpoint if necessary\n",
    "if continue_from_checkpoint:\n",
    "  experiment_df, last_bo_experiment, last_rs_experiment, last_bo_iteration, last_rs_iteration, init_x, init_y, best_init_y, best_observed_lr_rs, best_observed_gamma_rs, best_observed_result_rs, bo_done = load_checkpoint(experiment_name,\n",
    "                                            experiment_results,\n",
    "                                            experiment_configurations) \n",
    "else: \n",
    "  experiment_df = create_experiment_df()\n",
    "  \n",
    "  save_checkpoint(experiment_df, \n",
    "                  experiment_name)\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHTBIpJSQJVZ"
   },
   "source": [
    "First we try the bayesian optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jScxWwkmzEr",
    "outputId": "96830da6-8047-4f36-ad5d-d54acfde7d84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=-851.87 +/- 521.1562181712144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8369/2291881894.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  normalized_bounds = torch.tensor([np.zeros(len(bounds_tensor[0])), np.ones(len(bounds_tensor[0]))])\n",
      "/home/daniel/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=-343.31 +/- 96.31171471689615\n",
      "Number of iteration: 1\n",
      "  - Unnormalized learning rate: 0.0001\n",
      "  - Normalized learning rate: 0.0\n",
      "  - Unnormalized gamma: 0.9997\n",
      "  - Normalized gamma: 1.0\n",
      "  - Unstandardized lower reward bound: -439.6173378644187\n",
      "  - Standardized lower reward bound: 0.7071067811865476\n",
      "Best point performs this way: -439.6173378644187\n",
      "----------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 52\u001b[0m\n\u001b[1;32m     48\u001b[0m   experiment_results[rs_method,e,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m best_observed_result_bo\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(init_iteration, n_iterations\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;66;03m# Get the next points given our actual queries\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m   normalized_new_candidates \u001b[38;5;241m=\u001b[39m \u001b[43mget_next_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_x_normalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43minit_y_standardized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mbest_init_y_standardized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mnormalized_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mn_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m   \u001b[38;5;66;03m# Unnormalize the candidate hyperparameter value\u001b[39;00m\n\u001b[1;32m     59\u001b[0m   new_candidates \u001b[38;5;241m=\u001b[39m unnormalize(normalized_new_candidates,\n\u001b[1;32m     60\u001b[0m                                bounds\u001b[38;5;241m=\u001b[39mbounds_tensor)\n",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m, in \u001b[0;36mget_next_points\u001b[0;34m(init_x, init_y, best_init_y, normalized_bounds, n_points)\u001b[0m\n\u001b[1;32m     34\u001b[0m UCB \u001b[38;5;241m=\u001b[39m UpperConfidenceBound(model\u001b[38;5;241m=\u001b[39msingle_model,\n\u001b[1;32m     35\u001b[0m                            beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     36\u001b[0m                            maximize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Maximize the acquisition function to obtain our candidates \u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m candidates, _ \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_acqf\u001b[49m\u001b[43m(\u001b[49m\u001b[43macq_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUCB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalized_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_restarts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mraw_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                              \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_limit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m candidates\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/botorch/optim/optimize.py:544\u001b[0m, in \u001b[0;36moptimize_acqf\u001b[0;34m(acq_function, bounds, q, num_restarts, raw_samples, options, inequality_constraints, equality_constraints, nonlinear_inequality_constraints, fixed_features, post_processing_func, batch_initial_conditions, return_best_only, gen_candidates, sequential, ic_generator, timeout_sec, return_full_tree, retry_on_optimization_warning, **ic_gen_kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m     gen_candidates \u001b[38;5;241m=\u001b[39m gen_candidates_scipy\n\u001b[1;32m    522\u001b[0m opt_acqf_inputs \u001b[38;5;241m=\u001b[39m OptimizeAcqfInputs(\n\u001b[1;32m    523\u001b[0m     acq_function\u001b[38;5;241m=\u001b[39macq_function,\n\u001b[1;32m    524\u001b[0m     bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     ic_gen_kwargs\u001b[38;5;241m=\u001b[39mic_gen_kwargs,\n\u001b[1;32m    543\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_optimize_acqf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_acqf_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/botorch/optim/optimize.py:573\u001b[0m, in \u001b[0;36m_optimize_acqf\u001b[0;34m(opt_inputs)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _optimize_acqf_sequential_q(\n\u001b[1;32m    567\u001b[0m         opt_inputs\u001b[38;5;241m=\u001b[39mopt_inputs,\n\u001b[1;32m    568\u001b[0m         timeout_sec\u001b[38;5;241m=\u001b[39mtimeout_sec,\n\u001b[1;32m    569\u001b[0m         start_time\u001b[38;5;241m=\u001b[39mstart_time,\n\u001b[1;32m    570\u001b[0m     )\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# Batch optimization (including the case q=1)\u001b[39;00m\n\u001b[0;32m--> 573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_optimize_acqf_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_sec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_sec\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/botorch/optim/optimize.py:351\u001b[0m, in \u001b[0;36m_optimize_acqf_batch\u001b[0;34m(opt_inputs, start_time, timeout_sec)\u001b[0m\n\u001b[1;32m    348\u001b[0m         batch_acq_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch_acq_values_list)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch_candidates, batch_acq_values, opt_warnings\n\u001b[0;32m--> 351\u001b[0m batch_candidates, batch_acq_values, ws \u001b[38;5;241m=\u001b[39m \u001b[43m_optimize_batch_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_sec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m optimization_warning_raised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    354\u001b[0m     (\u001b[38;5;28missubclass\u001b[39m(w\u001b[38;5;241m.\u001b[39mcategory, OptimizationWarning) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m ws)\n\u001b[1;32m    355\u001b[0m )\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimization_warning_raised \u001b[38;5;129;01mand\u001b[39;00m opt_inputs\u001b[38;5;241m.\u001b[39mretry_on_optimization_warning:\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/botorch/optim/optimize.py:335\u001b[0m, in \u001b[0;36m_optimize_acqf_batch.<locals>._optimize_batch_candidates\u001b[0;34m(timeout_sec)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m ws:\n\u001b[1;32m    331\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malways\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39mOptimizationWarning)\n\u001b[1;32m    332\u001b[0m     (\n\u001b[1;32m    333\u001b[0m         batch_candidates_curr,\n\u001b[1;32m    334\u001b[0m         batch_acq_values_curr,\n\u001b[0;32m--> 335\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mopt_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_ics_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macq_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfiltered_gen_kwargs\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m opt_warnings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ws\n\u001b[1;32m    339\u001b[0m batch_candidates_list\u001b[38;5;241m.\u001b[39mappend(batch_candidates_curr)\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/botorch/generation/gen.py:232\u001b[0m, in \u001b[0;36mgen_candidates_scipy\u001b[0;34m(initial_conditions, acquisition_function, lower_bounds, upper_bounds, inequality_constraints, equality_constraints, nonlinear_inequality_constraints, options, fixed_features, timeout_sec)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x):\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39macquisition_function(x)\n\u001b[0;32m--> 232\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mminimize_with_timeout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf_np_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmethod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSLSQP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallback\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmethod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallback\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwith_grad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_sec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_sec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m _process_scipy_result(res\u001b[38;5;241m=\u001b[39mres, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m    250\u001b[0m candidates \u001b[38;5;241m=\u001b[39m fix_features(\n\u001b[1;32m    251\u001b[0m     X\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfrom_numpy(res\u001b[38;5;241m.\u001b[39mx)\u001b[38;5;241m.\u001b[39mto(initial_conditions)\u001b[38;5;241m.\u001b[39mreshape(shapeX),\n\u001b[1;32m    252\u001b[0m     fixed_features\u001b[38;5;241m=\u001b[39mfixed_features,\n\u001b[1;32m    253\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/botorch/optim/utils/timeout.py:80\u001b[0m, in \u001b[0;36mminimize_with_timeout\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options, timeout_sec)\u001b[0m\n\u001b[1;32m     77\u001b[0m     wrapped_callback \u001b[38;5;241m=\u001b[39m callback\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhessp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhessp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrapped_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OptimizationTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     95\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimization timed out after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/scipy/optimize/_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    693\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    694\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 696\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    699\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    700\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py:359\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    353\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/scipy/optimize/_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/scipy/optimize/_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 70\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/botorch/generation/gen.py:190\u001b[0m, in \u001b[0;36mgen_candidates_scipy.<locals>.f_np_wrapper\u001b[0;34m(x, f)\u001b[0m\n\u001b[1;32m    182\u001b[0m X \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    183\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfrom_numpy(x)\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;241m.\u001b[39mto(initial_conditions)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m )\n\u001b[1;32m    189\u001b[0m X_fix \u001b[38;5;241m=\u001b[39m fix_features(X, fixed_features\u001b[38;5;241m=\u001b[39mfixed_features)\n\u001b[0;32m--> 190\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_fix\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# compute gradient w.r.t. the inputs (does not accumulate in leaves)\u001b[39;00m\n\u001b[1;32m    192\u001b[0m gradf \u001b[38;5;241m=\u001b[39m _arrayify(torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(loss, X)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/botorch/generation/gen.py:230\u001b[0m, in \u001b[0;36mgen_candidates_scipy.<locals>.f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x):\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[43macquisition_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/botorch/utils/transforms.py:287\u001b[0m, in \u001b[0;36mt_batch_mode_transform.<locals>.decorator.<locals>.decorated\u001b[0;34m(acqf, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# add t-batch dim\u001b[39;00m\n\u001b[1;32m    286\u001b[0m X \u001b[38;5;241m=\u001b[39m X \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 287\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43macqf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(acqf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_fully_bayesian(acqf\u001b[38;5;241m.\u001b[39mmodel):\n\u001b[1;32m    289\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/botorch/acquisition/analytic.py:790\u001b[0m, in \u001b[0;36mUpperConfidenceBound.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;129m@t_batch_mode_transform\u001b[39m(expected_q\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    781\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate the Upper Confidence Bound on the candidate set X.\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \n\u001b[1;32m    783\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;124;03m        given design points `X`.\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m     mean, sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean_and_sigma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (mean \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaximize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39mmean) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m*\u001b[39m sigma\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/botorch/acquisition/analytic.py:111\u001b[0m, in \u001b[0;36mAnalyticAcquisitionFunction._mean_and_sigma\u001b[0;34m(self, X, compute_sigma, min_var)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the first and second moments of the model posterior.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    returns a single tensor of means if compute_sigma is True.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# ensures buffers / parameters are on the same device\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m posterior \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposterior\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposterior_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposterior_transform\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m mean \u001b[38;5;241m=\u001b[39m posterior\u001b[38;5;241m.\u001b[39mmean\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# removing redundant dimensions\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compute_sigma:\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/botorch/models/gpytorch.py:379\u001b[0m, in \u001b[0;36mBatchedMultiOutputGPyTorchModel.posterior\u001b[0;34m(self, X, output_indices, observation_noise, posterior_transform, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    376\u001b[0m     X, output_dim_idx \u001b[38;5;241m=\u001b[39m add_output_dim(\n\u001b[1;32m    377\u001b[0m         X\u001b[38;5;241m=\u001b[39mX, original_batch_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_batch_shape\n\u001b[1;32m    378\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m mvn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observation_noise \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(observation_noise):\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;66;03m# TODO: Validate noise shape\u001b[39;00m\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;66;03m# make observation_noise `batch_shape x q x n`\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/gpytorch/models/exact_gp.py:332\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# Make the prediction\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mcg_tolerance(settings\u001b[38;5;241m.\u001b[39meval_cg_tolerance\u001b[38;5;241m.\u001b[39mvalue()):\n\u001b[1;32m    329\u001b[0m     (\n\u001b[1;32m    330\u001b[0m         predictive_mean,\n\u001b[1;32m    331\u001b[0m         predictive_covar,\n\u001b[0;32m--> 332\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexact_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_covar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# Reshape predictive mean to match the appropriate event shape\u001b[39;00m\n\u001b[1;32m    335\u001b[0m predictive_mean \u001b[38;5;241m=\u001b[39m predictive_mean\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39mtest_shape)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/gpytorch/models/exact_prediction_strategies.py:264\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_prediction\u001b[0;34m(self, joint_mean, joint_covar)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# For efficiency - we can make things more efficient\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m joint_covar\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mmax_eager_kernel_size\u001b[38;5;241m.\u001b[39mvalue():\n\u001b[0;32m--> 264\u001b[0m     test_covar \u001b[38;5;241m=\u001b[39m \u001b[43mjoint_covar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     test_test_covar \u001b[38;5;241m=\u001b[39m test_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :]\n\u001b[1;32m    266\u001b[0m     test_train_covar \u001b[38;5;241m=\u001b[39m test_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train]\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:410\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.to_dense\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_dense()\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:25\u001b[0m, in \u001b[0;36mrecall_grad_state.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_grad_enabled):\n\u001b[0;32m---> 25\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:355\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.evaluate_kernel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m     temp_active_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m temp_active_dims\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# Check the size of the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/gpytorch/kernels/kernel.py:524\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[0;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[1;32m    521\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    523\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[0;32m--> 524\u001b[0m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mKernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx1_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/gpytorch/kernels/scale_kernel.py:109\u001b[0m, in \u001b[0;36mScaleKernel.forward\u001b[0;34m(self, x1, x2, last_dim_is_batch, diag, **params)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, last_dim_is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m--> 109\u001b[0m     orig_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_kernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     outputscales \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputscale\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m last_dim_is_batch:\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/gpytorch/kernels/matern_kernel.py:99\u001b[0m, in \u001b[0;36mMaternKernel.forward\u001b[0;34m(self, x1, x2, diag, **params)\u001b[0m\n\u001b[1;32m     97\u001b[0m x1_ \u001b[38;5;241m=\u001b[39m (x1 \u001b[38;5;241m-\u001b[39m mean)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlengthscale)\n\u001b[1;32m     98\u001b[0m x2_ \u001b[38;5;241m=\u001b[39m (x2 \u001b[38;5;241m-\u001b[39m mean)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlengthscale)\n\u001b[0;32m---> 99\u001b[0m distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovar_dist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m exp_component \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mmath\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnu \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m distance)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnu \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/gpytorch/kernels/kernel.py:351\u001b[0m, in \u001b[0;36mKernel.covar_dist\u001b[0;34m(self, x1, x2, diag, last_dim_is_batch, square_dist, **params)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m     dist_func \u001b[38;5;241m=\u001b[39m sq_dist \u001b[38;5;28;01mif\u001b[39;00m square_dist \u001b[38;5;28;01melse\u001b[39;00m dist\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdist_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1_eq_x2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/gpytorch/kernels/kernel.py:53\u001b[0m, in \u001b[0;36mdist\u001b[0;34m(x1, x2, x1_eq_x2)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdist\u001b[39m(x1, x2, x1_eq_x2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# TODO: use torch cdist once implementation is improved: https://github.com/pytorch/pytorch/pull/25799\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43msq_dist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1_eq_x2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx1_eq_x2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mclamp_min_(\u001b[38;5;241m1e-30\u001b[39m)\u001b[38;5;241m.\u001b[39msqrt_()\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/gpytorch/kernels/kernel.py:28\u001b[0m, in \u001b[0;36msq_dist\u001b[0;34m(x1, x2, x1_eq_x2)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msq_dist\u001b[39m(x1, x2, x1_eq_x2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# TODO: use torch squared cdist once implemented: https://github.com/pytorch/pytorch/pull/25799\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     adjustment \u001b[38;5;241m=\u001b[39m \u001b[43mx1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x1 \u001b[38;5;241m-\u001b[39m adjustment\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Compute squared distance matrix using quadratic expansion\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if continue_from_checkpoint:\n",
    "  init_experiment = last_bo_experiment\n",
    "  init_iteration = last_bo_iteration\n",
    "else:\n",
    "  init_experiment = 0\n",
    "  init_iteration = 1\n",
    "  bo_done = False\n",
    "\n",
    "if not bo_done:\n",
    "  for e in range(init_experiment, n_experiments):\n",
    "    print(f\"EXPERIMENT {e}\")\n",
    "    if not (continue_from_checkpoint and init_experiment == e):\n",
    "      # Sample initial hyperparameter values and evaluate the models obtained with them\n",
    "      init_x, init_y, best_init_y = generate_initial_data(bounds_tensor,\n",
    "                                                          1)\n",
    "\n",
    "    # We normalize the bounds of the hyperparameters as BOTorch assumes this\n",
    "    normalized_bounds = torch.tensor([np.zeros(len(bounds_tensor[0])), np.ones(len(bounds_tensor[0]))])\n",
    "\n",
    "    # Normalize the hyperparameter as BOTorch assumes this\n",
    "    init_x_normalized = normalize(init_x,\n",
    "                                  bounds=bounds_tensor)\n",
    "\n",
    "    # Standardize the objective as BOTorch assumes this\n",
    "    init_y_standardized = standardize(init_y)\n",
    "\n",
    "    # Obtain the best result among the initial random experiments\n",
    "    best_init_y_standardized = init_y_standardized.max().item()\n",
    "\n",
    "    best_observed_result_bo = best_init_y\n",
    "    best_observed_lr_bo, best_observed_gamma_bo = get_hyp_values(init_x[0])\n",
    "\n",
    "    if not (continue_from_checkpoint and init_experiment == e):\n",
    "      experiment_df = update_experiment_history(bo_method, \n",
    "                                                e, \n",
    "                                                0,\n",
    "                                                best_observed_lr_bo, # The learning rate selected for this iteration\n",
    "                                                best_observed_gamma_bo, # The gamma selected for this iteration\n",
    "                                                best_observed_result_bo, # The reward lower bound of the model\n",
    "                                                best_observed_lr_bo, # The best learning rate\n",
    "                                                best_observed_gamma_bo, # The best gamma\n",
    "                                                best_observed_result_bo, # The reward lower bound\n",
    "                                                experiment_df,\n",
    "                                                experiment_name)\n",
    "      \n",
    "      experiment_configurations[rs_method,e,0,0] = best_observed_lr_bo\n",
    "      experiment_configurations[rs_method,e,0,1] = best_observed_gamma_bo\n",
    "      experiment_results[rs_method,e,0] = best_observed_result_bo\n",
    "\n",
    "    for i in range(init_iteration, n_iterations+1):\n",
    "      # Get the next points given our actual queries\n",
    "      normalized_new_candidates = get_next_points(init_x_normalized,\n",
    "                                                  init_y_standardized, \n",
    "                                                  best_init_y_standardized, \n",
    "                                                  normalized_bounds,\n",
    "                                                  n_points=1)\n",
    "    \n",
    "      # Unnormalize the candidate hyperparameter value\n",
    "      new_candidates = unnormalize(normalized_new_candidates,\n",
    "                                   bounds=bounds_tensor)\n",
    "      \n",
    "      # Compute the performance of the model\n",
    "      new_results = torch.tensor([[target_function(new_candidates[0])]])\n",
    "\n",
    "      # Update our hyperparameters and rewards history\n",
    "      init_x = torch.cat([init_x, new_candidates])\n",
    "      init_y = torch.cat([init_y, new_results])\n",
    "\n",
    "      # Normalize our updated hyperparameters and rewards history\n",
    "      init_x_normalized = normalize(init_x, bounds=bounds_tensor)\n",
    "      init_y_standardized = standardize(init_y)\n",
    "\n",
    "      # Update the best reward\n",
    "      best_init_y = init_y.max().item()\n",
    "      best_init_y_standardized = init_y_standardized.max().item()\n",
    "      \n",
    "      # Show iteration info\n",
    "      \n",
    "      print(f\"Number of iteration: {i}\")\n",
    "      print(f\"  - Unnormalized learning rate: {new_candidates[0][0].item()}\")\n",
    "      print(f\"  - Normalized learning rate: {normalized_new_candidates[0][0].item()}\")\n",
    "      print(f\"  - Unnormalized gamma: {new_candidates[0][1].item()}\")\n",
    "      print(f\"  - Normalized gamma: {normalized_new_candidates[0][1].item()}\")\n",
    "      print(f\"  - Unstandardized lower reward bound: {new_results.item()}\")\n",
    "      print(f\"  - Standardized lower reward bound: {init_y_standardized[-1].item()}\")\n",
    "      print(f\"Best point performs this way: {best_init_y}\")\n",
    "\n",
    "\n",
    "      if best_observed_result_bo < new_results[0][0].item():\n",
    "        best_observed_result_bo = new_results[0][0].item()\n",
    "        best_observed_lr_bo, best_observed_gamma_bo = get_hyp_values(new_candidates[0])\n",
    "\n",
    "      experiment_df = update_experiment_history(bo_method, \n",
    "                                                e, \n",
    "                                                i,\n",
    "                                                new_candidates[0][0].item(), # The lr selected for this iteration\n",
    "                                                new_candidates[0][1].item(), # The gamma selected for this iteration\n",
    "                                                new_results[0][0].item(), # The reward lower bound of the model\n",
    "                                                best_observed_lr_bo, # The best learning rate\n",
    "                                                best_observed_gamma_bo, # The best gamma\n",
    "                                                best_observed_result_bo, # The reward lower bound\n",
    "                                                experiment_df,\n",
    "                                                experiment_name)\n",
    "\n",
    "      experiment_configurations[bo_method,e,i,0] = best_observed_lr_bo\n",
    "      experiment_configurations[bo_method,e,i,1] = best_observed_gamma_bo\n",
    "      experiment_results[bo_method,e,i] = best_observed_result_bo\n",
    "      print('----------------------')\n",
    "    init_iteration = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kbrnek5JW9f"
   },
   "source": [
    "Now we perform a random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8vpuRYDRJWmf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=-263.70 +/- 204.69715022061874\n",
      "mean_reward=-844.08 +/- 457.2561413948255\n",
      "mean_reward=-507.06 +/- 58.7860923011357\n",
      "mean_reward=-169.76 +/- 145.36366431745546\n",
      "mean_reward=-1203.63 +/- 589.8601764455119\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m random_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(bounds_tensor[\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;241m*\u001b[39m (bounds_tensor[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m bounds_tensor[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m bounds_tensor[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Evaluate the model with that hyperparameter value\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m rs_obj_fun_result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_value\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Update best reward and candidate found if necessary\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_observed_result_rs \u001b[38;5;241m<\u001b[39m rs_obj_fun_result:\n",
      "Cell \u001b[0;32mIn[2], line 105\u001b[0m, in \u001b[0;36mtarget_function\u001b[0;34m(hyperparams, timesteps, rl_env_name)\u001b[0m\n\u001b[1;32m    100\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(hyperparams, env_name\u001b[38;5;241m=\u001b[39mrl_env_name)\n\u001b[1;32m    102\u001b[0m train_model(model, \n\u001b[1;32m    103\u001b[0m             timesteps)\n\u001b[0;32m--> 105\u001b[0m lower_mean_reward \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mrl_env_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lower_mean_reward\n",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, rl_env_name, n_eval_episodes)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03mEvaluates the model for a number of episodes in a specified environment, this environment MUST be the same as the one the model has been trained in.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m                lower_mean_reward (float): A tensor of size 1x1 (1 row, 1 column) containing the mean_reward\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     76\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(rl_env_name)\n\u001b[0;32m---> 77\u001b[0m mean_reward, std_reward \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_reward=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m +/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m lower_mean_reward \u001b[38;5;241m=\u001b[39m mean_reward \u001b[38;5;241m-\u001b[39m std_reward\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-2hp-env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:97\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     95\u001b[0m current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m     96\u001b[0m current_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_envs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m episode_counts[i] \u001b[38;5;241m<\u001b[39m episode_count_targets[i]:\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# unpack values so that the callback can access the local variables\u001b[39;00m\n\u001b[1;32m    100\u001b[0m         reward \u001b[38;5;241m=\u001b[39m rewards[i]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if continue_from_checkpoint:\n",
    "  init_experiment = last_rs_experiment\n",
    "  init_iteration = last_rs_iteration\n",
    "else:\n",
    "  init_experiment = 0\n",
    "  init_iteration = 1\n",
    "\n",
    "for e in range(init_experiment, n_experiments):\n",
    "  if not (continue_from_checkpoint and init_experiment == e) or (init_experiment==0 and init_iteration==1):\n",
    "    # Initiate with a random value\n",
    "    random_value = torch.rand(1, len(bounds_tensor[0])) * (bounds_tensor[1] - bounds_tensor[0]) + bounds_tensor[0]\n",
    "    best_observed_result_rs = target_function(random_value[0])\n",
    "    best_observed_lr_rs, best_observed_gamma_rs = get_hyp_values(random_value[0])\n",
    "    # Update our experiments histories\n",
    "    experiment_df = update_experiment_history(rs_method, \n",
    "                                              e, \n",
    "                                              0,\n",
    "                                              best_observed_lr_rs, # The learning rate value selected for this iteration\n",
    "                                              best_observed_gamma_rs, # The gamma value selected for this iteration\n",
    "                                              best_observed_result_rs, # The reward lower bound of the model\n",
    "                                              best_observed_lr_rs, # The best learning rate\n",
    "                                              best_observed_gamma_rs, # The best gamma\n",
    "                                              best_observed_result_rs, # The reward lower bound\n",
    "                                              experiment_df,\n",
    "                                              experiment_name)\n",
    "  \n",
    "  # Iterate with random search\n",
    "  for i in range(init_iteration, n_iterations+1):\n",
    "    # Get a new random value for the hyperparameter\n",
    "    random_value = torch.rand(1, len(bounds_tensor[0])) * (bounds_tensor[1] - bounds_tensor[0]) + bounds_tensor[0]\n",
    "    # Evaluate the model with that hyperparameter value\n",
    "    rs_obj_fun_result = target_function(random_value[0])\n",
    "\n",
    "    # Update best reward and candidate found if necessary\n",
    "    if best_observed_result_rs < rs_obj_fun_result:\n",
    "      best_observed_result_rs = rs_obj_fun_result\n",
    "      best_observed_lr_rs, best_observed_gamma_rs = get_hyp_values(random_value[0])\n",
    "    \n",
    "    # Update our experiments histories\n",
    "    experiment_df = update_experiment_history(rs_method, \n",
    "                                              e, \n",
    "                                              i,\n",
    "                                              best_observed_lr_rs, # The learning rate value selected for this iteration\n",
    "                                              best_observed_gamma_rs, # The gamma value selected for this iteration\n",
    "                                              best_observed_result_rs, # The reward lower bound of the model\n",
    "                                              best_observed_lr_rs, # The best learning rate\n",
    "                                              best_observed_gamma_rs, # The best gamma\n",
    "                                              best_observed_result_rs, # The reward lower bound\n",
    "                                              experiment_df,\n",
    "                                              experiment_name)\n",
    "    \n",
    "    experiment_configurations[rs_method,e,i,0] = best_observed_lr_rs\n",
    "    experiment_configurations[rs_method,e,i,1] = best_observed_gamma_rs\n",
    "    experiment_results[rs_method,e,i] = best_observed_result_rs\n",
    "\n",
    "  init_iteration = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukMdAAapSR2c"
   },
   "source": [
    "# Step 7: Compare the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeN8EWoOSVxC"
   },
   "source": [
    "First we give the recommendation as the best observed result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlf2cyT-SZf2"
   },
   "outputs": [],
   "source": [
    "best_observed_result = np.max(experiment_results)\n",
    "index_set = np.where(experiment_results==best_observed_result)\n",
    "print(\"The best observed result is: \" + str(best_observed_result))\n",
    "print(\"The best observed result belong to the : \" + str(index_set[0][0]) + \" method. Its value is \" + str(experiment_configurations[index_set][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jZ5oFK6Sf3Q"
   },
   "source": [
    "And now we plot the results to compare both methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgo_wr7VSj7k"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(1, n_iterations, n_iterations).astype(int)\n",
    "mean_bo = np.mean(experiment_results[0,:,:], axis=0)\n",
    "mean_rs = np.mean(experiment_results[1,:,:], axis=0)\n",
    "std_bo = np.std(experiment_results[0,:,:], axis=0) * 0.25\n",
    "std_rs = np.std(experiment_results[1,:,:], axis=0) * 0.25\n",
    "bo_ub_results = go.Scatter(x=x, y=mean_bo + std_bo, mode='lines', name=\"\", line_color=\"green\", line_width=0.1)\n",
    "bo_results = go.Scatter(x=x, y=mean_bo, mode='lines', fill='tonexty', line_color=\"green\", name=\"Bayesian Optimization\")\n",
    "bo_lb_results = go.Scatter(x=x, y=mean_bo - std_bo, mode='lines', fill='tonexty', name=\"\", line_color=\"green\", line_width=0.1)\n",
    "\n",
    "rs_ub_results = go.Scatter(x=x, y=mean_rs + std_rs, mode='lines', name=\"\", line_color=\"red\", line_width=0.1)\n",
    "rs_results = go.Scatter(x=x, y=mean_rs, mode='lines', fill='tonexty', line_color=\"red\", name=\"Random Search\")\n",
    "rs_lb_results = go.Scatter(x=x, y=mean_rs - std_rs, mode='lines', fill='tonexty', name=\"\", line_color=\"red\", line_width=0.1)\n",
    "  \n",
    "fig = go.Figure()\n",
    "fig.add_trace(bo_ub_results)\n",
    "fig.add_trace(bo_results)\n",
    "fig.add_trace(bo_lb_results)\n",
    "fig.add_trace(rs_ub_results)\n",
    "fig.add_trace(rs_results)\n",
    "fig.add_trace(rs_lb_results)\n",
    "fig.update_layout(title=\"Performance comparison between BO and RS\", xaxis_title=\"Iterations\", yaxis_title=\"Reward lower bound\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "bo-drl-2hp-env",
   "language": "python",
   "name": "bo-drl-2hp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
