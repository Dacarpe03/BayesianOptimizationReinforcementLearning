{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR0o63R4Xa6F"
      },
      "source": [
        "# Step 1: Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-GEDX7xi_IE7"
      },
      "outputs": [],
      "source": [
        "#!pip3 install setuptools==65.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQXJ4h82Xt42",
        "outputId": "de83d2df-b6af-4a92-863f-4625b0ed29c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting swig\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gymnasium[box2d]\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.22.4)\n",
            "Collecting jax-jumpy>=1.0.0 (from gymnasium[box2d])\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[box2d])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.3 (from gymnasium[box2d])\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.1.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2812274 sha256=c10ebaa5bdb4d053add2fe0ea54d0da493471802239a7761b08211534dae99f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: farama-notifications, box2d-py, pygame, jax-jumpy, gymnasium\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.3.0\n",
            "    Uninstalling pygame-2.3.0:\n",
            "      Successfully uninstalled pygame-2.3.0\n",
            "Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0 pygame-2.1.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting botorch\n",
            "  Downloading botorch-0.8.5-py3-none-any.whl (530 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m530.3/530.3 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from botorch) (0.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from botorch) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.12 in /usr/local/lib/python3.10/dist-packages (from botorch) (2.0.0+cu118)\n",
            "Collecting pyro-ppl>=1.8.4 (from botorch)\n",
            "  Downloading pyro_ppl-1.8.4-py3-none-any.whl (730 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.7/730.7 kB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gpytorch==1.10 (from botorch)\n",
            "  Downloading gpytorch-1.10-py3-none-any.whl (255 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting linear-operator==0.4.0 (from botorch)\n",
            "  Downloading linear_operator-0.4.0-py3-none-any.whl (156 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.7/156.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch==1.10->botorch) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl>=1.8.4->botorch) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl>=1.8.4->botorch) (3.3.0)\n",
            "Collecting pyro-api>=0.1.1 (from pyro-ppl>=1.8.4->botorch)\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl>=1.8.4->botorch) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->botorch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->botorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->botorch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->botorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->botorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->botorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.12->botorch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.12->botorch) (16.0.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from multipledispatch->botorch) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12->botorch) (2.1.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch==1.10->botorch) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch==1.10->botorch) (3.1.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12->botorch) (1.3.0)\n",
            "Installing collected packages: pyro-api, linear-operator, pyro-ppl, gpytorch, botorch\n",
            "Successfully installed botorch-0.8.5 gpytorch-1.10 linear-operator-0.4.0 pyro-api-0.1.2 pyro-ppl-1.8.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable-baselines3[extra]>=2.0.0a4\n",
            "  Downloading stable_baselines3-2.0.0a5-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.5/177.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gymnasium==0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (0.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.0.0+cu118)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (4.7.0.72)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (4.65.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (13.3.4)\n",
            "Collecting shimmy[atari]~=0.2.1 (from stable-baselines3[extra]>=2.0.0a4)\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (8.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.6.0 (from stable-baselines3[extra]>=2.0.0a4)\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.1.3)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->stable-baselines3[extra]>=2.0.0a4) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->stable-baselines3[extra]>=2.0.0a4) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->stable-baselines3[extra]>=2.0.0a4) (0.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]>=2.0.0a4) (8.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]>=2.0.0a4) (2.27.1)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]>=2.0.0a4)\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=0.2.1->stable-baselines3[extra]>=2.0.0a4)\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3[extra]>=2.0.0a4) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3[extra]>=2.0.0a4) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3[extra]>=2.0.0a4) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3[extra]>=2.0.0a4) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3[extra]>=2.0.0a4) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->stable-baselines3[extra]>=2.0.0a4) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->stable-baselines3[extra]>=2.0.0a4) (16.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]>=2.0.0a4) (2022.7.1)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (2.14.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=0.2.1->stable-baselines3[extra]>=2.0.0a4) (5.12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a4) (0.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]>=2.0.0a4) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]>=2.0.0a4) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]>=2.0.0a4) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]>=2.0.0a4) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11->stable-baselines3[extra]>=2.0.0a4) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=103a4f54b1823fc67cb53006db5ef4e4e6c3ead92dda943144c878e0c38226ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: ale-py, AutoROM.accept-rom-license, autorom, shimmy, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 shimmy-0.2.1 stable-baselines3-2.0.0a5\n"
          ]
        }
      ],
      "source": [
        "#!pip install gym[box2d]\n",
        "!pip install swig\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install botorch\n",
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\"\n",
        "#!pip install stable-baselines3[extra] pyglet==1.5.27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpv4u1hZX2Vv"
      },
      "source": [
        "# Step 2: Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riwFQZMNtU_4"
      },
      "source": [
        "Libraries used for the Bayesian Optimization Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4mw25s5cYPuv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a877f5c1-b60f-4597-8751-0f8ff2488de6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "import botorch\n",
        "from botorch.utils.transforms import standardize, normalize, unnormalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSAValUHtY_h"
      },
      "source": [
        "Libraries used to save checkpoints in GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A8DoSb8utUAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "110c0c10-5b66-4a38-ffb9-e1afcac2b4cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/pexpect/popen_spawn.py:60: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
            "  self._read_thread.setDaemon(True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xdYx-gMYyeV"
      },
      "source": [
        "# Step 3: Define objetive function\n",
        "This will be the lower bound of the mean reward of a trained model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XNRXK99lY_T9"
      },
      "outputs": [],
      "source": [
        "def get_hyp_values(hyperparams_tensor):\n",
        "  '''\n",
        "  Returns a tuple of values from a tensor containing a hyperparameter configuration\n",
        "\n",
        "          Parameters:\n",
        "                  hyperparams_tensor (torch.DoubleTensor): A tensor of size 1xn (1 row, n columns) with n being the number of hyperparameters to tune\n",
        "          \n",
        "          Returns:\n",
        "                  hyperparams_tuple (tuple): A tuple with the unpacked values of the hyperparams_tensor \n",
        "\n",
        "  '''\n",
        "  hyperparams_list = [hyperparams_tensor[i].item() for i in range(len(hyperparams_tensor))]\n",
        "  hyperparams_tuple = tuple(hyperparams_list)\n",
        "  return tuple(hyperparams_list)\n",
        "\n",
        "\n",
        "def create_model(hyperparams,\n",
        "                 policy='MlpPolicy',\n",
        "                 env_name='LunarLander-v2'):\n",
        "  '''\n",
        "  Returns a PPO model given a policy, environment, and hyperparameters of PPO\n",
        "\n",
        "          Parameters:\n",
        "                  hyperparams (torch.DoubleTensor): A tensor of size 1x2 (1 row, n columns) with the learning rate and gamma to train the model with\n",
        "                  policy (str): The NN to train with PPO in the environment. Default is 'MlpPolicy'\n",
        "                  env (stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv): Specifies the gym environment to use for the training\n",
        "\n",
        "          Returns:\n",
        "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
        "  '''\n",
        "  lr, gamma  = get_hyp_values(hyperparams)\n",
        "  env = make_vec_env(env_name, n_envs=1)\n",
        "  model = PPO(policy = policy,\n",
        "              env = env,\n",
        "              learning_rate = lr,\n",
        "              n_steps = 1024,\n",
        "              batch_size = 64,\n",
        "              n_epochs = 4,\n",
        "              gamma = gamma,\n",
        "              gae_lambda = 0.98,\n",
        "              ent_coef = 0.01,\n",
        "              verbose=0)\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "def train_model(model, timesteps=500000):\n",
        "  '''\n",
        "  Trains a PPO model during a number of timesteps\n",
        "          \n",
        "          Parameters:\n",
        "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
        "                  timesteps (int): The number of timesteps used to train the model\n",
        "\n",
        "          Returns:\n",
        "                  None\n",
        "  '''\n",
        "  model.learn(total_timesteps=timesteps)\n",
        "  return\n",
        "\n",
        "\n",
        "def evaluate_model(model, \n",
        "                   rl_env_name='LunarLander-v2', \n",
        "                   n_eval_episodes=25):\n",
        "  '''\n",
        "  Evaluates the model for a number of episodes in a specified environment, this environment MUST be the same as the one the model has been trained in.\n",
        "\n",
        "          Parameters:\n",
        "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
        "                  rl_env_name (str): The name of the gym environment where the model has been trained\n",
        "                  n_eval_episodes (int): The number of episodes for which the model will be evaluated to obtain a mean and standard deviation\n",
        "\n",
        "          Returns:\n",
        "                  lower_mean_reward (float): A tensor of size 1x1 (1 row, 1 column) containing the mean_reward\n",
        "  '''\n",
        "  eval_env = gym.make(rl_env_name)\n",
        "  mean_reward, std_reward = evaluate_policy(model, \n",
        "                                            eval_env, \n",
        "                                            n_eval_episodes=n_eval_episodes, \n",
        "                                            deterministic=True)\n",
        "  \n",
        "  print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n",
        "  lower_mean_reward = mean_reward - std_reward\n",
        "  return lower_mean_reward\n",
        "\n",
        "\n",
        "def target_function(hyperparams, \n",
        "                    timesteps=1000,\n",
        "                    rl_env_name='LunarLander-v2'):\n",
        "  '''\n",
        "  Given a hyperparameter configuration, evaluates their performance\n",
        "          Parameters:\n",
        "                  hyperparams (torch.DoubleTensor): A tensor of size 1x2 (1 row, n columns) with the learning rate and gamma to train the model with\n",
        "                  timesteps (int): timesteps (int): The number of timesteps used to train the model\n",
        "                  rl_env_name (str): The name of the gym environment where the model has been trained\n",
        "\n",
        "          Returns:\n",
        "                  lower_mean_reward (float): A tensor of size 1x1 (1 row, 1 column) containing the mean_reward\n",
        "  '''\n",
        "  model = create_model(hyperparams, env_name=rl_env_name)\n",
        "  \n",
        "  train_model(model, \n",
        "              timesteps)\n",
        "  \n",
        "  lower_mean_reward = evaluate_model(model, \n",
        "                                     rl_env_name)\n",
        "  \n",
        "  return lower_mean_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjdeNdB_jJPj"
      },
      "source": [
        "# Step 4: Define hyperparameters to tune\n",
        "First define the bounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WXwDQp5oilS4"
      },
      "outputs": [],
      "source": [
        "lr = 0.05\n",
        "gamma = 0.85\n",
        "# Define here the list of parameters to tune\n",
        "hyperparams_list = [lr, gamma]\n",
        "# Define the lower bounds of the parameters\n",
        "lower_bounds = [0.0001, 0.8]\n",
        "# Define the upper bounds of the parameters\n",
        "upper_bounds = [0.1, 0.9997]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ILw3cqAjkbg"
      },
      "source": [
        "Then convert lists to tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PnU1DMG8jnuc"
      },
      "outputs": [],
      "source": [
        "# Create tensors with the hyperparameters configurations and bounds for BOTorch to use\n",
        "hyperparams_tensor = torch.DoubleTensor([hyperparams_list])\n",
        "bounds_tensor = torch.DoubleTensor([lower_bounds, upper_bounds])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igatCqADkIjz"
      },
      "source": [
        "# Step 5: Define functions needed for the Bayesian Optimization Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VYL5gMk_kK7m"
      },
      "outputs": [],
      "source": [
        "def generate_initial_data(bounds, \n",
        "                          n=3):\n",
        "  '''\n",
        "  Gets n values of the hyperparameter's bounded space and evaluates them\n",
        "          Parameters:\n",
        "                bounds (torch.DoubleTensor): The torch tensor containing the upper and lower bounds of the hyperparameters (lr and gamma in this case)\n",
        "                n (int): The number of initial points to get. Default is 3\n",
        "          \n",
        "          Returns:\n",
        "                train_x (torch.DoubleTensor): A tensor of size (n, 1) (n rows and 1 column) with the initial points\n",
        "                exact_obj (torch.DoubleTensor): A tensor of size (n, 1) (n rows and 1 column) containing the evaluation of the model with the sampled hyperparameters values\n",
        "                best_observed_vale: The best evaluation of the hyperparameters\n",
        "  '''\n",
        "  # Create our initial hyperparameter values\n",
        "  lower_bounds = bounds[0]\n",
        "  upper_bounds = bounds[1]\n",
        "  train_x = torch.rand(n, len(lower_bounds), dtype=torch.double) * (upper_bounds - lower_bounds) + lower_bounds\n",
        "\n",
        "  # Evaluate them and store them in a torch.Tensor\n",
        "  exact_obj = torch.tensor([[target_function(hyp)] for hyp in train_x])\n",
        "\n",
        "  # Get the best observed value\n",
        "  best_observed_value = exact_obj.max().item()\n",
        "  \n",
        "  return train_x, exact_obj, best_observed_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1MUKzEZkmAxw"
      },
      "outputs": [],
      "source": [
        "from botorch.acquisition.analytic import ExpectedImprovement, UpperConfidenceBound\n",
        "from botorch.optim import optimize_acqf\n",
        "from botorch.utils.transforms import standardize, normalize, unnormalize\n",
        "from botorch.models import SingleTaskGP\n",
        "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
        "from botorch import fit_gpytorch_model\n",
        "  \n",
        "\n",
        "def get_next_points(init_x,\n",
        "                    init_y,\n",
        "                    best_init_y,\n",
        "                    normalized_bounds,\n",
        "                    n_points=1):\n",
        "  '''\n",
        "  Function that computes the next point to add to the Gaussian Process and visualizes the acquisition function and function distribution\n",
        "          Parameters:\n",
        "                  init_x (torch.Tensor): A tensor of shape {iterations}x2 containing the previous hyperparameters\n",
        "                  init_y (torch.Tensor): A tensor of shape {iterations}x1 containing the previous rewards of the models trained with the init_x hyperparameters values\n",
        "                  best_init_y (float): Best reward obtained until the moment\n",
        "                  normalized_bounds (torch.Tensor): Normalized bounds of the hyperparameter values in the form of tensors of shape 2x1 (2 rows, 1 column), first row containing lower bound, second containing upper bound\n",
        "                  n_points (int): Number of candidates to obtain for the next iteration. Default is 1\n",
        "\n",
        "          Returns:\n",
        "                  candidates (torch.Tensor): A tensor of shape 1x2 containing the value of the hyperparameters that optimizes the acquisition function\n",
        "  '''\n",
        "  # Create our probabilistic model with the points\n",
        "  single_model = SingleTaskGP(init_x, init_y)\n",
        "  mll = ExactMarginalLogLikelihood(single_model.likelihood, \n",
        "                                   single_model)\n",
        "  # Fit our model\n",
        "  fit_gpytorch_model(mll)\n",
        "\n",
        "  # Instantiate the acquisition function given our model\n",
        "  UCB = UpperConfidenceBound(model=single_model,\n",
        "                             beta=0.2,\n",
        "                             maximize=True)\n",
        "  \n",
        "  # Maximize the acquisition function to obtain our candidates \n",
        "  candidates, _ = optimize_acqf(acq_function=UCB, \n",
        "                                bounds=normalized_bounds,\n",
        "                                q=n_points, num_restarts=200,\n",
        "                                raw_samples=512,\n",
        "                                options={\"batch_limit\": 5, \"maxiter\": 200})\n",
        "\n",
        "  return candidates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E038wSN_MFoQ"
      },
      "source": [
        "# Step 6: Set experiments' configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWEEF07XuQPj"
      },
      "source": [
        "First let us define functions to save and load checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "8sde3AjauPiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b34347-9962-4ea9-ba28-ac2676b74c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "def create_experiment_df():\n",
        "  '''\n",
        "  Creates an empty dataframe to save checkpoints\n",
        "          Parameters:\n",
        "\n",
        "          Returns:\n",
        "                  experiment_df (pandas.DataFrame): An empty Dataframe with columns specified below that will be used to save the experiment history\n",
        "  '''\n",
        "  columns = [\"method\",\n",
        "             \"experiment\",\n",
        "             \"iteration\",\n",
        "             \"learning_rate\",\n",
        "             \"gamma\",\n",
        "             \"reward_lower_bound\",\n",
        "             \"best_learning_rate\",\n",
        "             \"best_gamma\",\n",
        "             \"best_reward_lower_bound\"\n",
        "             ]\n",
        "\n",
        "  experiment_df = pd.DataFrame(columns=columns)\n",
        "  return experiment_df\n",
        "\n",
        "\n",
        "def get_filepath(experiment_name):\n",
        "  '''\n",
        "  Returns the path of the csv of the experiment specified.\n",
        "          Parameters:\n",
        "                  experiment_name (string): The name of the experiment\n",
        "\n",
        "          Returns:\n",
        "                  filepath (string): The path to the .csv file that has the data of the experiment\n",
        "  '''\n",
        "  filepath = f\"/content/gdrive/My Drive/{experiment_name}.csv\"\n",
        "  return filepath\n",
        "\n",
        "\n",
        "def update_experiment_history(method, \n",
        "                              experiment_number, \n",
        "                              iteration,\n",
        "                              lr,\n",
        "                              gamma,\n",
        "                              reward_lower_bound,\n",
        "                              best_lr,\n",
        "                              best_gamma,\n",
        "                              best_reward_lower_bound,\n",
        "                              experiment_df,\n",
        "                              experiment_name):\n",
        "  '''\n",
        "  Updates the experiment dataframe and saves it in GDrive\n",
        "          Parameters:\n",
        "                  method (int): 0 if Bayesian Optimization, 1 if Random Search\n",
        "                  experiment_number (int): The id of the experiment\n",
        "                  iteration (int): The current iteration of the experiment\n",
        "                  lr (float): The learning rate value selected for this iteration\n",
        "                  gamma (float): The gamma value selected for this iteration\n",
        "                  reward_lower_bound (float): The reward's lower bound obtained by the model trained this iteration\n",
        "                  best_lr (float): The learning rate value that has induced the best reward lower bound\n",
        "                  best_gamma (float): The gamma value that has induced the best reward lower bound\n",
        "                  best_reward_lower_bound (float): The best reward lower bound obtained until this iteration\n",
        "                  experiment_df (pandas.DataFrame): The dataframe containing the experiments history\n",
        "                  experiment_name (string): The name of the experiment\n",
        "\n",
        "          Returns:\n",
        "                  concatenated_df (pandas.DataFrame): The updated experiments history dataframe\n",
        "  '''\n",
        "  # Define the columns of the dataframe\n",
        "  columns = [\"method\",\n",
        "             \"experiment\",\n",
        "             \"iteration\",\n",
        "             \"learning_rate\",\n",
        "             \"gamma\",\n",
        "             \"reward_lower_bound\",\n",
        "             \"best_learning_rate\",\n",
        "             \"best_gamma\",\n",
        "             \"best_reward_lower_bound\"]\n",
        "\n",
        "  # Set the values of the columns given by the iteration configuration and result\n",
        "  iteration_results = [[method,\n",
        "                        experiment_number,\n",
        "                        iteration,\n",
        "                        lr,\n",
        "                        gamma,\n",
        "                        reward_lower_bound,\n",
        "                        best_lr,\n",
        "                        best_gamma,\n",
        "                        best_reward_lower_bound]]\n",
        "\n",
        "  # Create a one row dataframe for this experiment\n",
        "  new_iteration_df = pd.DataFrame(iteration_results, columns=columns)\n",
        "\n",
        "  # Concatenate the experiments history with this experiment's results\n",
        "  concatenated_df = pd.concat([experiment_df, new_iteration_df], ignore_index=True)\n",
        "\n",
        "  # Save the updated history in google drive\n",
        "  save_checkpoint(concatenated_df,\n",
        "                  experiment_name)\n",
        "  \n",
        "  # Return the concatenated dataframe representing the updated experiment history\n",
        "  return concatenated_df\n",
        "\n",
        "\n",
        "def save_checkpoint(experiment_df,\n",
        "                    experiment_name):\n",
        "  '''\n",
        "  Saves the experiments history dataframe in google drive\n",
        "          Parameters:\n",
        "                  experiment_df (pandas.DataFrame): The dataframe containing the experiments history\n",
        "                  experiment_name (string): The name of the experiment\n",
        "          \n",
        "          Returns:\n",
        "                  None\n",
        "  '''\n",
        "\n",
        "  filepath = get_filepath(experiment_name)\n",
        "  experiment_df.to_csv(filepath, index=False)\n",
        "\n",
        "\n",
        "def load_checkpoint(experiment_name,\n",
        "                    experiment_results,\n",
        "                    experiment_configurations):\n",
        "  '''\n",
        "  Loads a checkpoint of an experiment given its name\n",
        "\n",
        "          Parameters:\n",
        "                  experiment_name (string): The name of the experiment\n",
        "                  experiment_results (numpy.array): A numpy array of three dimensions (method, iteration, best_result)\n",
        "                  expeirment_configuration (numpy.array): A numpy array of three dimensions (method, iteration, best_learning rate)\n",
        "\n",
        "          Returns:\n",
        "                  experiment_df (pandas.DataFrame): A dataframe with the experiment history\n",
        "  '''\n",
        "  # First we retrieve the dataframe from GDrive\n",
        "  filepath = get_filepath(experiment_name)\n",
        "  experiment_df = pd.read_csv(filepath)\n",
        "\n",
        "  # Now we iterate through the rows of the dataframe to update the experiment history numpy arrays that we will use later to compare the methods and plot results\n",
        "  for index, row in experiment_df.iterrows():\n",
        "      # Unpack the columns\n",
        "      method, exp, iter, lr, gamma, rlb, best_lr, best_gamma, best_rlb = row.values\n",
        "      # Add them to the experiments arrays\n",
        "      experiment_results[int(method)][int(exp)][int(iter)] = best_rlb\n",
        "      experiment_configurations[int(method)][int(exp)][int(iter)][0] = best_lr\n",
        "      experiment_configurations[int(method)][int(exp)][int(iter)][1] = best_gamma\n",
        "  \n",
        "\n",
        "  method, exp, iter, lr, gamma, rlb, best_lr, best_gamma, best_rlb = experiment_df.iloc[-1]\n",
        "\n",
        "  if method == 0:\n",
        "    bo_done = False\n",
        "    last_bo_experiment = int(exp)\n",
        "    last_rs_experiment = 0\n",
        "\n",
        "    # Plus one because we want to start in the next one\n",
        "    last_bo_iteration = int(iter)+1\n",
        "    last_rs_iteration = 1\n",
        "\n",
        "  else:\n",
        "    bo_done = True\n",
        "    last_bo_experiment = experiment_configurations.shape[1]-1\n",
        "    last_rs_experiment = int(exp)\n",
        "\n",
        "    # Plus one because we want to start in the next one\n",
        "    last_bo_iteration = experiment_configurations.shape[2]-1\n",
        "    last_rs_iteration = int(iter)+1\n",
        "\n",
        "  # Now lets get the initial data\n",
        "  bo_experiment_df = experiment_df[(experiment_df[\"method\"]==method) & (experiment_df[\"experiment\"]==int(exp))]\n",
        "\n",
        "  init_x = torch.DoubleTensor([[float(lr), float(gamma)] for (lr, gamma) in zip(bo_experiment_df.learning_rate.values, bo_experiment_df.gamma.values)])\n",
        "\n",
        "  init_y = torch.DoubleTensor([[float(reward)] for reward in bo_experiment_df.reward_lower_bound.values])\n",
        "  best_init_y = init_y.max().item()\n",
        "\n",
        "  rs_experiment_df = experiment_df[experiment_df[\"method\"]==0]\n",
        "  if rs_experiment_df.empty:\n",
        "      best_rs_lr = 0\n",
        "      best_rs_gamma = 0\n",
        "      best_rs_r = 0\n",
        "  else:\n",
        "      best_rs_lr = rs_experiment_df.iloc[-1][\"best_learning_rate\"]\n",
        "      best_rs_gamma = rs_experiment_df.iloc[-1][\"best_gamma\"]\n",
        "      best_rs_r = rs_experiment_df.iloc[-1][\"best_reward_lower_bound\"]\n",
        "             \n",
        "  return experiment_df, last_bo_experiment, last_rs_experiment, last_bo_iteration, last_rs_iteration, init_x, init_y, best_init_y, best_rs_lr, best_rs_gamma, best_rs_r, bo_done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AChh4uxu6lk"
      },
      "source": [
        "Now let's set the configuration for the experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "FF5-qYTvN2zZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# The name of the file (WITHOUT EXTENSION) where the history of experiments will be saved\n",
        "experiment_name = \"lunar_lander_learning_rate_gamma\"\n",
        "# If true, this will look for the experiment history .csv in google drive and continue from there\n",
        "continue_from_checkpoint = False\n",
        "\n",
        "# Number of experiments per method\n",
        "n_experiments = 15\n",
        "\n",
        "# Number of iterations per experiment after the first random point being evaluated\n",
        "n_iterations = 25\n",
        "\n",
        "# Number of methods\n",
        "n_methods = 2\n",
        "\n",
        "# Index of Bayesian Optimization method\n",
        "bo_method = 0\n",
        "\n",
        "# Index of Random Search method\n",
        "rs_method = 1\n",
        "\n",
        "# Number of Hyperparameters\n",
        "n_hyperparameters = 2\n",
        "\n",
        "# Arrays containing the results and configurations of experiments\n",
        "experiment_results = np.zeros((n_methods, n_experiments, n_iterations+1))\n",
        "experiment_configurations = np.zeros((n_methods, n_experiments, n_iterations+1, len(bounds_tensor[0])))\n",
        "\n",
        "# Now load checkpoint if necessary\n",
        "if continue_from_checkpoint:\n",
        "  experiment_df, last_bo_experiment, last_rs_experiment, last_bo_iteration, last_rs_iteration, init_x, init_y, best_init_y, best_observed_lr_rs, best_observed_gamma_rs, best_observed_result_rs, bo_done = load_checkpoint(experiment_name,\n",
        "                                            experiment_results,\n",
        "                                            experiment_configurations) \n",
        "else: \n",
        "  experiment_df = create_experiment_df()\n",
        "  \n",
        "  save_checkpoint(experiment_df, \n",
        "                  experiment_name)\n",
        "\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHTBIpJSQJVZ"
      },
      "source": [
        "First we try the bayesian optimization method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "6jScxWwkmzEr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "534dac6a-c9cc-4219-da58-89fccae96346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPERIMENT 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-370.17 +/- 152.39812881547405\n",
            "Number of iteration: 10\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -522.567667458648\n",
            "  - Standardized lower reward bound: 0.5767006893193111\n",
            "Best point performs this way: -291.26085304130646\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-309.83 +/- 159.99221719414857\n",
            "Number of iteration: 11\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -469.8269841861811\n",
            "  - Standardized lower reward bound: 0.6302442221587514\n",
            "Best point performs this way: -291.26085304130646\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-255.06 +/- 70.33610783215379\n",
            "Number of iteration: 12\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -325.39769017561804\n",
            "  - Standardized lower reward bound: 0.8305849857814376\n",
            "Best point performs this way: -291.26085304130646\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-207.52 +/- 116.27536441930297\n",
            "Number of iteration: 13\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -323.7949934737776\n",
            "  - Standardized lower reward bound: 0.7847469936193782\n",
            "Best point performs this way: -291.26085304130646\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-188.92 +/- 95.49066462247679\n",
            "Number of iteration: 14\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -284.41227948169734\n",
            "  - Standardized lower reward bound: 0.8092600219218421\n",
            "Best point performs this way: -284.41227948169734\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-354.73 +/- 106.02161411765722\n",
            "Number of iteration: 15\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -460.7471825940511\n",
            "  - Standardized lower reward bound: 0.46297552131011677\n",
            "Best point performs this way: -284.41227948169734\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-97.75 +/- 37.435786638910656\n",
            "Number of iteration: 16\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -135.18141418692073\n",
            "  - Standardized lower reward bound: 1.018570453602593\n",
            "Best point performs this way: -135.18141418692073\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-903.53 +/- 498.5294006721949\n",
            "Number of iteration: 17\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -1402.0578680345363\n",
            "  - Standardized lower reward bound: -1.2880846520045595\n",
            "Best point performs this way: -135.18141418692073\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-167.81 +/- 145.66843362056815\n",
            "Number of iteration: 18\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -313.48003281465435\n",
            "  - Standardized lower reward bound: 0.7088378933578643\n",
            "Best point performs this way: -135.18141418692073\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-129.02 +/- 46.13525844898004\n",
            "Number of iteration: 19\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -175.15394861024967\n",
            "  - Standardized lower reward bound: 0.9239371085904982\n",
            "Best point performs this way: -135.18141418692073\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-984.67 +/- 520.7733089182417\n",
            "Number of iteration: 20\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -1505.4392533928085\n",
            "  - Standardized lower reward bound: -1.4662327151777366\n",
            "Best point performs this way: -135.18141418692073\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-128.85 +/- 43.30957770350484\n",
            "Number of iteration: 21\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -172.16359975147623\n",
            "  - Standardized lower reward bound: 0.9296485629777198\n",
            "Best point performs this way: -135.18141418692073\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-565.58 +/- 130.0769740424854\n",
            "Number of iteration: 22\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -695.661477197985\n",
            "  - Standardized lower reward bound: -0.029187177678582022\n",
            "Best point performs this way: -135.18141418692073\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-634.28 +/- 72.32224580570767\n",
            "Number of iteration: 23\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -706.6023423911416\n",
            "  - Standardized lower reward bound: -0.048707726546643855\n",
            "Best point performs this way: -135.18141418692073\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-130.79 +/- 82.1327323131419\n",
            "Number of iteration: 24\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -212.92329737919079\n",
            "  - Standardized lower reward bound: 0.8662943206274962\n",
            "Best point performs this way: -135.18141418692073\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-385.55 +/- 93.73010076921614\n",
            "Number of iteration: 25\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -479.2773302564183\n",
            "  - Standardized lower reward bound: 0.34558547263310907\n",
            "Best point performs this way: -135.18141418692073\n",
            "----------------------\n",
            "EXPERIMENT 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-234.47 +/- 61.99192125905865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-869.00 +/- 840.9369881605359\n",
            "Number of iteration: 1\n",
            "  - Unnormalized learning rate: 0.0001\n",
            "  - Normalized learning rate: 0.0\n",
            "  - Unnormalized gamma: 0.8\n",
            "  - Normalized gamma: 0.0\n",
            "  - Unstandardized lower reward bound: -1709.9355579120597\n",
            "  - Standardized lower reward bound: -0.7071067811865476\n",
            "Best point performs this way: -296.46408132118734\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-898.45 +/- 555.5035590287713\n",
            "Number of iteration: 2\n",
            "  - Unnormalized learning rate: 0.09332506821681877\n",
            "  - Normalized learning rate: 0.9331838660342219\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -1453.9527159777513\n",
            "  - Standardized lower reward bound: -0.3990052373580794\n",
            "Best point performs this way: -296.46408132118734\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-139.61 +/- 42.54071724824797\n",
            "Number of iteration: 3\n",
            "  - Unnormalized learning rate: 0.05759549141295583\n",
            "  - Normalized learning rate: 0.5755304445741324\n",
            "  - Unnormalized gamma: 0.9965683200495837\n",
            "  - Normalized gamma: 0.9843180773639646\n",
            "  - Unstandardized lower reward bound: -182.151867370924\n",
            "  - Standardized lower reward bound: 0.9296802901435415\n",
            "Best point performs this way: -182.151867370924\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-327.27 +/- 126.4985426292129\n",
            "Number of iteration: 4\n",
            "  - Unnormalized learning rate: 0.057598430004276994\n",
            "  - Normalized learning rate: 0.5755598599026726\n",
            "  - Unnormalized gamma: 0.9805360847218609\n",
            "  - Normalized gamma: 0.9040364783267949\n",
            "  - Unstandardized lower reward bound: -453.7728453996897\n",
            "  - Standardized lower reward bound: 0.515718929692551\n",
            "Best point performs this way: -182.151867370924\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-241.68 +/- 118.33260887629841\n",
            "Number of iteration: 5\n",
            "  - Unnormalized learning rate: 0.06305392847332601\n",
            "  - Normalized learning rate: 0.6301694541874475\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -360.0102971814994\n",
            "  - Standardized lower reward bound: 0.5789656394032057\n",
            "Best point performs this way: -182.151867370924\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-547.09 +/- 71.10552824685712\n",
            "Number of iteration: 6\n",
            "  - Unnormalized learning rate: 0.06176902325368376\n",
            "  - Normalized learning rate: 0.6173075400769145\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -618.1952456330172\n",
            "  - Standardized lower reward bound: 0.17634091383205797\n",
            "Best point performs this way: -182.151867370924\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-134.74 +/- 26.220384901007375\n",
            "Number of iteration: 7\n",
            "  - Unnormalized learning rate: 0.05510994691990677\n",
            "  - Normalized learning rate: 0.550650119318386\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -160.95991418283876\n",
            "  - Standardized lower reward bound: 0.829678170587966\n",
            "Best point performs this way: -160.95991418283876\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-134.05 +/- 38.93293847047995\n",
            "Number of iteration: 8\n",
            "  - Unnormalized learning rate: 0.04672775969751396\n",
            "  - Normalized learning rate: 0.466744341316456\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -172.97871727991188\n",
            "  - Standardized lower reward bound: 0.7390762033504434\n",
            "Best point performs this way: -160.95991418283876\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-244.51 +/- 164.10381995104018\n",
            "Number of iteration: 9\n",
            "  - Unnormalized learning rate: 0.041533809692776216\n",
            "  - Normalized learning rate: 0.4147528497775397\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -408.6116324745235\n",
            "  - Standardized lower reward bound: 0.3151106827682981\n",
            "Best point performs this way: -160.95991418283876\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-551.68 +/- 141.45833702623452\n",
            "Number of iteration: 10\n",
            "  - Unnormalized learning rate: 0.05110577856229751\n",
            "  - Normalized learning rate: 0.510568353976952\n",
            "  - Unnormalized gamma: 0.9997\n",
            "  - Normalized gamma: 1.0\n",
            "  - Unstandardized lower reward bound: -693.1419764105937\n",
            "  - Standardized lower reward bound: -0.19400287981975473\n",
            "Best point performs this way: -160.95991418283876\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/botorch/fit.py:139: DeprecationWarning: `fit_gpytorch_model` is marked for deprecation, consider using `fit_gpytorch_mll` instead.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-473cfeef226f>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0;31m# Compute the performance of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m       \u001b[0mnew_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_candidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0;31m# Update our hyperparameters and rewards history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-a754413b2184>\u001b[0m in \u001b[0;36mtarget_function\u001b[0;34m(hyperparams, timesteps, rl_env_name)\u001b[0m\n\u001b[1;32m    103\u001b[0m               timesteps)\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m   lower_mean_reward = evaluate_model(model, \n\u001b[0m\u001b[1;32m    106\u001b[0m                                      rl_env_name)\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-a754413b2184>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, rl_env_name, n_eval_episodes)\u001b[0m\n\u001b[1;32m     75\u001b[0m   '''\n\u001b[1;32m     76\u001b[0m   \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_env_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m   mean_reward, std_reward = evaluate_policy(model, \n\u001b[0m\u001b[1;32m     78\u001b[0m                                             \u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                                             \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mepisode_starts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode_counts\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepisode_count_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         actions, states = model.predict(\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecurrent\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \"\"\"\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;31m# Convert to numpy, and reshape to the original action shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTaken\u001b[0m \u001b[0maction\u001b[0m \u001b[0maccording\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \"\"\"\n\u001b[0;32m--> 679\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    711\u001b[0m         \"\"\"\n\u001b[1;32m    712\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi_features_extractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         \u001b[0mlatent_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/torch_layers.py\u001b[0m in \u001b[0;36mforward_actor\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if continue_from_checkpoint:\n",
        "  init_experiment = last_bo_experiment\n",
        "  init_iteration = last_bo_iteration\n",
        "else:\n",
        "  init_experiment = 0\n",
        "  init_iteration = 1\n",
        "  bo_done = False\n",
        "\n",
        "if not bo_done:\n",
        "  for e in range(init_experiment, n_experiments):\n",
        "    print(f\"EXPERIMENT {e}\")\n",
        "    if not (continue_from_checkpoint and init_experiment == e):\n",
        "      # Sample initial hyperparameter values and evaluate the models obtained with them\n",
        "      init_x, init_y, best_init_y = generate_initial_data(bounds_tensor,\n",
        "                                                          1)\n",
        "\n",
        "    # We normalize the bounds of the hyperparameters as BOTorch assumes this\n",
        "    normalized_bounds = torch.tensor([np.zeros(len(bounds_tensor[0])), np.ones(len(bounds_tensor[0]))])\n",
        "\n",
        "    # Normalize the hyperparameter as BOTorch assumes this\n",
        "    init_x_normalized = normalize(init_x,\n",
        "                                  bounds=bounds_tensor)\n",
        "\n",
        "    # Standardize the objective as BOTorch assumes this\n",
        "    init_y_standardized = standardize(init_y)\n",
        "\n",
        "    # Obtain the best result among the initial random experiments\n",
        "    best_init_y_standardized = init_y_standardized.max().item()\n",
        "\n",
        "    best_observed_result_bo = best_init_y\n",
        "    best_observed_lr_bo, best_observed_gamma_bo = get_hyp_values(init_x[0])\n",
        "\n",
        "    if not (continue_from_checkpoint and init_experiment == e):\n",
        "      experiment_df = update_experiment_history(bo_method, \n",
        "                                                e, \n",
        "                                                0,\n",
        "                                                best_observed_lr_bo, # The learning rate selected for this iteration\n",
        "                                                best_observed_gamma_bo, # The gamma selected for this iteration\n",
        "                                                best_observed_result_bo, # The reward lower bound of the model\n",
        "                                                best_observed_lr_bo, # The best learning rate\n",
        "                                                best_observed_gamma_bo, # The best gamma\n",
        "                                                best_observed_result_bo, # The reward lower bound\n",
        "                                                experiment_df,\n",
        "                                                experiment_name)\n",
        "      \n",
        "      experiment_configurations[rs_method,e,0,0] = best_observed_lr_bo\n",
        "      experiment_configurations[rs_method,e,0,1] = best_observed_gamma_bo\n",
        "      experiment_results[rs_method,e,0] = best_observed_result_bo\n",
        "\n",
        "    for i in range(init_iteration, n_iterations+1):\n",
        "      # Get the next points given our actual queries\n",
        "      normalized_new_candidates = get_next_points(init_x_normalized,\n",
        "                                                  init_y_standardized, \n",
        "                                                  best_init_y_standardized, \n",
        "                                                  normalized_bounds,\n",
        "                                                  n_points=1)\n",
        "    \n",
        "      # Unnormalize the candidate hyperparameter value\n",
        "      new_candidates = unnormalize(normalized_new_candidates,\n",
        "                                   bounds=bounds_tensor)\n",
        "      \n",
        "      # Compute the performance of the model\n",
        "      new_results = torch.tensor([[target_function(new_candidates[0])]])\n",
        "\n",
        "      # Update our hyperparameters and rewards history\n",
        "      init_x = torch.cat([init_x, new_candidates])\n",
        "      init_y = torch.cat([init_y, new_results])\n",
        "\n",
        "      # Normalize our updated hyperparameters and rewards history\n",
        "      init_x_normalized = normalize(init_x, bounds=bounds_tensor)\n",
        "      init_y_standardized = standardize(init_y)\n",
        "\n",
        "      # Update the best reward\n",
        "      best_init_y = init_y.max().item()\n",
        "      best_init_y_standardized = init_y_standardized.max().item()\n",
        "      \n",
        "      # Show iteration info\n",
        "      \n",
        "      print(f\"Number of iteration: {i}\")\n",
        "      print(f\"  - Unnormalized learning rate: {new_candidates[0][0].item()}\")\n",
        "      print(f\"  - Normalized learning rate: {normalized_new_candidates[0][0].item()}\")\n",
        "      print(f\"  - Unnormalized gamma: {new_candidates[0][1].item()}\")\n",
        "      print(f\"  - Normalized gamma: {normalized_new_candidates[0][1].item()}\")\n",
        "      print(f\"  - Unstandardized lower reward bound: {new_results.item()}\")\n",
        "      print(f\"  - Standardized lower reward bound: {init_y_standardized[-1].item()}\")\n",
        "      print(f\"Best point performs this way: {best_init_y}\")\n",
        "\n",
        "\n",
        "      if best_observed_result_bo < new_results[0][0].item():\n",
        "        best_observed_result_bo = new_results[0][0].item()\n",
        "        best_observed_lr_bo, best_observed_gamma_bo = get_hyp_values(new_candidates[0])\n",
        "\n",
        "      experiment_df = update_experiment_history(bo_method, \n",
        "                                                e, \n",
        "                                                i,\n",
        "                                                new_candidates[0][0].item(), # The lr selected for this iteration\n",
        "                                                new_candidates[0][1].item(), # The gamma selected for this iteration\n",
        "                                                new_results[0][0].item(), # The reward lower bound of the model\n",
        "                                                best_observed_lr_bo, # The best learning rate\n",
        "                                                best_observed_gamma_bo, # The best gamma\n",
        "                                                best_observed_result_bo, # The reward lower bound\n",
        "                                                experiment_df,\n",
        "                                                experiment_name)\n",
        "\n",
        "      experiment_configurations[bo_method,e,i,0] = best_observed_lr_bo\n",
        "      experiment_configurations[bo_method,e,i,1] = best_observed_gamma_bo\n",
        "      experiment_results[bo_method,e,i] = best_observed_result_bo\n",
        "      print('----------------------')\n",
        "    init_iteration = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kbrnek5JW9f"
      },
      "source": [
        "Now we perform a random search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "8vpuRYDRJWmf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "outputId": "003bc8ab-f2b8-4881-bbb1-042b4cc12d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-968.07 +/- 661.3927538572593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-504.82 +/- 114.45941114114414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-369.68 +/- 202.39413972549102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-205.03 +/- 51.11990776244808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-126.23 +/- 44.20861411344061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=-599.71 +/- 156.05074993114113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-0d41318a930a>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mrandom_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounds_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbounds_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbounds_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbounds_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Evaluate the model with that hyperparameter value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mrs_obj_fun_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Update best reward and candidate found if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-a754413b2184>\u001b[0m in \u001b[0;36mtarget_function\u001b[0;34m(hyperparams, timesteps, rl_env_name)\u001b[0m\n\u001b[1;32m    103\u001b[0m               timesteps)\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m   lower_mean_reward = evaluate_model(model, \n\u001b[0m\u001b[1;32m    106\u001b[0m                                      rl_env_name)\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-a754413b2184>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, rl_env_name, n_eval_episodes)\u001b[0m\n\u001b[1;32m     75\u001b[0m   '''\n\u001b[1;32m     76\u001b[0m   \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_env_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m   mean_reward, std_reward = evaluate_policy(model, \n\u001b[0m\u001b[1;32m     78\u001b[0m                                             \u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                                             \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mepisode_starts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode_counts\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepisode_count_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         actions, states = model.predict(\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecurrent\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \"\"\"\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \"\"\"\n\u001b[1;32m    343\u001b[0m         \u001b[0;31m# Switch to eval mode (this affects batch norm / dropout)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_training_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorized_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mset_training_mode\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mset\u001b[0m \u001b[0mto\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \"\"\"\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_vectorized_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2285\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training mode is expected to be boolean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2286\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2287\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2288\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1627\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1628\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m                 raise AttributeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parameter.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_ParameterMeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TensorMeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Make `isinstance(t, Parameter)` return True for custom tensor instances that have the _is_param flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         return super().__instancecheck__(instance) or (\n\u001b[1;32m     10\u001b[0m             isinstance(instance, torch.Tensor) and getattr(instance, '_is_param', False))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if continue_from_checkpoint:\n",
        "  init_experiment = last_rs_experiment\n",
        "  init_iteration = last_rs_iteration\n",
        "else:\n",
        "  init_experiment = 0\n",
        "  init_iteration = 1\n",
        "\n",
        "for e in range(init_experiment, n_experiments):\n",
        "  if not (continue_from_checkpoint and init_experiment == e) or (init_experiment==0 and init_iteration==1):\n",
        "    # Initiate with a random value\n",
        "    random_value = torch.rand(1, len(bounds_tensor[0])) * (bounds_tensor[1] - bounds_tensor[0]) + bounds_tensor[0]\n",
        "    best_observed_result_rs = target_function(random_value[0])\n",
        "    best_observed_lr_rs, best_observed_gamma_rs = get_hyp_values(random_value[0])\n",
        "    # Update our experiments histories\n",
        "    experiment_df = update_experiment_history(rs_method, \n",
        "                                              e, \n",
        "                                              0,\n",
        "                                              best_observed_lr_rs, # The learning rate value selected for this iteration\n",
        "                                              best_observed_gamma_rs, # The gamma value selected for this iteration\n",
        "                                              best_observed_result_rs, # The reward lower bound of the model\n",
        "                                              best_observed_lr_rs, # The best learning rate\n",
        "                                              best_observed_gamma_rs, # The best gamma\n",
        "                                              best_observed_result_rs, # The reward lower bound\n",
        "                                              experiment_df,\n",
        "                                              experiment_name)\n",
        "  \n",
        "  # Iterate with random search\n",
        "  for i in range(init_iteration, n_iterations+1):\n",
        "    # Get a new random value for the hyperparameter\n",
        "    random_value = torch.rand(1, len(bounds_tensor[0])) * (bounds_tensor[1] - bounds_tensor[0]) + bounds_tensor[0]\n",
        "    # Evaluate the model with that hyperparameter value\n",
        "    rs_obj_fun_result = target_function(random_value[0])\n",
        "\n",
        "    # Update best reward and candidate found if necessary\n",
        "    if best_observed_result_rs < rs_obj_fun_result:\n",
        "      best_observed_result_rs = rs_obj_fun_result\n",
        "      best_observed_lr_rs, best_observed_gamma_rs = get_hyp_values(random_value[0])\n",
        "    \n",
        "    # Update our experiments histories\n",
        "    experiment_df = update_experiment_history(rs_method, \n",
        "                                              e, \n",
        "                                              i,\n",
        "                                              best_observed_lr_rs, # The learning rate value selected for this iteration\n",
        "                                              best_observed_gamma_rs, # The gamma value selected for this iteration\n",
        "                                              best_observed_result_rs, # The reward lower bound of the model\n",
        "                                              best_observed_lr_rs, # The best learning rate\n",
        "                                              best_observed_gamma_rs, # The best gamma\n",
        "                                              best_observed_result_rs, # The reward lower bound\n",
        "                                              experiment_df,\n",
        "                                              experiment_name)\n",
        "    \n",
        "    experiment_configurations[rs_method,e,i,0] = best_observed_lr_rs\n",
        "    experiment_configurations[rs_method,e,i,1] = best_observed_gamma_rs\n",
        "    experiment_results[rs_method,e,i] = best_observed_result_rs\n",
        "\n",
        "  init_iteration = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukMdAAapSR2c"
      },
      "source": [
        "# Step 7: Compare the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeN8EWoOSVxC"
      },
      "source": [
        "First we give the recommendation as the best observed result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlf2cyT-SZf2"
      },
      "outputs": [],
      "source": [
        "best_observed_result = np.max(experiment_results)\n",
        "index_set = np.where(experiment_results==best_observed_result)\n",
        "print(\"The best observed result is: \" + str(best_observed_result))\n",
        "print(\"The best observed result belong to the : \" + str(index_set[0][0]) + \" method. Its value is \" + str(experiment_configurations[index_set][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jZ5oFK6Sf3Q"
      },
      "source": [
        "And now we plot the results to compare both methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgo_wr7VSj7k"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(1, n_iterations, n_iterations).astype(int)\n",
        "mean_bo = np.mean(experiment_results[0,:,:], axis=0)\n",
        "mean_rs = np.mean(experiment_results[1,:,:], axis=0)\n",
        "std_bo = np.std(experiment_results[0,:,:], axis=0) * 0.25\n",
        "std_rs = np.std(experiment_results[1,:,:], axis=0) * 0.25\n",
        "bo_ub_results = go.Scatter(x=x, y=mean_bo + std_bo, mode='lines', name=\"\", line_color=\"green\", line_width=0.1)\n",
        "bo_results = go.Scatter(x=x, y=mean_bo, mode='lines', fill='tonexty', line_color=\"green\", name=\"Bayesian Optimization\")\n",
        "bo_lb_results = go.Scatter(x=x, y=mean_bo - std_bo, mode='lines', fill='tonexty', name=\"\", line_color=\"green\", line_width=0.1)\n",
        "\n",
        "rs_ub_results = go.Scatter(x=x, y=mean_rs + std_rs, mode='lines', name=\"\", line_color=\"red\", line_width=0.1)\n",
        "rs_results = go.Scatter(x=x, y=mean_rs, mode='lines', fill='tonexty', line_color=\"red\", name=\"Random Search\")\n",
        "rs_lb_results = go.Scatter(x=x, y=mean_rs - std_rs, mode='lines', fill='tonexty', name=\"\", line_color=\"red\", line_width=0.1)\n",
        "  \n",
        "fig = go.Figure()\n",
        "fig.add_trace(bo_ub_results)\n",
        "fig.add_trace(bo_results)\n",
        "fig.add_trace(bo_lb_results)\n",
        "fig.add_trace(rs_ub_results)\n",
        "fig.add_trace(rs_results)\n",
        "fig.add_trace(rs_lb_results)\n",
        "fig.update_layout(title=\"Performance comparison between BO and RS\", xaxis_title=\"Iterations\", yaxis_title=\"Reward lower bound\")\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}