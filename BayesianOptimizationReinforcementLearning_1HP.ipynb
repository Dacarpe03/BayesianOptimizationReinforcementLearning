{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR0o63R4Xa6F"
      },
      "source": [
        "# Step 1: Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQXJ4h82Xt42",
        "outputId": "83d3f561-638b-4a92-88ec-5a7f12174024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (1.22.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting box2d-py==2.3.5\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting swig==4.*\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.15.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "Installing collected packages: swig, box2d-py, pygame\n",
            "  Running setup.py install for box2d-py ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: box2d-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed box2d-py-2.3.5 pygame-2.1.0 swig-4.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-1.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.8/171.8 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata~=4.13\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (1.22.4)\n",
            "Collecting gym==0.21\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (1.3.5)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (1.13.1+cu116)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (3.5.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (4.6.0.66)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (8.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (2.11.2)\n",
            "Collecting rich\n",
            "  Downloading rich-13.3.2-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.7/238.7 KB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (4.64.1)\n",
            "Collecting ale-py==0.7.4\n",
            "  Downloading ale_py-0.7.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (5.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (8.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.25.1)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.5.5.tar.gz (22 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata~=4.13->stable-baselines3[extra]) (3.15.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.19.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.2.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.38.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.51.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.11->stable-baselines3[extra]) (4.5.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3[extra]) (4.38.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3[extra]) (23.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->stable-baselines3[extra]) (2022.7.1)\n",
            "Collecting markdown-it-py<3.0.0,>=2.2.0\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments<3.0.0,>=2.13.0\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.26.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.2)\n",
            "Collecting libtorrent\n",
            "  Using cached libtorrent-2.0.7-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (8.6 MB)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616821 sha256=82bf0b5296ed97576a251dc6b998e00a487f02536e3bef0b212ee6d08882fee6\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/6d/b3/a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.5.5-py3-none-any.whl size=441098 sha256=2bd9552ffe42e77bc67d43c8e84bb426eceb003a372038461a7e1a48df9dc7e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/86/6f/e96885ff274388b9f0636418a2926f46f076cd7e891670321d\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: libtorrent, pygments, mdurl, importlib-metadata, gym, markdown-it-py, AutoROM.accept-rom-license, autorom, ale-py, stable-baselines3, rich\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.0.0\n",
            "    Uninstalling importlib-metadata-6.0.0:\n",
            "      Successfully uninstalled importlib-metadata-6.0.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed AutoROM.accept-rom-license-0.5.5 ale-py-0.7.4 autorom-0.4.2 gym-0.21.0 importlib-metadata-4.13.0 libtorrent-2.0.7 markdown-it-py-2.2.0 mdurl-0.1.2 pygments-2.14.0 rich-13.3.2 stable-baselines3-1.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting botorch\n",
            "  Downloading botorch-0.8.2-py3-none-any.whl (520 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.0/521.0 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from botorch) (1.10.1)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.8/dist-packages (from botorch) (0.6.0)\n",
            "Collecting pyro-ppl>=1.8.4\n",
            "  Downloading pyro_ppl-1.8.4-py3-none-any.whl (730 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.7/730.7 KB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gpytorch==1.9.1\n",
            "  Downloading gpytorch-1.9.1-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.9/250.9 KB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.12 in /usr/local/lib/python3.8/dist-packages (from botorch) (1.13.1+cu116)\n",
            "Collecting linear-operator==0.3.0\n",
            "  Downloading linear_operator-0.3.0-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 KB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from gpytorch==1.9.1->botorch) (1.2.1)\n",
            "Collecting pyro-api>=0.1.1\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.8/dist-packages (from pyro-ppl>=1.8.4->botorch) (4.64.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from pyro-ppl>=1.8.4->botorch) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.8/dist-packages (from pyro-ppl>=1.8.4->botorch) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.12->botorch) (4.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from multipledispatch->botorch) (1.15.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->gpytorch==1.9.1->botorch) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->gpytorch==1.9.1->botorch) (3.1.0)\n",
            "Installing collected packages: pyro-api, pyro-ppl, linear-operator, gpytorch, botorch\n",
            "Successfully installed botorch-0.8.2 gpytorch-1.9.1 linear-operator-0.3.0 pyro-api-0.1.2 pyro-ppl-1.8.4\n"
          ]
        }
      ],
      "source": [
        "!pip install gym[box2d]\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install botorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpv4u1hZX2Vv"
      },
      "source": [
        "# Step 2: Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riwFQZMNtU_4"
      },
      "source": [
        "Libraries used for the Bayesian Optimization Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mw25s5cYPuv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import gym\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "import botorch\n",
        "from botorch.utils.transforms import standardize, normalize, unnormalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSAValUHtY_h"
      },
      "source": [
        "Libraries used to save checkpoints in GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8DoSb8utUAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d84bc5-a751-416d-efde-fcd5db653954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xdYx-gMYyeV"
      },
      "source": [
        "# Step 3: Define objetive function\n",
        "This will be the lower bound of the mean reward of a trained model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNRXK99lY_T9"
      },
      "outputs": [],
      "source": [
        "def get_hyp_values(hyperparams_tensor):\n",
        "  '''\n",
        "  Returns a tuple of values from a tensor containing a hyperparameter configuration\n",
        "\n",
        "          Parameters:\n",
        "                  hyperparams_tensor (torch.DoubleTensor): A tensor of size 1xn (1 row, n columns) with n being the number of hyperparameters to tune\n",
        "          \n",
        "          Returns:\n",
        "                  hyperparams_tuple (tuple): A tuple with the unpacked values of the hyperparams_tensor \n",
        "\n",
        "  '''\n",
        "  hyperparams_list = [hyperparams_tensor[0][i].item() for i in range(len(hyperparams_tensor[0]))]\n",
        "  hyperparams_tuple = tuple(hyperparams_list)\n",
        "  return tuple(hyperparams_list)\n",
        "\n",
        "\n",
        "def create_model(hyp,\n",
        "                 policy='MlpPolicy',\n",
        "                 env='CartPole-v1'):\n",
        "  '''\n",
        "  Returns a PPO model given a policy, environment, and hyperparameters of PPO\n",
        "\n",
        "          Parameters:\n",
        "                  hyp (float): The value of the hyperparameter to train the model with\n",
        "                  policy (str): The NN to train with PPO in the environment. Default is 'MlpPolicy'\n",
        "                  env (stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv): Specifies the gym environment to use for the training\n",
        "\n",
        "          Returns:\n",
        "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
        "  '''\n",
        "  # lr,  = get_hyp_values(hyperparams)\n",
        "  model = PPO(policy = policy,\n",
        "              env = env,\n",
        "              learning_rate = 0.04,\n",
        "              n_steps = 1024,\n",
        "              batch_size = 64,\n",
        "              n_epochs = 4,\n",
        "              gamma = hyp,\n",
        "              gae_lambda = 0.98,\n",
        "              ent_coef = 0.01,\n",
        "              verbose=0)\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "def train_model(model, timesteps=100000):\n",
        "  '''\n",
        "  Trains a PPO model during a number of timesteps\n",
        "          \n",
        "          Parameters:\n",
        "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
        "                  timesteps (int): The number of timesteps used to train the model\n",
        "\n",
        "          Returns:\n",
        "                  None\n",
        "  '''\n",
        "  model.learn(total_timesteps=timesteps)\n",
        "  return\n",
        "\n",
        "\n",
        "def evaluate_model(model, \n",
        "                   rl_env_name='CartPole-v1', \n",
        "                   n_eval_episodes=25):\n",
        "  '''\n",
        "  Evaluates the model for a number of episodes in a specified environment, this environment MUST be the same as the one the model has been trained in.\n",
        "\n",
        "          Parameters:\n",
        "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
        "                  rl_env_name (str): The name of the gym environment where the model has been trained\n",
        "                  n_eval_episodes (int): The number of episodes for which the model will be evaluated to obtain a mean and standard deviation\n",
        "\n",
        "          Returns:\n",
        "                  lower_mean_reward (float): A tensor of size 1x1 (1 row, 1 column) containing the mean_reward\n",
        "  '''\n",
        "  eval_env = gym.make(rl_env_name)\n",
        "  mean_reward, std_reward = evaluate_policy(model, \n",
        "                                            eval_env, \n",
        "                                            n_eval_episodes=n_eval_episodes, \n",
        "                                            deterministic=True)\n",
        "  \n",
        "  print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n",
        "  lower_mean_reward = mean_reward - std_reward\n",
        "  return lower_mean_reward\n",
        "\n",
        "\n",
        "def target_function(hyperparams, \n",
        "                    timesteps=10000,\n",
        "                    rl_env_name='CartPole-v1'):\n",
        "  '''\n",
        "  Given a hyperparameter configuration, evaluates their performance\n",
        "          Parameters:\n",
        "                  hyperparams (float): The value of the learning_rate to train the model with\n",
        "                  timesteps (int): timesteps (int): The number of timesteps used to train the model\n",
        "                  rl_env_name (str): The name of the gym environment where the model has been trained\n",
        "\n",
        "          Returns:\n",
        "                  lower_mean_reward (float): A tensor of size 1x1 (1 row, 1 column) containing the mean_reward\n",
        "  '''\n",
        "  model = create_model(hyperparams)\n",
        "  \n",
        "  train_model(model, \n",
        "              timesteps)\n",
        "  \n",
        "  lower_mean_reward = evaluate_model(model, \n",
        "                                     rl_env_name)\n",
        "  \n",
        "  return lower_mean_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjdeNdB_jJPj"
      },
      "source": [
        "# Step 4: Define hyperparameters to tune\n",
        "First define the bounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXwDQp5oilS4"
      },
      "outputs": [],
      "source": [
        "gamma = 0.9\n",
        "# Define here the list of parameters to tune\n",
        "hyperparams_list = [gamma]\n",
        "# Define the lower bounds of the parameters\n",
        "lower_bounds = [0.8]\n",
        "# Define the upper bounds of the parameters\n",
        "upper_bounds = [0.9997]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ILw3cqAjkbg"
      },
      "source": [
        "Then convert lists to tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnU1DMG8jnuc"
      },
      "outputs": [],
      "source": [
        "# Create tensors with the hyperparameters configurations and bounds for BOTorch to use\n",
        "hyperparams_tensor = torch.DoubleTensor([hyperparams_list])\n",
        "bounds_tensor = torch.DoubleTensor([lower_bounds, upper_bounds])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igatCqADkIjz"
      },
      "source": [
        "# Step 5: Define functions needed for the Bayesian Optimization Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYL5gMk_kK7m"
      },
      "outputs": [],
      "source": [
        "def generate_initial_data(upper_bound, \n",
        "                          lower_bound, \n",
        "                          n=3):\n",
        "  '''\n",
        "  Gets n values of the hyperparameter's bounded space and evaluates them\n",
        "          Parameters:\n",
        "                upper_bound (float): The upper bound of the hyperparameter value\n",
        "                lower_bound (float): The lower bound of the hyperparameter value\n",
        "                n (int): The number of initial points to get. Default is 3\n",
        "          \n",
        "          Returns:\n",
        "                train_x (torch.DoubleTensor): A tensor of size (n, 1) (n rows and 1 column) with the initial points\n",
        "                exact_obj (torch.DoubleTensor): A tensor of size (n, 1) (n rows and 1 column) containing the evaluation of the model with the sampled hyperparameters values\n",
        "                best_observed_vale: The best evaluation of the hyperparameters\n",
        "  '''\n",
        "  # Create our initial hyperparameter values\n",
        "  train_x = torch.rand(n, 1, dtype=torch.double) * (upper_bound - lower_bound) + lower_bound\n",
        "\n",
        "  # Evaluate them and store them in a torch.Tensor\n",
        "  exact_obj = torch.tensor([[target_function(float(gamma))] for gamma in train_x])\n",
        "\n",
        "  # Get the best observed value\n",
        "  best_observed_value = exact_obj.max().item()\n",
        "  \n",
        "  return train_x, exact_obj, best_observed_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MUKzEZkmAxw"
      },
      "outputs": [],
      "source": [
        "from botorch.acquisition.analytic import ExpectedImprovement\n",
        "from botorch.optim import optimize_acqf\n",
        "from botorch.utils.transforms import standardize, normalize, unnormalize\n",
        "from botorch.models import SingleTaskGP\n",
        "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
        "from botorch import fit_gpytorch_model\n",
        "\n",
        "def compute_acquisition_function(single_model, \n",
        "                                 best_init_y,\n",
        "                                 l_bound=-2.,\n",
        "                                 h_bound=10.,\n",
        "                                 resolution=1000):\n",
        "  \n",
        "  '''\n",
        "  Evaluates the acquisition function in the discretized space of the bounded space\n",
        "          Parameters:\n",
        "                  single_model (botorch.models.gp_regression): A Gaussian Process regression model\n",
        "                  best_init_y (float): The best lower_bound_reward obtained until the moment\n",
        "                  l_bound (float): The lower bound of the hyperparameter value\n",
        "                  h_bound (float): The upper bound of the hyperparameter value\n",
        "                  resolution (int): The number of discretized points of the hyperparameter value bounded space\n",
        "\n",
        "          Returns:\n",
        "                  result_tensor (torch.Tensor): A tensor of shape 1xresolution (1 row, {resolution} colums) containing the evaluations of the acquisition function\n",
        "  '''\n",
        "  # Discretize the bounded hyperparameter value space\n",
        "  linspace = torch.linspace(l_bound, \n",
        "                            h_bound,\n",
        "                            steps=resolution)\n",
        "  x_test = torch.tensor([linspace[0]]).unsqueeze(-1)\n",
        "\n",
        "  # Compute our acquistion function\n",
        "  EI = ExpectedImprovement(model=single_model, \n",
        "                           best_f=best_init_y, \n",
        "                           maximize=True)\n",
        "  result = []\n",
        "\n",
        "  # Evaluate the acquisition function in the discretized space\n",
        "  for x in linspace:\n",
        "    x_test = torch.tensor([x]).unsqueeze(-1)\n",
        "    result.append(EI(x_test))\n",
        "  \n",
        "  # Convert results list in a tensor\n",
        "  result_tensor = torch.tensor(result)\n",
        "  return result_tensor\n",
        "\n",
        "\n",
        "def print_acquisition_function(acq_fun, \n",
        "                               iteration,\n",
        "                               l_bound=-2.,\n",
        "                               h_bound=10.,\n",
        "                               resolution=1000, \n",
        "                               suggested=None):\n",
        "  '''\n",
        "  Plots the acquistion function given a series of evaluations previously computed\n",
        "          Parameters:\n",
        "                  acq_fun (torch.Tensor): A tensor of shape 1xresolution with the evaluations of the acquisition function\n",
        "                  iteration (int): The iteration number of the Gaussian Process\n",
        "                  l_bound (float): The lower bound of the hyperparameter value\n",
        "                  h_bound (float): The upper bound of the hyperparameter value\n",
        "                  resolution (int): The number of discretized points of the hyperparameter value bounded space\n",
        "                  suggested (float): The suggested value of the hyperparameter\n",
        "\n",
        "          Returns:\n",
        "                  None\n",
        "  '''\n",
        "\n",
        "  # Discretize the hyperparameter value space\n",
        "  x = torch.linspace(l_bound, h_bound, steps=resolution).detach().numpy()\n",
        "  x_new = x.reshape((resolution,-1))\n",
        "  z = acq_fun\n",
        "\n",
        "  # Get the value that maximizes the acquisition value\n",
        "  max_acq_fun = x[((acq_fun == acq_fun.max().item()).nonzero(as_tuple=True)[0])]\n",
        "\n",
        "  # Plot our data\n",
        "  data = go.Scatter(x=x, y=z, line_color=\"yellow\")\n",
        "\n",
        "  # Axis\n",
        "  fig = go.Figure(data=data)\n",
        "  fig.update_layout(title=\"Expected Improvement acquisition function. Iteration \" + str(iteration), xaxis_title=\"input\", yaxis_title=\"output\")\n",
        "\n",
        "  # If we have suggested a point, draw a red vertical line on it, otherwise draw a red vertical line on the value that maximizes the acq function\n",
        "  if(suggested==None):\n",
        "    fig.add_vline(x=max_acq_fun, line_width=3, line_color=\"red\")\n",
        "  else:\n",
        "    fig.add_vline(x=float(suggested[0][0]), line_width=3, line_color=\"red\")\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "def compute_predictive_distribution(single_model,\n",
        "                                    best_init_y,\n",
        "                                    l_bound=-2.,\n",
        "                                    h_bound=10., \n",
        "                                    resolution=1000):\n",
        "  '''\n",
        "  Computes the predictive distribution of the functions given a Gaussian Process Regresion model\n",
        "          Parameters:\n",
        "                  single_model (botorch.models.gp_regression): A Gaussian Process regression model\n",
        "                  best_init_y (float): The best lower_bound_reward obtained until the moment\n",
        "                  l_bound (float): The lower bound of the hyperparameter value\n",
        "                  h_bound (float): The upper bound of the hyperparameter value\n",
        "                  resolution (int): The number of discretized points of the hyperparameter value bounded space\n",
        "\n",
        "          Returns:\n",
        "                  means_tensor (torch.Tensor): A tensor of shape 1xresolution (1 row, {resolution} colums) containing the means of the discretized points\n",
        "                  variances_tensor (torch.Tensor): A tensor of shape 1xresolution (1 row, {resolution} colums) containing the variances of the discretized points\n",
        "  '''\n",
        "  # Discretize the hyperparameter value bounded space\n",
        "  linspace = torch.linspace(l_bound, h_bound, steps=resolution)\n",
        "  x_test = torch.tensor([linspace[0]]).unsqueeze(-1)\n",
        "\n",
        "  # Initialize our lists, result for the means\n",
        "  result = []\n",
        "  variances = []\n",
        "\n",
        "  # Evaluate means and variances given the GP model\n",
        "  for x in linspace:\n",
        "    x_test = torch.tensor([x]).unsqueeze(-1)\n",
        "    result.append(single_model.posterior(x_test).mean)\n",
        "    variances.append(single_model.posterior(x_test).variance)\n",
        "  \n",
        "  # Convert results into tensors\n",
        "  means_tensor = torch.tensor(result)\n",
        "  variances_tensor = torch.tensor(variances)\n",
        "\n",
        "  return torch.tensor(result), torch.tensor(variances)\n",
        "\n",
        "\n",
        "def print_predictive_mean(predictive_mean, \n",
        "                          predictive_variance,\n",
        "                          iteration, \n",
        "                          l_bound=-2.,\n",
        "                          h_bound=10.,\n",
        "                          resolution=1000,\n",
        "                          suggested=None,\n",
        "                          old_obs=[],\n",
        "                          old_values=[]):\n",
        "  '''\n",
        "  Plots the function distribution obtained by the Gaussian Process\n",
        "          Parameters:\n",
        "                  predictive_mean (torch.Tensor): A tensor of shape 1xresolution (1 row, {resolution} colums) containing the means of the discretized points\n",
        "                  predictive_variance (torch.Tensor): A tensor of shape 1xresolution (1 row, {resolution} colums) containing the variances of the discretized points\n",
        "                  iteration (int): The iteration number of the Gaussian Process\n",
        "                  l_bound (float): The lower bound of the hyperparameter value\n",
        "                  h_bound (float): The upper bound of the hyperparameter value\n",
        "                  resolution (int): The number of discretized points of the hyperparameter value bounded space\n",
        "                  suggested (float): The suggested value of the hyperparameter\n",
        "                  old_obs (list): A list with previous candidates selected by the optimization of the acquisition function\n",
        "                  old_values (list): A list with previous lower bound reward of the hyperparameter candidates selected previously by the optimization of the acquisition function\n",
        "\n",
        "          Returns:\n",
        "                  None\n",
        "  '''\n",
        "\n",
        "  # Discretize the space\n",
        "  x = torch.linspace(l_bound, h_bound, steps=resolution).detach().numpy()\n",
        "  x_new = x.reshape((resolution,-1))\n",
        "  z = predictive_mean\n",
        "\n",
        "  # Get the value that maximizes the predictive mean\n",
        "  max_predictive_mean = x[((predictive_mean == predictive_mean.max().item()).nonzero(as_tuple=True)[0])]\n",
        "\n",
        "  # Create figure\n",
        "  fig = go.Figure()\n",
        "\n",
        "  # Plot upper bound of the expected reward (predictive mean + predictive variance)\n",
        "  fig.add_trace(go.Scatter(x=x, \n",
        "                           y= predictive_mean + np.sqrt(predictive_variance),\n",
        "                           mode='lines',\n",
        "                           line=dict(color=\"#19D3F3\",width =0.1),\n",
        "                           name='upper bound'))\n",
        "  \n",
        "  # Plot predictive mean of each point's expected reward\n",
        "  fig.add_trace(go.Scatter(x=x, \n",
        "                           y= predictive_mean,\n",
        "                           mode='lines',\n",
        "                           line=dict(color=\"blue\"),\n",
        "                           fill='tonexty',\n",
        "                           name='predictive mean'))\n",
        "  \n",
        "  # Plot lower bound of the expected reward (predictive mean - predictive variance)\n",
        "  fig.add_trace(go.Scatter(x=x, y= predictive_mean - np.sqrt(predictive_variance),\n",
        "                         mode='lines',\n",
        "                         line=dict(color=\"blue\", width =0.1),\n",
        "                         fill='tonexty',\n",
        "                         name='lower bound'))\n",
        "  \n",
        "  \n",
        "  # Axis\n",
        "  fig.update_layout(title=\"GP Predictive distribution. Iteration \" + str(iteration), xaxis_title=\"Gamma\", yaxis_title=\"Expected reward\", showlegend=False)\n",
        "\n",
        "  # Add a vertical line in the point that maximizes the expected reward or in the suggested point if provided in the parameters\n",
        "  if(suggested==None):\n",
        "    fig.add_vline(x=max_predictive_mean, line_width=3, line_color=\"red\")\n",
        "  else:\n",
        "    fig.add_vline(x=float(suggested[0][0]), line_width=3, line_color=\"red\")  \n",
        "\n",
        "  # Plot old values\n",
        "  if(len(old_obs)>0):\n",
        "    fig.add_trace(go.Scatter(x=old_obs, y=old_values, mode = 'markers', marker_color=\"black\", marker_size=10))\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "def visualize_functions(single_model,\n",
        "                        best_init_y,\n",
        "                        best_candidate,\n",
        "                        candidate_acq_fun,\n",
        "                        iteration,\n",
        "                        previous_observations,\n",
        "                        previous_values,\n",
        "                        bounds,\n",
        "                        best_candidate_normalized):\n",
        "  '''\n",
        "  Function that visualizes the acquisition function and gaussian process\n",
        "          Parameters:\n",
        "                  single_model (botorch.models.gp_regression): A Gaussian Process regression model\n",
        "                  best_init_y (float): The best lower_bound_reward obtained until the moment\n",
        "                  best_candidate (float): The normalized best candidate until this iteration of the optimization process \n",
        "                  candidate_acq_fun (float): The selected unnormalized candidate in this iteration of the optimization process\n",
        "                  iteration (int): The iteration number of the optimization process\n",
        "                  previous_observations (list): A list with previous candidates selected by the optimization of the acquisition function\n",
        "                  previous_values (list): A list with previous lower bound reward of the hyperparameter candidates selected previously by the optimization of the acquisition function\n",
        "                  bounds (torch.Tensor): Normalized bounds of the hyperparameter values in the form of tensors of shape 2x1 (2 rows, 1 column), first row containing lower bound, second containing upper bound\n",
        "\n",
        "          Returns:\n",
        "                  None\n",
        "\n",
        "  '''\n",
        "\n",
        "  # Compute the mean and variance of the function distribution given by the gaussian process (our single model)\n",
        "  predictive_mean, predictive_variance = compute_predictive_distribution(single_model, \n",
        "                                                                         best_init_y,\n",
        "                                                                         l_bound=0,\n",
        "                                                                         h_bound=1)\n",
        "  # Plot the distribution\n",
        "  print_predictive_mean(predictive_mean, \n",
        "                        predictive_variance, \n",
        "                        iteration,\n",
        "                        suggested=candidate_acq_fun, \n",
        "                        old_obs=previous_observations,\n",
        "                        old_values=previous_values,\n",
        "                        l_bound=bounds[0][0],\n",
        "                        h_bound=bounds[1][0])\n",
        "  \n",
        "  # Compute the acquisition function \n",
        "  acq_fun = compute_acquisition_function(single_model, \n",
        "                                         best_init_y, \n",
        "                                         l_bound=0,\n",
        "                                         h_bound=1)\n",
        "\n",
        "  # Plot the acquisition function\n",
        "  print_acquisition_function(acq_fun,\n",
        "                             iteration,\n",
        "                             suggested=candidate_acq_fun, \n",
        "                             l_bound=bounds[0][0],\n",
        "                             h_bound=bounds[1][0])\n",
        "  \n",
        "\n",
        "def get_next_points_and_visualize_norm(init_x,\n",
        "                                       init_y, \n",
        "                                       best_init_y, \n",
        "                                       normalized_bounds, \n",
        "                                       iteration, \n",
        "                                       previous_observations,\n",
        "                                       previous_values,\n",
        "                                       bounds,\n",
        "                                       n_points=1,\n",
        "                                       visualize=False):\n",
        "  '''\n",
        "  Function that computes the next point to add to the Gaussian Process and visualizes the acquisition function and function distribution\n",
        "          Parameters:\n",
        "                  init_x (torch.Tensor): A tensor of shape {iteration}x1 containing the previous hyperparameters\n",
        "                  init_y (torch.Tensor): A tensor of shape {iteration}x1 containing the previous rewards of the models trained with x_init hyperparameters values\n",
        "                  best_init_y (float): Best reward obtained until the moment\n",
        "                  normalized_bounds (torch.Tensor): Normalized bounds of the hyperparameter values in the form of tensors of shape 2x1 (2 rows, 1 column), first row containing lower bound, second containing upper bound\n",
        "                  iteration (int): The iteration number of the Bayesian Optimization process\n",
        "                  previous_observations (list): A list with previous hyperparameter values\n",
        "                  previous_values (list): A list with previous rewards obtained\n",
        "                  bounds (torch.Tensor): Actual bounds of the hyperparameter values in the form of tensors of shape 2x1 (2 rows, 1 column), first row containing lower bound, second containing upper bound\n",
        "                  n_points (int): Number of candidates to obtain for the next iteration. Default is 1\n",
        "                  visualize (bool): If True, then visualize the GP. Default is True\n",
        "\n",
        "          Returns:\n",
        "                  candidates (torch.Tensor): A tensor of shape 1x1 containing the value of the hyperparameter that optimizes the acquisition function\n",
        "  '''\n",
        "  # Create our model with the points\n",
        "  single_model = SingleTaskGP(init_x, init_y)\n",
        "\n",
        "  mll = ExactMarginalLogLikelihood(single_model.likelihood, single_model)\n",
        "  fit_gpytorch_model(mll)\n",
        "\n",
        "  # Instantiaet the acquisition function given our model\n",
        "  EI = ExpectedImprovement(model=single_model, best_f=best_init_y, maximize=True)\n",
        "  \n",
        "  # Optimize the acquisition function\n",
        "  candidates, _ = optimize_acqf(acq_function=EI, \n",
        "                                bounds=normalized_bounds, \n",
        "                                q=n_points,\n",
        "                                num_restarts=200,\n",
        "                                raw_samples=512, \n",
        "                                options={\"batch_limit\": 5, \"maxiter\": 200})\n",
        "  \n",
        "  # Get the best candidate unnormalized\n",
        "  best_candidate = unnormalize(init_x[((init_y == best_init_y).nonzero(as_tuple=True)[0])][0][0], bounds=normalized_bounds)\n",
        "  # Get our best candidate normalized for the GP to use\n",
        "  best_candidate_normalized = init_x[((init_y == best_init_y).nonzero(as_tuple=True)[0])][0][0]\n",
        "\n",
        "  # Visualize acquisition functions and GP regression\n",
        "  if visualize:\n",
        "    visualize_functions(single_model, \n",
        "                        best_init_y,\n",
        "                        best_candidate,\n",
        "                        unnormalize(candidates, bounds=bounds),\n",
        "                        iteration, previous_observations,\n",
        "                        previous_values, \n",
        "                        bounds, \n",
        "                        best_candidate_normalized)\n",
        "\n",
        "  return candidates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E038wSN_MFoQ"
      },
      "source": [
        "# Step 6: Set experiments' configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWEEF07XuQPj"
      },
      "source": [
        "First let us define functions to save and load checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sde3AjauPiS"
      },
      "outputs": [],
      "source": [
        "def create_experiment_df():\n",
        "  '''\n",
        "  Creates an empty dataframe to save checkpoints\n",
        "          Parameters:\n",
        "\n",
        "          Returns:\n",
        "                  experiment_df (pandas.DataFrame): An empty Dataframe with columns specified below that will be used to save the experiment history\n",
        "  '''\n",
        "  columns = [\"method\",\n",
        "             \"experiment\",\n",
        "             \"iteration\",\n",
        "             \"gamma\",\n",
        "             \"reward_lower_bound\",\n",
        "             \"best_gamma\",\n",
        "             \"best_reward_lower_bound\"\n",
        "             ]\n",
        "\n",
        "  experiment_df = pd.DataFrame(columns=columns)\n",
        "  return experiment_df\n",
        "\n",
        "\n",
        "def get_filepath(experiment_name):\n",
        "  '''\n",
        "  Returns the path of the csv of the experiment specified.\n",
        "          Parameters:\n",
        "                  experiment_name (string): The name of the experiment\n",
        "\n",
        "          Returns:\n",
        "                  filepath (string): The path to the .csv file that has the data of the experiment\n",
        "  '''\n",
        "  filepath = f\"/content/gdrive/My Drive/{experiment_name}.csv\"\n",
        "  return filepath\n",
        "\n",
        "\n",
        "def update_experiment_history(method, \n",
        "                              experiment_number, \n",
        "                              iteration,\n",
        "                              gamma,\n",
        "                              reward_lower_bound,\n",
        "                              best_gamma,\n",
        "                              best_reward_lower_bound,\n",
        "                              experiment_df,\n",
        "                              experiment_name):\n",
        "  '''\n",
        "  Updates the experiment dataframe and saves it in GDrive\n",
        "          Parameters:\n",
        "                  method (int): 0 if Bayesian Optimization, 1 if Random Search\n",
        "                  experiment_number (int): The id of the experiment\n",
        "                  iteration (int): The current iteration of the experiment\n",
        "                  gamma (float): The gamma selected for this iteration\n",
        "                  reward_lower_bound (float): The reward's lower bound obtained by the model trained this iteration\n",
        "                  best_gamma (float): The gamma that has induced the best reward lower bound\n",
        "                  best_reward_lower_bound (float): The best reward lower bound obtained until this iteration\n",
        "                  experiment_df (pandas.DataFrame): The dataframe containing the experiments history\n",
        "                  experiment_name (string): The name of the experiment\n",
        "\n",
        "          Returns:\n",
        "                  concatenated_df (pandas.DataFrame): The updated experiments history dataframe\n",
        "  '''\n",
        "  # Define the columns of the dataframe\n",
        "  columns = [\"method\",\n",
        "             \"experiment\",\n",
        "             \"iteration\",\n",
        "             \"gamma\",\n",
        "             \"reward_lower_bound\",\n",
        "             \"best_gamma\",\n",
        "             \"best_reward_lower_bound\"]\n",
        "\n",
        "  # Set the values of the columns given by the iteration configuration and result\n",
        "  iteration_results = [[method,\n",
        "                        experiment_number,\n",
        "                        iteration,\n",
        "                        gamma,\n",
        "                        reward_lower_bound,\n",
        "                        best_gamma,\n",
        "                        best_reward_lower_bound]]\n",
        "\n",
        "  # Create a one row dataframe for this experiment\n",
        "  new_iteration_df = pd.DataFrame(iteration_results, columns=columns)\n",
        "\n",
        "  # Concatenate the experiments history with this experiment's results\n",
        "  concatenated_df = pd.concat([experiment_df, new_iteration_df], ignore_index=True)\n",
        "\n",
        "  # Save the updated history in google drive\n",
        "  save_checkpoint(concatenated_df,\n",
        "                  experiment_name)\n",
        "  \n",
        "  # Return the concatenated dataframe representing the updated experiment history\n",
        "  return concatenated_df\n",
        "\n",
        "\n",
        "def save_checkpoint(experiment_df,\n",
        "                    experiment_name):\n",
        "  '''\n",
        "  Saves the experiments history dataframe in google drive\n",
        "          Parameters:\n",
        "                  experiment_df (pandas.DataFrame): The dataframe containing the experiments history\n",
        "                  experiment_name (string): The name of the experiment\n",
        "          \n",
        "          Returns:\n",
        "                  None\n",
        "  '''\n",
        "\n",
        "  filepath = get_filepath(experiment_name)\n",
        "  experiment_df.to_csv(filepath, index=False)\n",
        "\n",
        "\n",
        "def load_checkpoint(experiment_name,\n",
        "                    experiment_results,\n",
        "                    experiment_configurations):\n",
        "  '''\n",
        "  Loads a checkpoint of an experiment given its name\n",
        "\n",
        "          Parameters:\n",
        "                  experiment_name (string): The name of the experiment\n",
        "                  experiment_results (numpy.array): A numpy array of three dimensions (method, iteration, best_result)\n",
        "                  expeirment_configuration (numpy.array): A numpy array of three dimensions (method, iteration, best_learning rate)\n",
        "\n",
        "          Returns:\n",
        "                  experiment_df (pandas.DataFrame): A dataframe with the experiment history\n",
        "  '''\n",
        "  # First we retrieve the dataframe from GDrive\n",
        "  filepath = get_filepath(experiment_name)\n",
        "  experiment_df = pd.read_csv(filepath)\n",
        "\n",
        "  # Now we iterate through the rows of the dataframe to update the experiment history numpy arrays that we will use later to compare the methods and plot results\n",
        "  for index, row in experiment_df.iterrows():\n",
        "      # Unpack the columns\n",
        "      method, exp, iter, gamma, rlb, best_gamma, best_rlb = row.values\n",
        "      # Add them to the experiments arrays\n",
        "      experiment_results[int(method)][int(exp)][int(iter)] = best_rlb\n",
        "      experiment_configurations[int(method)][int(exp)][int(iter)] = best_gamma\n",
        "\n",
        "  method, exp, iter, gamma, rlb, best_gamma, best_rlb = experiment_df.iloc[-1]\n",
        "  print(experiment_df.iloc[-1])\n",
        "  if method == 0:\n",
        "    bo_done = False\n",
        "    last_bo_experiment = int(exp)\n",
        "    last_rs_experiment = 0\n",
        "\n",
        "    # Plus one because we want to start in the next one\n",
        "    last_bo_iteration = int(iter)+1\n",
        "    last_rs_iteration = 1\n",
        "\n",
        "  else:\n",
        "    bo_done = True\n",
        "    last_bo_experiment = experiment_configurations.shape[1]-1\n",
        "    last_rs_experiment = int(exp)\n",
        "\n",
        "    # Plus one because we want to start in the next one\n",
        "    last_bo_iteration = experiment_configurations.shape[2]-1\n",
        "    last_rs_iteration = int(iter)+1\n",
        "\n",
        "  # Now lets get the initial data\n",
        "  bo_experiment_df = experiment_df[experiment_df[\"method\"]==0]\n",
        "  init_x = torch.DoubleTensor([[float(lr)] for lr in bo_experiment_df.gamma.values])\n",
        "  init_y = torch.DoubleTensor([[float(reward)] for reward in bo_experiment_df.reward_lower_bound.values])\n",
        "  best_init_y = init_y.max().item()\n",
        "\n",
        "  rs_experiment_df = experiment_df[experiment_df[\"method\"]==0]\n",
        "  if rs_experiment_df.empty:\n",
        "      best_rs_gamma = 0\n",
        "      best_rs_r = 0\n",
        "  else:\n",
        "      best_rs_gamma = rs_experiment_df.iloc[-1][\"best_gamma\"]\n",
        "      best_rs_r = rs_experiment_df.iloc[-1][\"best_reward_lower_bound\"]\n",
        "             \n",
        "  return experiment_df, last_bo_experiment, last_rs_experiment, last_bo_iteration, last_rs_iteration, init_x, init_y, best_init_y, best_rs_gamma, best_rs_r, bo_done"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EMAFy2fu1C21"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AChh4uxu6lk"
      },
      "source": [
        "Now let's set the configuration for the experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF5-qYTvN2zZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf42203-c9a0-4ee2-c2f5-782635829c61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "method                       1.000000\n",
            "experiment                  21.000000\n",
            "iteration                    4.000000\n",
            "gamma                        0.915398\n",
            "reward_lower_bound         240.155559\n",
            "best_gamma                   0.915398\n",
            "best_reward_lower_bound    240.155559\n",
            "Name: 1430, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# The name of the file (WITHOUT EXTENSION) where the history of experiments will be saved\n",
        "experiment_name = \"cartpole_gamma_low_fidelity\"\n",
        "# If true, this will look for the experiment history .csv in google drive and continue from there\n",
        "continue_from_checkpoint = True\n",
        "\n",
        "# Number of experiments per method\n",
        "n_experiments = 25\n",
        "\n",
        "# Number of iterations per experiment after the first random point being evaluated\n",
        "n_iterations = 30\n",
        "\n",
        "# Number of methods\n",
        "n_methods = 2\n",
        "\n",
        "# Index of Bayesian Optimization method\n",
        "bo_method = 0\n",
        "\n",
        "# Index of Random Search method\n",
        "rs_method = 1\n",
        "\n",
        "# Arrays containing the results and configurations of experiments\n",
        "experiment_results = np.zeros((n_methods, n_experiments, n_iterations+1))\n",
        "experiment_configurations = np.zeros((n_methods, n_experiments, n_iterations+1))\n",
        "\n",
        "# Now load checkpoint if necessary\n",
        "if continue_from_checkpoint:\n",
        "  experiment_df, last_bo_experiment, last_rs_experiment, last_bo_iteration, last_rs_iteration, init_x, init_y, best_init_y, best_observed_candidate_rs, best_observed_result_rs, bo_done= load_checkpoint(experiment_name,\n",
        "                                            experiment_results,\n",
        "                                            experiment_configurations)\n",
        "else:\n",
        "  experiment_df = create_experiment_df()\n",
        "  \n",
        "  save_checkpoint(experiment_df, \n",
        "                  experiment_name)\n",
        "\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jScxWwkmzEr"
      },
      "outputs": [],
      "source": [
        "if continue_from_checkpoint:\n",
        "  init_experiment = last_bo_experiment\n",
        "  init_iteration = last_bo_iteration\n",
        "else:\n",
        "  init_experiment = 0\n",
        "  init_iteration = 1\n",
        "  bo_done = False\n",
        "\n",
        "if not bo_done:\n",
        "  for e in range(init_experiment, n_experiments):\n",
        "    print(f\"EXPERIMENT {e}\")\n",
        "    if not (continue_from_checkpoint and init_experiment == e):\n",
        "      # Sample initial hyperparameter values and evaluate the models obtained with them\n",
        "      init_x, init_y, best_init_y = generate_initial_data(upper_bounds[0],\n",
        "                                                          lower_bounds[0],\n",
        "                                                          1)\n",
        "\n",
        "    # We normalize the bounds of the hyperparameters as BOTorch assumes this\n",
        "    normalized_bounds = torch.tensor([[0.0], [1.0]])\n",
        "\n",
        "    # Normalize the hyperparameter as BOTorch assumes this\n",
        "    init_x_normalized = normalize(init_x,\n",
        "                                  bounds=bounds_tensor)\n",
        "\n",
        "    # Standardize the objective as BOTorch assumes this\n",
        "    init_y_standardized = standardize(init_y)\n",
        "\n",
        "    # Obtain the best result among the initial random experiments\n",
        "    best_init_y_standardized = init_y_standardized.max().item()\n",
        "\n",
        "    candidates=[]\n",
        "    results=[]\n",
        "\n",
        "    best_observed_result_bo = best_init_y\n",
        "    best_observed_candidate_bo = init_x[0][0].item()\n",
        "\n",
        "    if not (continue_from_checkpoint and init_experiment == e):\n",
        "      experiment_df = update_experiment_history(bo_method, \n",
        "                                                e, \n",
        "                                                0,\n",
        "                                                best_observed_candidate_bo, # The gamma selected for this iteration\n",
        "                                                best_observed_result_bo, # The reward lower bound of the model\n",
        "                                                best_observed_candidate_bo, # The best_gamma\n",
        "                                                best_observed_result_bo, # The reward lower bound\n",
        "                                                experiment_df,\n",
        "                                                experiment_name)\n",
        "      \n",
        "      experiment_configurations[rs_method,e,0] = best_observed_candidate_bo\n",
        "      experiment_results[rs_method,e,0] = best_observed_result_bo\n",
        "\n",
        "    for i in range(init_iteration, n_iterations+1):\n",
        "      print(f\"Number of iterations done: {i}\")\n",
        "      # Get the next points given our actual queries\n",
        "      normalized_new_candidates = get_next_points_and_visualize_norm(init_x_normalized,\n",
        "                                                                    init_y_standardized, \n",
        "                                                                    best_init_y_standardized, \n",
        "                                                                    normalized_bounds,\n",
        "                                                                    i, \n",
        "                                                                    init_x,\n",
        "                                                                    init_y,\n",
        "                                                                    bounds_tensor,\n",
        "                                                                    1,\n",
        "                                                                    True)\n",
        "      \n",
        "      # Unnormalize the candidate hyperparameter value\n",
        "      new_candidates = unnormalize(normalized_new_candidates,\n",
        "                                  bounds=bounds_tensor)\n",
        "      \n",
        "      # Compute the performance of the model\n",
        "      new_results = torch.tensor([[target_function(float(new_candidates))]])\n",
        "\n",
        "\n",
        "      print(f\"New candidates are: {new_candidates}\")\n",
        "      # Update our hyperparameters and rewards history\n",
        "      init_x = torch.cat([init_x, new_candidates])\n",
        "      init_y = torch.cat([init_y, new_results])\n",
        "\n",
        "      # Normalize our updated hyperparameters and rewards history\n",
        "      init_x_normalized = normalize(init_x, bounds=bounds_tensor)\n",
        "      init_y_standardized = standardize(init_y)\n",
        "\n",
        "      # Update the best reward\n",
        "      best_init_y = init_y.max().item()\n",
        "      best_init_y_standardized = init_y_standardized.max().item()\n",
        "\n",
        "      print(f\"Best point performs this way: {best_init_y}\")\n",
        "      candidates.append(float(normalized_new_candidates[0][0]))\n",
        "      results.append(float(standardize(new_results[0][0])))\n",
        "\n",
        "      if best_observed_result_bo < new_results[0][0]:\n",
        "        best_observed_result_bo = new_results[0][0].item()\n",
        "        best_observed_candidate_bo = new_candidates[0][0].item()\n",
        "\n",
        "      experiment_df = update_experiment_history(bo_method, \n",
        "                                                e, \n",
        "                                                i,\n",
        "                                                new_candidates[0][0].item(), # The gamma selected for this iteration\n",
        "                                                new_results[0][0].item(), # The reward lower bound of the model\n",
        "                                                best_observed_candidate_bo, # The best_gamma\n",
        "                                                best_observed_result_bo, # The reward lower bound\n",
        "                                                experiment_df,\n",
        "                                                experiment_name)\n",
        "\n",
        "      experiment_configurations[bo_method,e,i] = best_observed_candidate_bo\n",
        "      experiment_results[bo_method,e,i] = best_observed_result_bo\n",
        "      print('----------------------')\n",
        "    init_iteration = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHTBIpJSQJVZ"
      },
      "source": [
        "First we try the bayesian optimization method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kbrnek5JW9f"
      },
      "source": [
        "Now we perform a random search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vpuRYDRJWmf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa7af7b-e879-4ee5-ad3a-110bea6efbc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=101.68 +/- 12.581637413309922\n",
            "mean_reward=79.12 +/- 55.513832510465356\n",
            "mean_reward=146.84 +/- 18.034810783592935\n",
            "mean_reward=179.76 +/- 88.79922522184526\n",
            "mean_reward=9.32 +/- 0.6764613810115105\n",
            "mean_reward=58.32 +/- 42.01020828322564\n",
            "mean_reward=71.24 +/- 17.74548956777468\n",
            "mean_reward=468.32 +/- 82.1947540905136\n",
            "mean_reward=450.12 +/- 67.42362790595\n",
            "mean_reward=500.00 +/- 0.0\n",
            "mean_reward=213.76 +/- 7.436558343750152\n",
            "mean_reward=141.76 +/- 13.633136102892832\n",
            "mean_reward=121.88 +/- 15.161319203816007\n",
            "mean_reward=127.32 +/- 16.387116891021435\n",
            "mean_reward=102.32 +/- 10.387376954746564\n",
            "mean_reward=280.24 +/- 204.7918514003914\n",
            "mean_reward=115.68 +/- 8.152153089828477\n",
            "mean_reward=9.24 +/- 0.8616263691415207\n",
            "mean_reward=164.80 +/- 61.684357822708996\n",
            "mean_reward=9.16 +/- 0.7310266752998826\n",
            "mean_reward=72.60 +/- 48.117356535869675\n",
            "mean_reward=108.04 +/- 7.507223188369984\n",
            "mean_reward=30.52 +/- 3.710741165858918\n",
            "mean_reward=343.40 +/- 154.36165326919766\n",
            "mean_reward=9.24 +/- 0.5851495535331118\n",
            "mean_reward=73.48 +/- 35.603505445391185\n",
            "mean_reward=60.44 +/- 7.430100941440837\n",
            "mean_reward=78.12 +/- 70.03103312103856\n",
            "mean_reward=9.52 +/- 0.574108003776293\n",
            "mean_reward=9.40 +/- 0.565685424949238\n",
            "mean_reward=314.12 +/- 76.93806340167394\n",
            "mean_reward=155.88 +/- 50.26117388203343\n",
            "mean_reward=9.36 +/- 0.8890444308357147\n",
            "mean_reward=61.96 +/- 31.249294392033878\n",
            "mean_reward=276.36 +/- 75.01833375915517\n",
            "mean_reward=128.68 +/- 30.243967993634698\n",
            "mean_reward=50.80 +/- 17.83704011320264\n",
            "mean_reward=9.44 +/- 0.8039900496896711\n",
            "mean_reward=30.36 +/- 4.585891407349285\n",
            "mean_reward=72.28 +/- 24.83629602013956\n",
            "mean_reward=9.28 +/- 0.8255906976220118\n",
            "mean_reward=9.56 +/- 0.6974238309665077\n",
            "mean_reward=500.00 +/- 0.0\n",
            "mean_reward=121.20 +/- 7.381056834898374\n",
            "mean_reward=296.64 +/- 39.09258753267683\n",
            "mean_reward=67.72 +/- 19.488499172588945\n",
            "mean_reward=276.12 +/- 131.28939637305064\n",
            "mean_reward=118.20 +/- 71.52677820229287\n",
            "mean_reward=56.44 +/- 17.207161299877445\n",
            "mean_reward=144.96 +/- 121.8167410498245\n",
            "mean_reward=89.28 +/- 99.8284608716372\n",
            "mean_reward=147.04 +/- 140.73534879340016\n",
            "mean_reward=70.68 +/- 23.259785037699725\n",
            "mean_reward=52.76 +/- 34.15409785077041\n",
            "mean_reward=31.24 +/- 3.8186908751560398\n",
            "mean_reward=405.08 +/- 123.13567151723339\n",
            "mean_reward=44.48 +/- 22.450158128619048\n",
            "mean_reward=9.32 +/- 0.6764613810115105\n",
            "mean_reward=76.76 +/- 11.353519278179784\n",
            "mean_reward=195.88 +/- 30.655922755643815\n",
            "mean_reward=416.44 +/- 107.81134634165366\n",
            "mean_reward=9.28 +/- 0.664529909033446\n",
            "mean_reward=9.52 +/- 0.8059776671843953\n",
            "mean_reward=39.72 +/- 14.81761114350083\n",
            "mean_reward=157.16 +/- 51.203656119460845\n",
            "mean_reward=165.08 +/- 75.33812846095927\n",
            "mean_reward=30.28 +/- 3.105092591212056\n",
            "mean_reward=9.40 +/- 0.9380831519646858\n",
            "mean_reward=64.32 +/- 49.66666487695746\n",
            "mean_reward=93.16 +/- 90.89188302593362\n",
            "mean_reward=126.64 +/- 6.150642242888137\n",
            "mean_reward=9.36 +/- 0.6858571279792899\n",
            "mean_reward=9.28 +/- 0.664529909033446\n",
            "mean_reward=71.56 +/- 27.56676259556062\n",
            "mean_reward=9.24 +/- 0.7088018058667741\n",
            "mean_reward=24.80 +/- 3.730951621235526\n",
            "mean_reward=9.52 +/- 0.574108003776293\n",
            "mean_reward=80.16 +/- 22.626851305473327\n",
            "mean_reward=9.36 +/- 0.7418894796396563\n",
            "mean_reward=81.84 +/- 34.40892907371573\n",
            "mean_reward=451.36 +/- 84.8668981405589\n",
            "mean_reward=9.36 +/- 0.7418894796396563\n",
            "mean_reward=9.36 +/- 0.7939773296511683\n",
            "mean_reward=192.96 +/- 114.22380837636257\n",
            "mean_reward=157.48 +/- 35.4650475820913\n",
            "mean_reward=41.12 +/- 16.322548820573335\n",
            "mean_reward=149.20 +/- 16.2234398325386\n",
            "mean_reward=118.36 +/- 21.741904240429356\n",
            "mean_reward=95.56 +/- 7.20044443072787\n",
            "mean_reward=41.80 +/- 12.477179168385778\n",
            "mean_reward=9.44 +/- 0.7525955088890711\n",
            "mean_reward=50.48 +/- 28.408618410616167\n",
            "mean_reward=9.56 +/- 0.7525955088890712\n",
            "mean_reward=75.04 +/- 23.91565177869924\n",
            "mean_reward=103.60 +/- 20.31157305577291\n",
            "mean_reward=500.00 +/- 0.0\n",
            "mean_reward=19.12 +/- 3.808621797973645\n",
            "mean_reward=203.80 +/- 133.15975367955588\n",
            "mean_reward=14.32 +/- 1.4620533505997653\n",
            "mean_reward=45.32 +/- 12.56573117649745\n",
            "mean_reward=26.80 +/- 2.3832750575625967\n",
            "mean_reward=146.36 +/- 15.75786787608019\n",
            "mean_reward=49.24 +/- 7.742247735638533\n",
            "mean_reward=103.16 +/- 17.494410535939757\n",
            "mean_reward=67.84 +/- 18.353593653560058\n",
            "mean_reward=9.04 +/- 0.72\n",
            "mean_reward=9.32 +/- 0.6764613810115105\n",
            "mean_reward=9.40 +/- 0.6928203230275509\n",
            "mean_reward=9.56 +/- 0.5713142742834281\n",
            "mean_reward=71.48 +/- 5.470795189001321\n",
            "mean_reward=39.92 +/- 3.9891853805006354\n",
            "mean_reward=96.56 +/- 56.83455287059096\n",
            "mean_reward=416.40 +/- 148.85697833826939\n",
            "mean_reward=204.76 +/- 14.486628317175809\n",
            "mean_reward=9.44 +/- 0.8039900496896711\n",
            "mean_reward=314.20 +/- 60.297263619504335\n",
            "mean_reward=49.60 +/- 4.791659420284375\n",
            "mean_reward=31.36 +/- 3.740374312819507\n",
            "mean_reward=243.88 +/- 158.00792891497565\n"
          ]
        }
      ],
      "source": [
        "if continue_from_checkpoint:\n",
        "  init_experiment = last_rs_experiment\n",
        "  init_iteration = last_rs_iteration\n",
        "else:\n",
        "  init_experiment = 0\n",
        "  init_iteration = 1\n",
        "\n",
        "for e in range(init_experiment, n_experiments):\n",
        "  if not (continue_from_checkpoint and init_experiment == e) or (init_experiment==0 and init_iteration==1):\n",
        "    # Initiate with a random value\n",
        "    random_value = np.random.random() * (upper_bounds[0] - lower_bounds[0]) + lower_bounds[0]\n",
        "    best_observed_result_rs = target_function(random_value)\n",
        "    best_observed_candidate_rs = random_value\n",
        "    # Update our experiments histories\n",
        "    experiment_df = update_experiment_history(rs_method, \n",
        "                                              e, \n",
        "                                              0,\n",
        "                                              random_value, # The gamma selected for this iteration\n",
        "                                              best_observed_result_rs, # The reward lower bound of the model\n",
        "                                              best_observed_candidate_rs, # The best_gamma\n",
        "                                              best_observed_result_rs, # The reward lower bound\n",
        "                                              experiment_df,\n",
        "                                              experiment_name)\n",
        "  \n",
        "  # Iterate with random search\n",
        "  for i in range(init_iteration, n_iterations+1):\n",
        "    # Get a new random value for the hyperparameter\n",
        "    random_value = np.random.random() * (upper_bounds[0] - lower_bounds[0]) + lower_bounds[0]\n",
        "    # Evaluate the model with that hyperparameter value\n",
        "    rs_obj_fun_result = target_function(random_value)\n",
        "\n",
        "    # Update best reward and candidate found if necessary\n",
        "    if best_observed_result_rs < rs_obj_fun_result:\n",
        "      best_observed_result_rs = rs_obj_fun_result\n",
        "      best_observed_candidate_rs = random_value\n",
        "    \n",
        "    # Update our experiments histories\n",
        "    experiment_df = update_experiment_history(rs_method, \n",
        "                                              e, \n",
        "                                              i,\n",
        "                                              random_value, # The gamma selected for this iteration\n",
        "                                              rs_obj_fun_result, # The reward lower bound of the model\n",
        "                                              best_observed_candidate_rs, # The best_gamma\n",
        "                                              best_observed_result_rs, # The reward lower bound\n",
        "                                              experiment_df,\n",
        "                                              experiment_name)\n",
        "    experiment_configurations[rs_method,e,i] = best_observed_candidate_rs\n",
        "    experiment_results[rs_method,e,i] = best_observed_result_rs\n",
        "\n",
        "  init_iteration = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukMdAAapSR2c"
      },
      "source": [
        "# Step 7: Compare the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeN8EWoOSVxC"
      },
      "source": [
        "First we give the recommendation as the best observed result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlf2cyT-SZf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aadaf265-b689-47e8-cf8f-03429821462b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best observed result is: 500.0\n",
            "The best observed result belong to the : 0 method. Its value is 0.9993213045418264\n"
          ]
        }
      ],
      "source": [
        "best_observed_result = np.max(experiment_results)\n",
        "index_set = np.where(experiment_results==best_observed_result)\n",
        "print(\"The best observed result is: \" + str(best_observed_result))\n",
        "print(\"The best observed result belong to the : \" + str(index_set[0][0]) + \" method. Its value is \" + str(experiment_configurations[index_set][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jZ5oFK6Sf3Q"
      },
      "source": [
        "And now we plot the results to compare both methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgo_wr7VSj7k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "0ebe9a0d-24b8-45bf-a475-fe0812e28945"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"8a582eb6-6bc7-4a70-80f5-9c07ef6735b8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8a582eb6-6bc7-4a70-80f5-9c07ef6735b8\")) {                    Plotly.newPlot(                        \"8a582eb6-6bc7-4a70-80f5-9c07ef6735b8\",                        [{\"line\":{\"color\":\"green\",\"width\":0.1},\"mode\":\"lines\",\"name\":\"\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],\"y\":[223.00299618448247,452.86194890303955,505.933779912795,527.6766301640878,545.0721955232877,554.7530740744076,568.1570333510876,570.1451013367109,570.0335530104115,580.6015960315085,557.975308607037,557.9825973675295,559.6726795515974,531.0266066343869,531.0266066343869,531.0266066343869,520.9751272327644,520.9751272327644,513.5932074752034,513.5932074752034,513.5932074752034,513.5932074752034,504.8626697163556,504.8626697163556,504.8626697163556,504.8626697163556,500.0,500.0,500.0,500.0,500.0],\"type\":\"scatter\"},{\"fill\":\"tonexty\",\"line\":{\"color\":\"green\"},\"mode\":\"lines\",\"name\":\"Bayesian Optimization\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],\"y\":[92.75795449843532,254.13053076031443,316.80833943834995,351.77462367115396,371.4967256240627,379.41862071141384,409.4244711216355,435.54636060041486,436.0656294080668,466.1780804630155,478.59323216389333,478.85808653676315,484.69530815114575,492.0423775633484,492.0423775633484,492.0423775633484,494.6203545542077,494.6203545542077,496.51364991133596,496.51364991133596,496.51364991133596,496.51364991133596,498.75283526513624,498.75283526513624,498.75283526513624,498.75283526513624,500.0,500.0,500.0,500.0,500.0],\"type\":\"scatter\"},{\"fill\":\"tonexty\",\"line\":{\"color\":\"green\",\"width\":0.1},\"mode\":\"lines\",\"name\":\"\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],\"y\":[-37.487087187611806,55.39911261758928,127.68289896390488,175.8726171782201,197.92125572483775,204.08416734842012,250.69190889218342,300.94761986411885,302.09770580572206,351.75456489452245,399.2111557207496,399.7335757059969,409.71793675069404,453.0581484923099,453.0581484923099,453.0581484923099,468.26558187565104,468.26558187565104,479.43409234746855,479.43409234746855,479.43409234746855,479.43409234746855,492.6430008139169,492.6430008139169,492.6430008139169,492.6430008139169,500.0,500.0,500.0,500.0,500.0],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"width\":0.1},\"mode\":\"lines\",\"name\":\"\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],\"y\":[205.84180443528956,303.6685370509729,347.1776913685048,356.53521073653684,388.69082229409014,458.9119244712971,474.8244393626045,488.94928881479393,488.94928881479393,503.1895902919679,502.9634051541209,501.5567571019205,501.5567571019205,510.28552054472505,518.8107902014626,524.3792390724504,539.1672777967319,542.7872672043081,542.0832242866261,547.0440387351006,546.0025319250152,546.0025319250152,546.0025319250152,539.5763133432996,537.6126385816926,534.9440935019101,534.9440935019101,534.9440935019101,534.9440935019101,534.9440935019101,534.9440935019101],\"type\":\"scatter\"},{\"fill\":\"tonexty\",\"line\":{\"color\":\"red\"},\"mode\":\"lines\",\"name\":\"Random Search\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],\"y\":[75.97389988535724,144.3725516142815,184.7281835025569,195.6001182924935,226.80772748360047,295.1863010645591,309.3102455838823,325.7758633611114,325.7758633611114,341.1495893677279,341.6883035187163,347.7443581818874,347.7443581818874,368.32004099958135,382.261385082874,397.85557001924104,417.0959250492279,419.6768924200613,420.76198383321554,431.7738206282439,436.96387835208327,436.96387835208327,436.96387835208327,455.02238186520924,457.8406146078484,460.15700903983424,460.15700903983424,460.15700903983424,460.15700903983424,460.15700903983424,460.15700903983424],\"type\":\"scatter\"},{\"fill\":\"tonexty\",\"line\":{\"color\":\"red\",\"width\":0.1},\"mode\":\"lines\",\"name\":\"\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],\"y\":[-53.89400466457508,-14.92343382240989,22.278675636609023,34.66502584845017,64.92463267311078,131.4606776578211,143.79605180516012,162.60243790742894,162.60243790742894,179.10958844348798,180.4132018833117,193.93195926185427,193.93195926185427,226.35456145443763,245.71197996428538,271.3319009660317,295.0245723017239,296.5665176358145,299.44074337980504,316.5036025213873,327.92522477915134,327.92522477915134,327.92522477915134,370.4684503871189,378.06859063400424,385.3699245777584,385.3699245777584,385.3699245777584,385.3699245777584,385.3699245777584,385.3699245777584],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Performance comparison between BO and RS. MLP-alpha experiment.\"},\"xaxis\":{\"title\":{\"text\":\"Iterations\"}},\"yaxis\":{\"title\":{\"text\":\"Reward lower bound\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8a582eb6-6bc7-4a70-80f5-9c07ef6735b8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "x = np.linspace(1, n_iterations, n_iterations).astype(int)\n",
        "mean_bo = np.mean(experiment_results[0,:,:], axis=0)\n",
        "mean_rs = np.mean(experiment_results[1,:,:], axis=0)\n",
        "std_bo = np.std(experiment_results[0,:,:], axis=0)\n",
        "std_rs = np.std(experiment_results[1,:,:], axis=0)\n",
        "bo_ub_results = go.Scatter(x=x, y=mean_bo + std_bo, mode='lines', name=\"\", line_color=\"green\", line_width=0.1)\n",
        "bo_results = go.Scatter(x=x, y=mean_bo, mode='lines', fill='tonexty', line_color=\"green\", name=\"Bayesian Optimization\")\n",
        "bo_lb_results = go.Scatter(x=x, y=mean_bo - std_bo, mode='lines', fill='tonexty', name=\"\", line_color=\"green\", line_width=0.1)\n",
        "\n",
        "rs_ub_results = go.Scatter(x=x, y=mean_rs + std_rs, mode='lines', name=\"\", line_color=\"red\", line_width=0.1)\n",
        "rs_results = go.Scatter(x=x, y=mean_rs, mode='lines', fill='tonexty', line_color=\"red\", name=\"Random Search\")\n",
        "rs_lb_results = go.Scatter(x=x, y=mean_rs - std_rs, mode='lines', fill='tonexty', name=\"\", line_color=\"red\", line_width=0.1)\n",
        "  \n",
        "fig = go.Figure()\n",
        "fig.add_trace(bo_ub_results)\n",
        "fig.add_trace(bo_results)\n",
        "fig.add_trace(bo_lb_results)\n",
        "fig.add_trace(rs_ub_results)\n",
        "fig.add_trace(rs_results)\n",
        "fig.add_trace(rs_lb_results)\n",
        "fig.update_layout(title=\"Performance comparison between BO and RS. MLP-alpha experiment.\", xaxis_title=\"Iterations\", yaxis_title=\"Reward lower bound\")\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}