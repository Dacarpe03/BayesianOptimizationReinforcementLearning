{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR0o63R4Xa6F"
   },
   "source": [
    "# Step 1: Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kpv4u1hZX2Vv"
   },
   "source": [
    "# Step 2: Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riwFQZMNtU_4"
   },
   "source": [
    "Libraries used for the Bayesian Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4mw25s5cYPuv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning in stationary: failed to import cython module: falling back to numpy\n",
      "warning in coregionalize: failed to import cython module: falling back to numpy\n",
      "warning in choleskies: failed to import cython module: falling back to numpy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from hebo.design_space.design_space import DesignSpace\n",
    "from hebo.optimizers.hebo import HEBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xdYx-gMYyeV"
   },
   "source": [
    "# Step 3: Define objetive function\n",
    "This will be the lower bound of the mean reward of a trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XNRXK99lY_T9"
   },
   "outputs": [],
   "source": [
    "def get_hyp_values(hyperparams_dataframe):\n",
    "  '''\n",
    "  Returns a tuple of values from a tensor containing a hyperparameter configuration\n",
    "\n",
    "          Parameters:\n",
    "                  hyperparams_tensor (torch.DoubleTensor): A tensor of size 1xn (1 row, n columns) with n being the number of hyperparameters to tune\n",
    "          \n",
    "          Returns:\n",
    "                  hyperparams_tuple (tuple): A tuple with the unpacked values of the hyperparams_tensor \n",
    "\n",
    "  '''\n",
    "  hyp_values = hyperparams_dataframe.values\n",
    "  lr = hyp_values[0][0]\n",
    "  gamma = hyp_values[0][1]\n",
    "  return lr, gamma\n",
    "\n",
    "\n",
    "def create_model(hyperparams,\n",
    "                 policy='MlpPolicy',\n",
    "                 env_name='LunarLander-v2'):\n",
    "  '''\n",
    "  Returns a PPO model given a policy, environment, and hyperparameters of PPO\n",
    "\n",
    "          Parameters:\n",
    "                  hyperparams (pd.Dataframe): A dataframe of 1 row with columns learning_rate and gamma to train the model with\n",
    "                  policy (str): The NN to train with PPO in the environment. Default is 'MlpPolicy'\n",
    "                  env (stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv): Specifies the gym environment to use for the training\n",
    "\n",
    "          Returns:\n",
    "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
    "  '''\n",
    "  lr, gamma  = get_hyp_values(hyperparams)\n",
    "  env = make_vec_env(env_name, n_envs=1)\n",
    "  model = PPO(policy = policy,\n",
    "              env = env,\n",
    "              learning_rate = lr,\n",
    "              n_steps = 1024,\n",
    "              batch_size = 64,\n",
    "              n_epochs = 4,\n",
    "              gamma = gamma,\n",
    "              gae_lambda = 0.98,\n",
    "              ent_coef = 0.01,\n",
    "              verbose=0)\n",
    "  \n",
    "  return model\n",
    "\n",
    "\n",
    "def train_model(model, timesteps=1000000):\n",
    "  '''\n",
    "  Trains a PPO model during a number of timesteps\n",
    "          \n",
    "          Parameters:\n",
    "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
    "                  timesteps (int): The number of timesteps used to train the model\n",
    "\n",
    "          Returns:\n",
    "                  None\n",
    "  '''\n",
    "  model.learn(total_timesteps=timesteps)\n",
    "  return\n",
    "\n",
    "\n",
    "def evaluate_model(model, \n",
    "                   rl_env_name='LunarLander-v2', \n",
    "                   n_eval_episodes=25):\n",
    "  '''\n",
    "  Evaluates the model for a number of episodes in a specified environment, this environment MUST be the same as the one the model has been trained in.\n",
    "\n",
    "          Parameters:\n",
    "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
    "                  rl_env_name (str): The name of the gym environment where the model has been trained\n",
    "                  n_eval_episodes (int): The number of episodes for which the model will be evaluated to obtain a mean and standard deviation\n",
    "\n",
    "          Returns:\n",
    "                  lower_mean_reward (numpy.array): An array of shape (1,1) containing the lower mean reward multiplied by -1 as we are trying to maximize and hebo minimizes\n",
    "  '''\n",
    "  eval_env = gym.make(rl_env_name)\n",
    "  mean_reward, std_reward = evaluate_policy(model, \n",
    "                                            eval_env, \n",
    "                                            n_eval_episodes=n_eval_episodes, \n",
    "                                            deterministic=True)\n",
    "  \n",
    "  print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n",
    "  lower_mean_reward = -1 * (mean_reward - std_reward)\n",
    "  lower_mean_reward_np = np.array([lower_mean_reward]).reshape(-1,1)\n",
    "  return lower_mean_reward_np\n",
    "\n",
    "\n",
    "def target_function(hyperparams, \n",
    "                    timesteps=1000000,\n",
    "                    rl_env_name='LunarLander-v2'):\n",
    "  '''\n",
    "  Given a hyperparameter configuration, evaluates their performance\n",
    "          Parameters:\n",
    "                  hyperparams (pd.Dataframe): A dataframe of 1 row with columns learning_rate and gamma to train the model with\n",
    "                  timesteps (int): timesteps (int): The number of timesteps used to train the model\n",
    "                  rl_env_name (str): The name of the gym environment where the model has been trained\n",
    "\n",
    "          Returns:\n",
    "                  lower_mean_reward (float): An array of shape (1,1) containing the lower mean reward multiplied by -1 as we are trying to maximize and hebo minimizes\n",
    "  '''\n",
    "  model = create_model(hyperparams, env_name=rl_env_name)\n",
    "  \n",
    "  train_model(model, \n",
    "              timesteps)\n",
    "  \n",
    "  lower_mean_reward = evaluate_model(model, \n",
    "                                     rl_env_name)\n",
    "  \n",
    "  return lower_mean_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjdeNdB_jJPj"
   },
   "source": [
    "# Step 4: Define hyperparameters to tune\n",
    "We will use the DesignSpace object from HEBO library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WXwDQp5oilS4"
   },
   "outputs": [],
   "source": [
    "hyperparams = [\n",
    "    {'name' : 'learning_rate', \n",
    "     'type' : 'num',  # Float\n",
    "     'lb' : 0.0001,   # Lower bound\n",
    "     'ub' : 0.1},     # Upper bound\n",
    "    \n",
    "    {'name' : 'gamma',  \n",
    "     'type' : 'num',  # Float\n",
    "     'lb' : 0.8,      # Lower bound  \n",
    "     'ub' : 0.9997}   # Upper bound\n",
    "]\n",
    "\n",
    "\n",
    "hyperparams_space = DesignSpace().parse(hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igatCqADkIjz"
   },
   "source": [
    "# Step 5: Define functions needed for the Bayesian Optimization Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E038wSN_MFoQ"
   },
   "source": [
    "# Step 6: Set experiments' configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWEEF07XuQPj"
   },
   "source": [
    "First let us define functions to save and load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8sde3AjauPiS"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def create_experiment_df():\n",
    "  '''\n",
    "  Creates an empty dataframe to save checkpoints\n",
    "          Parameters:\n",
    "\n",
    "          Returns:\n",
    "                  experiment_df (pandas.DataFrame): An empty Dataframe with columns specified below that will be used to save the experiment history\n",
    "  '''\n",
    "  columns = [\"method\",\n",
    "             \"experiment\",\n",
    "             \"iteration\",\n",
    "             \"learning_rate\",\n",
    "             \"gamma\",\n",
    "             \"reward_lower_bound\",\n",
    "             \"best_learning_rate\",\n",
    "             \"best_gamma\",\n",
    "             \"best_reward_lower_bound\"\n",
    "             ]\n",
    "\n",
    "  experiment_df = pd.DataFrame(columns=columns)\n",
    "  return experiment_df\n",
    "\n",
    "\n",
    "def get_filepath(experiment_name):\n",
    "  '''\n",
    "  Returns the path of the csv of the experiment specified.\n",
    "          Parameters:\n",
    "                  experiment_name (string): The name of the experiment\n",
    "\n",
    "          Returns:\n",
    "                  filepath (string): The path to the .csv file that has the data of the experiment\n",
    "  '''\n",
    "  filepath = f\"./experiments_results/{experiment_name}.csv\"\n",
    "  return filepath\n",
    "\n",
    "\n",
    "def update_experiment_history(method, \n",
    "                              experiment_number, \n",
    "                              iteration,\n",
    "                              lr,\n",
    "                              gamma,\n",
    "                              reward_lower_bound,\n",
    "                              best_lr,\n",
    "                              best_gamma,\n",
    "                              best_reward_lower_bound,\n",
    "                              experiment_df,\n",
    "                              experiment_name):\n",
    "  '''\n",
    "  Updates the experiment dataframe and saves it in GDrive\n",
    "          Parameters:\n",
    "                  method (int): 0 if Bayesian Optimization, 1 if Random Search\n",
    "                  experiment_number (int): The id of the experiment\n",
    "                  iteration (int): The current iteration of the experiment\n",
    "                  lr (float): The learning rate value selected for this iteration\n",
    "                  gamma (float): The gamma value selected for this iteration\n",
    "                  reward_lower_bound (float): The reward's lower bound obtained by the model trained this iteration\n",
    "                  best_lr (float): The learning rate value that has induced the best reward lower bound\n",
    "                  best_gamma (float): The gamma value that has induced the best reward lower bound\n",
    "                  best_reward_lower_bound (float): The best reward lower bound obtained until this iteration\n",
    "                  experiment_df (pandas.DataFrame): The dataframe containing the experiments history\n",
    "                  experiment_name (string): The name of the experiment\n",
    "\n",
    "          Returns:\n",
    "                  concatenated_df (pandas.DataFrame): The updated experiments history dataframe\n",
    "  '''\n",
    "  # Define the columns of the dataframe\n",
    "  columns = [\"method\",\n",
    "             \"experiment\",\n",
    "             \"iteration\",\n",
    "             \"learning_rate\",\n",
    "             \"gamma\",\n",
    "             \"reward_lower_bound\",\n",
    "             \"best_learning_rate\",\n",
    "             \"best_gamma\",\n",
    "             \"best_reward_lower_bound\"]\n",
    "\n",
    "  # Set the values of the columns given by the iteration configuration and result\n",
    "  iteration_results = [[method,\n",
    "                        experiment_number,\n",
    "                        iteration,\n",
    "                        lr,\n",
    "                        gamma,\n",
    "                        reward_lower_bound,\n",
    "                        best_lr,\n",
    "                        best_gamma,\n",
    "                        best_reward_lower_bound]]\n",
    "\n",
    "  # Create a one row dataframe for this experiment\n",
    "  new_iteration_df = pd.DataFrame(iteration_results, columns=columns)\n",
    "\n",
    "  # Concatenate the experiments history with this experiment's results\n",
    "  concatenated_df = pd.concat([experiment_df, new_iteration_df], ignore_index=True)\n",
    "\n",
    "  # Save the updated history in google drive\n",
    "  save_checkpoint(concatenated_df,\n",
    "                  experiment_name)\n",
    "  \n",
    "  # Return the concatenated dataframe representing the updated experiment history\n",
    "  return concatenated_df\n",
    "\n",
    "\n",
    "def save_checkpoint(experiment_df,\n",
    "                    experiment_name):\n",
    "  '''\n",
    "  Saves the experiments history dataframe in google drive\n",
    "          Parameters:\n",
    "                  experiment_df (pandas.DataFrame): The dataframe containing the experiments history\n",
    "                  experiment_name (string): The name of the experiment\n",
    "          \n",
    "          Returns:\n",
    "                  None\n",
    "  '''\n",
    "\n",
    "  filepath = get_filepath(experiment_name)\n",
    "  experiment_df.to_csv(filepath, index=False)\n",
    "\n",
    "\n",
    "def load_checkpoint(experiment_name):\n",
    "  '''\n",
    "  Loads a checkpoint of an experiment given its name\n",
    "\n",
    "          Parameters:\n",
    "                  experiment_name (string): The name of the experiment\n",
    "          Returns:\n",
    "                  experiment_df (pandas.DataFrame): A dataframe with the experiment history\n",
    "                  last_bo_experiment (int): The number of the last bayesian optimization experiment\n",
    "                  last_rs_experiment (int): The number of the last random search experiment\n",
    "                  last_bo_iteration (int): The last iteration in the last bayesian optimization experiment\n",
    "                  last_rs_iteration (int): The last iteration in the last random search experiment\n",
    "                  past_observations (pandas.Dataframe): With the learning rate and gammas of the last experiments observations\n",
    "                  past_results (numpy.array): A numpy array of shape (1,n) containing the lower mean rewards of the experiments. n being the number of iterations done in the experiment\n",
    "                  best_init_y (float): The best lower_mean_reward bound obtained in the last experiment\n",
    "                  best_rs_lr: The best random search learning rate found \n",
    "                  best_rs_gamma: The best gamma found \n",
    "                  best_rs_r: The best lower mean reward bound obtained in the last experiment\n",
    "                  bo_done: If the bayesian optimization has finised\n",
    "  '''\n",
    "  # First we retrieve the dataframe from GDrive\n",
    "  filepath = get_filepath(experiment_name)\n",
    "  experiment_df = pd.read_csv(filepath)\n",
    "  \n",
    "  \n",
    "\n",
    "  method, exp, iter, lr, gamma, rlb, best_lr, best_gamma, best_rlb = experiment_df.iloc[-1]\n",
    "\n",
    "  if method == 0:\n",
    "    bo_done = False\n",
    "    last_bo_experiment = int(exp)\n",
    "    last_rs_experiment = 0\n",
    "\n",
    "    # Plus one because we want to start in the next one\n",
    "    last_bo_iteration = int(iter)+1\n",
    "    last_rs_iteration = 1\n",
    "\n",
    "  else:\n",
    "    bo_done = True\n",
    "    last_bo_experiment = experiment_configurations.shape[1]-1\n",
    "    last_rs_experiment = int(exp)\n",
    "\n",
    "    # Plus one because we want to start in the next one\n",
    "    last_bo_iteration = experiment_configurations.shape[2]-1\n",
    "    last_rs_iteration = int(iter)+1\n",
    "\n",
    "  # Now lets get the current experiment data\n",
    "  current_experiment_df = experiment_df[(experiment_df[\"method\"]==method) & (experiment_df[\"experiment\"]==int(exp))]\n",
    "  past_results = current_experiment_df.reward_lower_bound.to_numpy().reshape(-1,1)\n",
    "  past_observations = current_experiment_df[[\"learning_rate\", \"gamma\"]].copy()\n",
    "  past_observations = past_observations.reset_index()\n",
    "  past_observations.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "\n",
    "  rs_experiment_df = experiment_df[experiment_df[\"method\"]==0]\n",
    "  if rs_experiment_df.empty:\n",
    "      best_rs_lr = 0\n",
    "      best_rs_gamma = 0\n",
    "      best_rs_r = 0\n",
    "  else:\n",
    "      best_rs_lr = rs_experiment_df.iloc[-1][\"best_learning_rate\"]\n",
    "      best_rs_gamma = rs_experiment_df.iloc[-1][\"best_gamma\"]\n",
    "      best_rs_r = rs_experiment_df.iloc[-1][\"best_reward_lower_bound\"]\n",
    "             \n",
    "  return (experiment_df, \n",
    "         last_bo_experiment, \n",
    "         last_rs_experiment, \n",
    "         last_bo_iteration, \n",
    "         last_rs_iteration, \n",
    "         past_observations, \n",
    "         past_results, \n",
    "         best_rs_lr, \n",
    "         best_rs_gamma, \n",
    "         best_rs_r,\n",
    "         bo_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AChh4uxu6lk"
   },
   "source": [
    "Now let's set the configuration for the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FF5-qYTvN2zZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# The name of the file (WITHOUT EXTENSION) where the history of experiments will be saved\n",
    "experiment_name = \"lunar_lander_learning_rate_gamma_hebo\"\n",
    "# If true, this will look for the experiment history .csv in google drive and continue from there\n",
    "continue_from_checkpoint = True\n",
    "\n",
    "# Number of experiments per method\n",
    "n_experiments = 15\n",
    "\n",
    "# Number of iterations per experiment after the first random point being evaluated\n",
    "n_iterations = 25\n",
    "\n",
    "# Number of methods\n",
    "n_methods = 2\n",
    "\n",
    "# Index of Bayesian Optimization method\n",
    "bo_method = 0\n",
    "\n",
    "# Index of Random Search method\n",
    "rs_method = 1\n",
    "\n",
    "# Number of Hyperparameters\n",
    "n_hyperparameters = 2\n",
    "\n",
    "# Arrays containing the results and configurations of experiments\n",
    "experiment_results = np.zeros((n_methods, n_experiments, n_iterations+1))\n",
    "experiment_configurations = np.zeros((n_methods, n_experiments, n_iterations+1, n_hyperparameters))\n",
    "\n",
    "# Now load checkpoint if necessary\n",
    "if continue_from_checkpoint:\n",
    "  experiment_df, last_bo_experiment, last_rs_experiment, last_bo_iteration, last_rs_iteration, past_observations, past_rewards, best_observed_lr_rs, best_observed_gamma_rs, best_observed_result_rs, bo_done = load_checkpoint(experiment_name) \n",
    "  \n",
    "  hebo_seq = HEBO(hyperparams_space, \n",
    "                  model_name = 'gpy')\n",
    "  hebo_seq.X = past_observations\n",
    "  hebo_seq.y = past_rewards\n",
    "    \n",
    "else: \n",
    "  experiment_df = create_experiment_df()\n",
    "  \n",
    "  save_checkpoint(experiment_df, \n",
    "                  experiment_name)\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHTBIpJSQJVZ"
   },
   "source": [
    "First we try the bayesian optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jScxWwkmzEr",
    "outputId": "96830da6-8047-4f36-ad5d-d54acfde7d84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 13\n",
      "mean_reward=68.51 +/- 106.57128512357207\n",
      "Number of iteration: 7\n",
      "  - Unnormalized learning rate: 0.000774509547557224\n",
      "  - Unnormalized gamma: 0.9734308286343196\n",
      "  - Unstandardized lower reward bound: -38.06205814750088\n",
      "Best point performs this way: 226.6685793556269\n",
      "mean_reward=142.50 +/- 106.60717282100016\n",
      "Number of iteration: 8\n",
      "  - Unnormalized learning rate: 0.00011250200978512859\n",
      "  - Unnormalized gamma: 0.9955019650981138\n",
      "  - Unstandardized lower reward bound: 35.89741716456511\n",
      "Best point performs this way: 226.6685793556269\n",
      "mean_reward=181.94 +/- 35.1760351402758\n",
      "Number of iteration: 9\n",
      "  - Unnormalized learning rate: 0.00010185715320068463\n",
      "  - Unnormalized gamma: 0.995995294651266\n",
      "  - Unstandardized lower reward bound: 146.76479568767576\n",
      "Best point performs this way: 226.6685793556269\n",
      "mean_reward=254.48 +/- 70.53224524684765\n",
      "Number of iteration: 10\n",
      "  - Unnormalized learning rate: 0.0010166926214597032\n",
      "  - Unnormalized gamma: 0.9963549064144002\n",
      "  - Unstandardized lower reward bound: 183.9427594305657\n",
      "Best point performs this way: 226.6685793556269\n",
      "mean_reward=275.10 +/- 22.441540609238142\n",
      "Number of iteration: 11\n",
      "  - Unnormalized learning rate: 0.0014151500686733476\n",
      "  - Unnormalized gamma: 0.9961136367076568\n",
      "  - Unstandardized lower reward bound: 252.65450023020225\n",
      "Best point performs this way: 252.65450023020225\n",
      "mean_reward=67.93 +/- 168.69423860995516\n",
      "Number of iteration: 12\n",
      "  - Unnormalized learning rate: 0.005441992836873019\n",
      "  - Unnormalized gamma: 0.9961361472323239\n",
      "  - Unstandardized lower reward bound: -100.76628766800144\n",
      "Best point performs this way: 252.65450023020225\n",
      "mean_reward=-78.04 +/- 104.49528414633144\n",
      "Number of iteration: 13\n",
      "  - Unnormalized learning rate: 0.0009094100364946749\n",
      "  - Unnormalized gamma: 0.8168674928140396\n",
      "  - Unstandardized lower reward bound: -182.5330218013737\n",
      "Best point performs this way: 252.65450023020225\n",
      "mean_reward=265.07 +/- 27.562429639999888\n",
      "Number of iteration: 14\n",
      "  - Unnormalized learning rate: 0.0013725798900007043\n",
      "  - Unnormalized gamma: 0.9947629877639176\n",
      "  - Unstandardized lower reward bound: 237.5112106662686\n",
      "Best point performs this way: 252.65450023020225\n",
      "mean_reward=158.12 +/- 103.43759913395594\n",
      "Number of iteration: 15\n",
      "  - Unnormalized learning rate: 0.00010000703987928005\n",
      "  - Unnormalized gamma: 0.9979815104563183\n",
      "  - Unstandardized lower reward bound: 54.683796735923735\n",
      "Best point performs this way: 252.65450023020225\n",
      "mean_reward=-53.87 +/- 54.31362241534955\n",
      "Number of iteration: 16\n",
      "  - Unnormalized learning rate: 0.00010000429012310494\n",
      "  - Unnormalized gamma: 0.9629761953718925\n",
      "  - Unstandardized lower reward bound: -108.18061851632041\n",
      "Best point performs this way: 252.65450023020225\n",
      "mean_reward=244.89 +/- 62.49465502427685\n",
      "Number of iteration: 17\n",
      "  - Unnormalized learning rate: 0.00043719494581409524\n",
      "  - Unnormalized gamma: 0.9969848028390096\n",
      "  - Unstandardized lower reward bound: 182.3943461163442\n",
      "Best point performs this way: 252.65450023020225\n",
      "mean_reward=177.24 +/- 80.52342045120831\n",
      "Number of iteration: 18\n",
      "  - Unnormalized learning rate: 0.0015595755465594675\n",
      "  - Unnormalized gamma: 0.9978708250625088\n",
      "  - Unstandardized lower reward bound: 96.71445370314022\n",
      "Best point performs this way: 252.65450023020225\n",
      "mean_reward=213.68 +/- 92.52911208551637\n",
      "Number of iteration: 19\n",
      "  - Unnormalized learning rate: 0.0004867118939060911\n",
      "  - Unnormalized gamma: 0.9922227828378112\n",
      "  - Unstandardized lower reward bound: 121.1550904546155\n",
      "Best point performs this way: 252.65450023020225\n",
      "mean_reward=198.28 +/- 208.02677288279804\n",
      "Number of iteration: 20\n",
      "  - Unnormalized learning rate: 0.0011303530023840653\n",
      "  - Unnormalized gamma: 0.9916715412823677\n",
      "  - Unstandardized lower reward bound: -9.748099377913462\n",
      "Best point performs this way: 252.65450023020225\n",
      "mean_reward=204.92 +/- 32.41129615844265\n",
      "Number of iteration: 21\n",
      "  - Unnormalized learning rate: 0.00010013034314237242\n",
      "  - Unnormalized gamma: 0.9927436725015935\n",
      "  - Unstandardized lower reward bound: 172.51029758037822\n",
      "Best point performs this way: 252.65450023020225\n",
      "mean_reward=-31.86 +/- 33.78016818604527\n",
      "Number of iteration: 22\n",
      "  - Unnormalized learning rate: 0.0001001294371230963\n",
      "  - Unnormalized gamma: 0.985525156646987\n",
      "  - Unstandardized lower reward bound: -65.6424306118965\n",
      "Best point performs this way: 252.65450023020225\n",
      "mean_reward=265.15 +/- 46.31282790025657\n",
      "Number of iteration: 23\n",
      "  - Unnormalized learning rate: 0.001352672236065305\n",
      "  - Unnormalized gamma: 0.9952517838789733\n",
      "  - Unstandardized lower reward bound: 218.83764792899248\n",
      "Best point performs this way: 252.65450023020225\n",
      "mean_reward=279.26 +/- 18.692255249392343\n",
      "Number of iteration: 24\n",
      "  - Unnormalized learning rate: 0.0020191272322160167\n",
      "  - Unnormalized gamma: 0.995866093044205\n",
      "  - Unstandardized lower reward bound: 260.5628699892976\n",
      "Best point performs this way: 260.5628699892976\n",
      "mean_reward=253.95 +/- 50.30930698688017\n",
      "Number of iteration: 25\n",
      "  - Unnormalized learning rate: 0.0018897077035280165\n",
      "  - Unnormalized gamma: 0.9956922446171644\n",
      "  - Unstandardized lower reward bound: 203.64220749674757\n",
      "Best point performs this way: 260.5628699892976\n",
      "EXPERIMENT 14\n",
      "mean_reward=154.42 +/- 122.90768981640993\n",
      "mean_reward=-813.87 +/- 518.8817456490411\n",
      "Number of iteration: 1\n",
      "  - Unnormalized learning rate: 0.025074999779462814\n",
      "  - Unnormalized gamma: 0.949774980545044\n",
      "  - Unstandardized lower reward bound: -1332.7562559614196\n",
      "Best point performs this way: 31.51573143047996\n",
      "mean_reward=-117.69 +/- 41.60505465141708\n",
      "Number of iteration: 2\n",
      "  - Unnormalized learning rate: 0.037562500685453415\n",
      "  - Unnormalized gamma: 0.8748875260353088\n",
      "  - Unstandardized lower reward bound: -159.2938067352979\n",
      "Best point performs this way: 31.51573143047996\n",
      "mean_reward=-739.05 +/- 274.0833705374887\n",
      "Number of iteration: 3\n",
      "  - Unnormalized learning rate: 0.016274360793372203\n",
      "  - Unnormalized gamma: 0.8001457599980407\n",
      "  - Unstandardized lower reward bound: -1013.1315881783804\n",
      "Best point performs this way: 31.51573143047996\n",
      "mean_reward=-135.37 +/- 40.38710800204243\n",
      "Number of iteration: 4\n",
      "  - Unnormalized learning rate: 0.000618686733955414\n",
      "  - Unnormalized gamma: 0.8976647025399836\n",
      "  - Unstandardized lower reward bound: -175.75994623216005\n",
      "Best point performs this way: 31.51573143047996\n",
      "mean_reward=-118.36 +/- 39.90506750149183\n",
      "Number of iteration: 5\n",
      "  - Unnormalized learning rate: 0.0024142278620737367\n",
      "  - Unnormalized gamma: 0.8005293467910425\n",
      "  - Unstandardized lower reward bound: -158.26867342474662\n",
      "Best point performs this way: 31.51573143047996\n",
      "mean_reward=-128.51 +/- 27.175705987198594\n",
      "Number of iteration: 6\n",
      "  - Unnormalized learning rate: 0.09748361270088853\n",
      "  - Unnormalized gamma: 0.8004341694272221\n",
      "  - Unstandardized lower reward bound: -155.69005214071353\n",
      "Best point performs this way: 31.51573143047996\n",
      "mean_reward=33.57 +/- 191.36552616947242\n",
      "Number of iteration: 7\n",
      "  - Unnormalized learning rate: 0.0003686449491070856\n",
      "  - Unnormalized gamma: 0.8010560103528168\n",
      "  - Unstandardized lower reward bound: -157.7951590445061\n",
      "Best point performs this way: 31.51573143047996\n",
      "mean_reward=-606.63 +/- 161.28142729518586\n",
      "Number of iteration: 8\n",
      "  - Unnormalized learning rate: 0.06288663578898533\n",
      "  - Unnormalized gamma: 0.9177494411057585\n",
      "  - Unstandardized lower reward bound: -767.9152253186481\n",
      "Best point performs this way: 31.51573143047996\n",
      "mean_reward=-128.96 +/- 78.17269239963915\n",
      "Number of iteration: 9\n",
      "  - Unnormalized learning rate: 0.0883107149736657\n",
      "  - Unnormalized gamma: 0.9031612866346562\n",
      "  - Unstandardized lower reward bound: -207.1298257288751\n",
      "Best point performs this way: 31.51573143047996\n",
      "mean_reward=-504.74 +/- 151.1547626689394\n",
      "Number of iteration: 10\n",
      "  - Unnormalized learning rate: 0.005646339733049529\n",
      "  - Unnormalized gamma: 0.9528225884262465\n",
      "  - Unstandardized lower reward bound: -655.8954223071293\n",
      "Best point performs this way: 31.51573143047996\n",
      "mean_reward=101.98 +/- 139.49899205498312\n",
      "Number of iteration: 11\n",
      "  - Unnormalized learning rate: 9.999999747378752e-05\n",
      "  - Unnormalized gamma: 0.8213781909108455\n",
      "  - Unstandardized lower reward bound: -37.51956096182046\n",
      "Best point performs this way: 31.51573143047996\n",
      "mean_reward=-553.31 +/- 130.63939917679164\n",
      "Number of iteration: 12\n",
      "  - Unnormalized learning rate: 0.04339702891696652\n",
      "  - Unnormalized gamma: 0.8000059121010724\n",
      "  - Unstandardized lower reward bound: -683.9447017924044\n",
      "Best point performs this way: 31.51573143047996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=74.02 +/- 149.9054057125762\n",
      "Number of iteration: 13\n",
      "  - Unnormalized learning rate: 0.00014997506605598815\n",
      "  - Unnormalized gamma: 0.8000002167223786\n",
      "  - Unstandardized lower reward bound: -75.89021165199026\n",
      "Best point performs this way: 31.51573143047996\n",
      "mean_reward=116.24 +/- 132.40196812653144\n",
      "Number of iteration: 14\n",
      "  - Unnormalized learning rate: 9.999999747378752e-05\n",
      "  - Unnormalized gamma: 0.8000000908083785\n",
      "  - Unstandardized lower reward bound: -16.157681375520454\n",
      "Best point performs this way: 31.51573143047996\n",
      "mean_reward=172.43 +/- 93.80299925847159\n",
      "Number of iteration: 15\n",
      "  - Unnormalized learning rate: 9.999999747378752e-05\n",
      "  - Unnormalized gamma: 0.8001250165065338\n",
      "  - Unstandardized lower reward bound: 78.63086050752581\n",
      "Best point performs this way: 78.63086050752581\n",
      "mean_reward=140.57 +/- 133.17306529993047\n",
      "Number of iteration: 16\n",
      "  - Unnormalized learning rate: 0.00010025612953864368\n",
      "  - Unnormalized gamma: 0.8004815400457832\n",
      "  - Unstandardized lower reward bound: 7.3985502848696285\n",
      "Best point performs this way: 78.63086050752581\n",
      "mean_reward=103.72 +/- 124.54040473707624\n",
      "Number of iteration: 17\n",
      "  - Unnormalized learning rate: 0.00010000006789398165\n",
      "  - Unnormalized gamma: 0.8001249358713597\n",
      "  - Unstandardized lower reward bound: -20.823898596482906\n",
      "Best point performs this way: 78.63086050752581\n",
      "mean_reward=151.76 +/- 103.02606026035222\n",
      "Number of iteration: 18\n",
      "  - Unnormalized learning rate: 0.00010366655278016518\n",
      "  - Unnormalized gamma: 0.8005088951936411\n",
      "  - Unstandardized lower reward bound: 48.736131567815775\n",
      "Best point performs this way: 78.63086050752581\n",
      "mean_reward=118.92 +/- 110.41675727566971\n",
      "Number of iteration: 19\n",
      "  - Unnormalized learning rate: 0.00010868270245434383\n",
      "  - Unnormalized gamma: 0.8002827473224664\n",
      "  - Unstandardized lower reward bound: 8.499784007684738\n",
      "Best point performs this way: 78.63086050752581\n",
      "mean_reward=-43.89 +/- 139.5815889444213\n",
      "Number of iteration: 20\n",
      "  - Unnormalized learning rate: 0.0009767622190104117\n",
      "  - Unnormalized gamma: 0.827666519040945\n",
      "  - Unstandardized lower reward bound: -183.46786236321887\n",
      "Best point performs this way: 78.63086050752581\n",
      "mean_reward=21.92 +/- 139.93685280610947\n",
      "Number of iteration: 21\n",
      "  - Unnormalized learning rate: 0.004009047176546442\n",
      "  - Unnormalized gamma: 0.8002157540702821\n",
      "  - Unstandardized lower reward bound: -118.02052122554755\n",
      "Best point performs this way: 78.63086050752581\n",
      "mean_reward=148.53 +/- 98.66683147287492\n",
      "Number of iteration: 22\n",
      "  - Unnormalized learning rate: 0.00012658282878019473\n",
      "  - Unnormalized gamma: 0.8266168544191643\n",
      "  - Unstandardized lower reward bound: 49.86099628453749\n",
      "Best point performs this way: 78.63086050752581\n",
      "mean_reward=99.49 +/- 190.79517708990676\n",
      "Number of iteration: 23\n",
      "  - Unnormalized learning rate: 0.00012205754868809507\n",
      "  - Unnormalized gamma: 0.8455016832353757\n",
      "  - Unstandardized lower reward bound: -91.30783466843235\n",
      "Best point performs this way: 78.63086050752581\n",
      "mean_reward=182.41 +/- 111.0165751441332\n",
      "Number of iteration: 24\n",
      "  - Unnormalized learning rate: 0.00011434091304355423\n",
      "  - Unnormalized gamma: 0.8162866616136565\n",
      "  - Unstandardized lower reward bound: 71.39428498491237\n",
      "Best point performs this way: 78.63086050752581\n",
      "mean_reward=132.55 +/- 142.06768584482373\n",
      "Number of iteration: 25\n",
      "  - Unnormalized learning rate: 0.00011877704979078176\n",
      "  - Unnormalized gamma: 0.8173758138477493\n",
      "  - Unstandardized lower reward bound: -9.513457987884408\n",
      "Best point performs this way: 78.63086050752581\n"
     ]
    }
   ],
   "source": [
    "if continue_from_checkpoint:\n",
    "    init_experiment = last_bo_experiment\n",
    "    init_iteration = last_bo_iteration\n",
    "else:\n",
    "    init_experiment = 0\n",
    "    init_iteration = 1\n",
    "    bo_done = False\n",
    "\n",
    "if not bo_done:\n",
    "    for e in range(init_experiment, n_experiments):\n",
    "        print(f\"EXPERIMENT {e}\")\n",
    "        if not (continue_from_checkpoint and init_experiment == e):\n",
    "            # Create our optimizer\n",
    "            hebo_seq = HEBO(hyperparams_space, \n",
    "                        model_name = 'gpy')\n",
    "\n",
    "\n",
    "            # Do the first observation\n",
    "            new_candidates = hebo_seq.suggest(n_suggestions=3)\n",
    "            hebo_seq.observe(new_candidates, target_function(new_candidates))\n",
    "     \n",
    "            # Initialize the observations\n",
    "            best_observed_result_bo = hebo_seq.y.min()\n",
    "            best_observed_lr_bo, best_observed_gamma_bo = get_hyp_values(new_candidates)\n",
    "\n",
    "            # Update the experiment history\n",
    "            experiment_df = update_experiment_history(bo_method,\n",
    "                                                  e,\n",
    "                                                  0,\n",
    "                                                  best_observed_lr_bo, # The learning rate selected for this iteration\n",
    "                                                  best_observed_gamma_bo, # The gamma selected for this iteration\n",
    "                                                  best_observed_result_bo, # The reward lower bound of the model \n",
    "                                                  best_observed_lr_bo, # The best learning rate\n",
    "                                                  best_observed_gamma_bo, # The best gamma\n",
    "                                                  best_observed_result_bo, # The reward lower bound \n",
    "                                                  experiment_df,\n",
    "                                                  experiment_name)\n",
    "\n",
    "        else:\n",
    "            # Initialize the observations\n",
    "            best_observed_result_bo = hebo_seq.y.min()\n",
    "            best_observed_lr_bo = experiment_df.iloc[-1][\"best_learning_rate\"]\n",
    "            best_observed_gamma_bo = experiment_df.iloc[-1][\"best_gamma\"]\n",
    "        \n",
    "    \n",
    "        for i in range(init_iteration, n_iterations+1):\n",
    "            # Obtain new candidates from HEBO\n",
    "            new_candidates = hebo_seq.suggest(n_suggestions=1)\n",
    "            candidate_lr_bo, candidate_gamma_bo = get_hyp_values(new_candidates)\n",
    "\n",
    "            # Evaluate new candidates\n",
    "            hebo_seq.observe(new_candidates, target_function(new_candidates))\n",
    "\n",
    "            if (hebo_seq.y.min() < best_observed_result_bo):\n",
    "                best_observed_result_bo = hebo_seq.y.min()\n",
    "                best_observed_lr_bo, best_observed_gamma_bo = get_hyp_values(new_candidates)\n",
    "\n",
    "            # Show iteration info\n",
    "            print(f\"Number of iteration: {i}\")\n",
    "            print(f\"  - Unnormalized learning rate: {candidate_lr_bo}\")\n",
    "            print(f\"  - Unnormalized gamma: {candidate_gamma_bo}\")\n",
    "            print(f\"  - Unstandardized lower reward bound: {-hebo_seq.y[-1][0]}\") # (the minus is added because we multiplied by -1 the reward in the target function in order to maximize it as HEBO uses LCB, so now to obtain the real reading we multiply by -1 again)\n",
    "            print(f\"Best point performs this way: {-best_observed_result_bo}\") # (the minus is added because we multiplied by -1 the reward in the target function in order to maximize it as HEBO uses LCB, so now to obtain the real reading we multiply by -1 again)\n",
    "\n",
    "\n",
    "            experiment_df = update_experiment_history(bo_method, \n",
    "                                                      e, \n",
    "                                                      i,\n",
    "                                                      candidate_lr_bo, # The lr selected for this iteration\n",
    "                                                      candidate_gamma_bo, # The gamma selected for this iteration\n",
    "                                                      hebo_seq.y[-1][0], # The reward lower bound of the model \n",
    "                                                      best_observed_lr_bo, # The best learning rate\n",
    "                                                      best_observed_gamma_bo, # The best gamma\n",
    "                                                      best_observed_result_bo, # The reward lower bound\n",
    "                                                      experiment_df,\n",
    "                                                      experiment_name)\n",
    "        \n",
    "        init_iteration = 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "hebo-env",
   "language": "python",
   "name": "hebo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
