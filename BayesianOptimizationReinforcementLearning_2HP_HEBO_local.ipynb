{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR0o63R4Xa6F"
   },
   "source": [
    "# Step 1: Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kpv4u1hZX2Vv"
   },
   "source": [
    "# Step 2: Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riwFQZMNtU_4"
   },
   "source": [
    "Libraries used for the Bayesian Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4mw25s5cYPuv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning in stationary: failed to import cython module: falling back to numpy\n",
      "warning in coregionalize: failed to import cython module: falling back to numpy\n",
      "warning in choleskies: failed to import cython module: falling back to numpy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from hebo.design_space.design_space import DesignSpace\n",
    "from hebo.optimizers.hebo import HEBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xdYx-gMYyeV"
   },
   "source": [
    "# Step 3: Define objetive function\n",
    "This will be the lower bound of the mean reward of a trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XNRXK99lY_T9"
   },
   "outputs": [],
   "source": [
    "def get_hyp_values(hyperparams_dataframe):\n",
    "  '''\n",
    "  Returns a tuple of values from a tensor containing a hyperparameter configuration\n",
    "\n",
    "          Parameters:\n",
    "                  hyperparams_tensor (torch.DoubleTensor): A tensor of size 1xn (1 row, n columns) with n being the number of hyperparameters to tune\n",
    "          \n",
    "          Returns:\n",
    "                  hyperparams_tuple (tuple): A tuple with the unpacked values of the hyperparams_tensor \n",
    "\n",
    "  '''\n",
    "  hyp_values = hyperparams_dataframe.values\n",
    "  lr = hyp_values[0][0]\n",
    "  gamma = hyp_values[0][1]\n",
    "  return lr, gamma\n",
    "\n",
    "\n",
    "def create_model(hyperparams,\n",
    "                 policy='MlpPolicy',\n",
    "                 env_name='LunarLander-v2'):\n",
    "  '''\n",
    "  Returns a PPO model given a policy, environment, and hyperparameters of PPO\n",
    "\n",
    "          Parameters:\n",
    "                  hyperparams (pd.Dataframe): A dataframe of 1 row with columns learning_rate and gamma to train the model with\n",
    "                  policy (str): The NN to train with PPO in the environment. Default is 'MlpPolicy'\n",
    "                  env (stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv): Specifies the gym environment to use for the training\n",
    "\n",
    "          Returns:\n",
    "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
    "  '''\n",
    "  lr, gamma  = get_hyp_values(hyperparams)\n",
    "  env = make_vec_env(env_name, n_envs=1)\n",
    "  model = PPO(policy = policy,\n",
    "              env = env,\n",
    "              learning_rate = lr,\n",
    "              n_steps = 1024,\n",
    "              batch_size = 64,\n",
    "              n_epochs = 4,\n",
    "              gamma = gamma,\n",
    "              gae_lambda = 0.98,\n",
    "              ent_coef = 0.01,\n",
    "              verbose=0)\n",
    "  \n",
    "  return model\n",
    "\n",
    "\n",
    "def train_model(model, timesteps=1000000):\n",
    "  '''\n",
    "  Trains a PPO model during a number of timesteps\n",
    "          \n",
    "          Parameters:\n",
    "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
    "                  timesteps (int): The number of timesteps used to train the model\n",
    "\n",
    "          Returns:\n",
    "                  None\n",
    "  '''\n",
    "  model.learn(total_timesteps=timesteps)\n",
    "  return\n",
    "\n",
    "\n",
    "def evaluate_model(model, \n",
    "                   rl_env_name='LunarLander-v2', \n",
    "                   n_eval_episodes=25):\n",
    "  '''\n",
    "  Evaluates the model for a number of episodes in a specified environment, this environment MUST be the same as the one the model has been trained in.\n",
    "\n",
    "          Parameters:\n",
    "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
    "                  rl_env_name (str): The name of the gym environment where the model has been trained\n",
    "                  n_eval_episodes (int): The number of episodes for which the model will be evaluated to obtain a mean and standard deviation\n",
    "\n",
    "          Returns:\n",
    "                  lower_mean_reward (numpy.array): An array of shape (1,1) containing the lower mean reward multiplied by -1 as we are trying to maximize and hebo minimizes\n",
    "  '''\n",
    "  eval_env = gym.make(rl_env_name)\n",
    "  mean_reward, std_reward = evaluate_policy(model, \n",
    "                                            eval_env, \n",
    "                                            n_eval_episodes=n_eval_episodes, \n",
    "                                            deterministic=True)\n",
    "  \n",
    "  print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n",
    "  lower_mean_reward = -1 * (mean_reward - std_reward)\n",
    "  lower_mean_reward_np = np.array([lower_mean_reward]).reshape(-1,1)\n",
    "  return lower_mean_reward_np\n",
    "\n",
    "\n",
    "def target_function(hyperparams, \n",
    "                    timesteps=1000000,\n",
    "                    rl_env_name='LunarLander-v2'):\n",
    "  '''\n",
    "  Given a hyperparameter configuration, evaluates their performance\n",
    "          Parameters:\n",
    "                  hyperparams (pd.Dataframe): A dataframe of 1 row with columns learning_rate and gamma to train the model with\n",
    "                  timesteps (int): timesteps (int): The number of timesteps used to train the model\n",
    "                  rl_env_name (str): The name of the gym environment where the model has been trained\n",
    "\n",
    "          Returns:\n",
    "                  lower_mean_reward (float): An array of shape (1,1) containing the lower mean reward multiplied by -1 as we are trying to maximize and hebo minimizes\n",
    "  '''\n",
    "  model = create_model(hyperparams, env_name=rl_env_name)\n",
    "  \n",
    "  train_model(model, \n",
    "              timesteps)\n",
    "  \n",
    "  lower_mean_reward = evaluate_model(model, \n",
    "                                     rl_env_name)\n",
    "  \n",
    "  return lower_mean_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjdeNdB_jJPj"
   },
   "source": [
    "# Step 4: Define hyperparameters to tune\n",
    "We will use the DesignSpace object from HEBO library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WXwDQp5oilS4"
   },
   "outputs": [],
   "source": [
    "hyperparams = [\n",
    "    {'name' : 'learning_rate', \n",
    "     'type' : 'num',  # Float\n",
    "     'lb' : 0.0001,   # Lower bound\n",
    "     'ub' : 0.1},     # Upper bound\n",
    "    \n",
    "    {'name' : 'gamma',  \n",
    "     'type' : 'num',  # Float\n",
    "     'lb' : 0.8,      # Lower bound  \n",
    "     'ub' : 0.9997}   # Upper bound\n",
    "]\n",
    "\n",
    "\n",
    "hyperparams_space = DesignSpace().parse(hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igatCqADkIjz"
   },
   "source": [
    "# Step 5: Define functions needed for the Bayesian Optimization Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E038wSN_MFoQ"
   },
   "source": [
    "# Step 6: Set experiments' configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWEEF07XuQPj"
   },
   "source": [
    "First let us define functions to save and load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8sde3AjauPiS"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def create_experiment_df():\n",
    "  '''\n",
    "  Creates an empty dataframe to save checkpoints\n",
    "          Parameters:\n",
    "\n",
    "          Returns:\n",
    "                  experiment_df (pandas.DataFrame): An empty Dataframe with columns specified below that will be used to save the experiment history\n",
    "  '''\n",
    "  columns = [\"method\",\n",
    "             \"experiment\",\n",
    "             \"iteration\",\n",
    "             \"learning_rate\",\n",
    "             \"gamma\",\n",
    "             \"reward_lower_bound\",\n",
    "             \"best_learning_rate\",\n",
    "             \"best_gamma\",\n",
    "             \"best_reward_lower_bound\"\n",
    "             ]\n",
    "\n",
    "  experiment_df = pd.DataFrame(columns=columns)\n",
    "  return experiment_df\n",
    "\n",
    "\n",
    "def get_filepath(experiment_name):\n",
    "  '''\n",
    "  Returns the path of the csv of the experiment specified.\n",
    "          Parameters:\n",
    "                  experiment_name (string): The name of the experiment\n",
    "\n",
    "          Returns:\n",
    "                  filepath (string): The path to the .csv file that has the data of the experiment\n",
    "  '''\n",
    "  filepath = f\"./experiments_results/{experiment_name}.csv\"\n",
    "  return filepath\n",
    "\n",
    "\n",
    "def update_experiment_history(method, \n",
    "                              experiment_number, \n",
    "                              iteration,\n",
    "                              lr,\n",
    "                              gamma,\n",
    "                              reward_lower_bound,\n",
    "                              best_lr,\n",
    "                              best_gamma,\n",
    "                              best_reward_lower_bound,\n",
    "                              experiment_df,\n",
    "                              experiment_name):\n",
    "  '''\n",
    "  Updates the experiment dataframe and saves it in GDrive\n",
    "          Parameters:\n",
    "                  method (int): 0 if Bayesian Optimization, 1 if Random Search\n",
    "                  experiment_number (int): The id of the experiment\n",
    "                  iteration (int): The current iteration of the experiment\n",
    "                  lr (float): The learning rate value selected for this iteration\n",
    "                  gamma (float): The gamma value selected for this iteration\n",
    "                  reward_lower_bound (float): The reward's lower bound obtained by the model trained this iteration\n",
    "                  best_lr (float): The learning rate value that has induced the best reward lower bound\n",
    "                  best_gamma (float): The gamma value that has induced the best reward lower bound\n",
    "                  best_reward_lower_bound (float): The best reward lower bound obtained until this iteration\n",
    "                  experiment_df (pandas.DataFrame): The dataframe containing the experiments history\n",
    "                  experiment_name (string): The name of the experiment\n",
    "\n",
    "          Returns:\n",
    "                  concatenated_df (pandas.DataFrame): The updated experiments history dataframe\n",
    "  '''\n",
    "  # Define the columns of the dataframe\n",
    "  columns = [\"method\",\n",
    "             \"experiment\",\n",
    "             \"iteration\",\n",
    "             \"learning_rate\",\n",
    "             \"gamma\",\n",
    "             \"reward_lower_bound\",\n",
    "             \"best_learning_rate\",\n",
    "             \"best_gamma\",\n",
    "             \"best_reward_lower_bound\"]\n",
    "\n",
    "  # Set the values of the columns given by the iteration configuration and result\n",
    "  iteration_results = [[method,\n",
    "                        experiment_number,\n",
    "                        iteration,\n",
    "                        lr,\n",
    "                        gamma,\n",
    "                        reward_lower_bound,\n",
    "                        best_lr,\n",
    "                        best_gamma,\n",
    "                        best_reward_lower_bound]]\n",
    "\n",
    "  # Create a one row dataframe for this experiment\n",
    "  new_iteration_df = pd.DataFrame(iteration_results, columns=columns)\n",
    "\n",
    "  # Concatenate the experiments history with this experiment's results\n",
    "  concatenated_df = pd.concat([experiment_df, new_iteration_df], ignore_index=True)\n",
    "\n",
    "  # Save the updated history in google drive\n",
    "  save_checkpoint(concatenated_df,\n",
    "                  experiment_name)\n",
    "  \n",
    "  # Return the concatenated dataframe representing the updated experiment history\n",
    "  return concatenated_df\n",
    "\n",
    "\n",
    "def save_checkpoint(experiment_df,\n",
    "                    experiment_name):\n",
    "  '''\n",
    "  Saves the experiments history dataframe in google drive\n",
    "          Parameters:\n",
    "                  experiment_df (pandas.DataFrame): The dataframe containing the experiments history\n",
    "                  experiment_name (string): The name of the experiment\n",
    "          \n",
    "          Returns:\n",
    "                  None\n",
    "  '''\n",
    "\n",
    "  filepath = get_filepath(experiment_name)\n",
    "  experiment_df.to_csv(filepath, index=False)\n",
    "\n",
    "\n",
    "def load_checkpoint(experiment_name):\n",
    "  '''\n",
    "  Loads a checkpoint of an experiment given its name\n",
    "\n",
    "          Parameters:\n",
    "                  experiment_name (string): The name of the experiment\n",
    "          Returns:\n",
    "                  experiment_df (pandas.DataFrame): A dataframe with the experiment history\n",
    "                  last_bo_experiment (int): The number of the last bayesian optimization experiment\n",
    "                  last_rs_experiment (int): The number of the last random search experiment\n",
    "                  last_bo_iteration (int): The last iteration in the last bayesian optimization experiment\n",
    "                  last_rs_iteration (int): The last iteration in the last random search experiment\n",
    "                  past_observations (pandas.Dataframe): With the learning rate and gammas of the last experiments observations\n",
    "                  past_results (numpy.array): A numpy array of shape (1,n) containing the lower mean rewards of the experiments. n being the number of iterations done in the experiment\n",
    "                  best_init_y (float): The best lower_mean_reward bound obtained in the last experiment\n",
    "                  best_rs_lr: The best random search learning rate found \n",
    "                  best_rs_gamma: The best gamma found \n",
    "                  best_rs_r: The best lower mean reward bound obtained in the last experiment\n",
    "                  bo_done: If the bayesian optimization has finised\n",
    "  '''\n",
    "  # First we retrieve the dataframe from GDrive\n",
    "  filepath = get_filepath(experiment_name)\n",
    "  experiment_df = pd.read_csv(filepath)\n",
    "  \n",
    "  \n",
    "\n",
    "  method, exp, iter, lr, gamma, rlb, best_lr, best_gamma, best_rlb = experiment_df.iloc[-1]\n",
    "\n",
    "  if method == 0:\n",
    "    bo_done = False\n",
    "    last_bo_experiment = int(exp)\n",
    "    last_rs_experiment = 0\n",
    "\n",
    "    # Plus one because we want to start in the next one\n",
    "    last_bo_iteration = int(iter)+1\n",
    "    last_rs_iteration = 1\n",
    "\n",
    "  else:\n",
    "    bo_done = True\n",
    "    last_bo_experiment = experiment_configurations.shape[1]-1\n",
    "    last_rs_experiment = int(exp)\n",
    "\n",
    "    # Plus one because we want to start in the next one\n",
    "    last_bo_iteration = experiment_configurations.shape[2]-1\n",
    "    last_rs_iteration = int(iter)+1\n",
    "\n",
    "  # Now lets get the current experiment data\n",
    "  current_experiment_df = experiment_df[(experiment_df[\"method\"]==method) & (experiment_df[\"experiment\"]==int(exp))]\n",
    "  past_results = current_experiment_df.reward_lower_bound.to_numpy().reshape(-1,1)\n",
    "  past_observations = current_experiment_df[[\"learning_rate\", \"gamma\"]].copy()\n",
    "  past_observations = past_observations.reset_index()\n",
    "  past_observations.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "\n",
    "  rs_experiment_df = experiment_df[experiment_df[\"method\"]==0]\n",
    "  if rs_experiment_df.empty:\n",
    "      best_rs_lr = 0\n",
    "      best_rs_gamma = 0\n",
    "      best_rs_r = 0\n",
    "  else:\n",
    "      best_rs_lr = rs_experiment_df.iloc[-1][\"best_learning_rate\"]\n",
    "      best_rs_gamma = rs_experiment_df.iloc[-1][\"best_gamma\"]\n",
    "      best_rs_r = rs_experiment_df.iloc[-1][\"best_reward_lower_bound\"]\n",
    "             \n",
    "  return (experiment_df, \n",
    "         last_bo_experiment, \n",
    "         last_rs_experiment, \n",
    "         last_bo_iteration, \n",
    "         last_rs_iteration, \n",
    "         past_observations, \n",
    "         past_results, \n",
    "         best_init_y, \n",
    "         best_rs_lr, \n",
    "         best_rs_gamma, \n",
    "         best_rs_r,\n",
    "         bo_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AChh4uxu6lk"
   },
   "source": [
    "Now let's set the configuration for the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FF5-qYTvN2zZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# The name of the file (WITHOUT EXTENSION) where the history of experiments will be saved\n",
    "experiment_name = \"lunar_lander_learning_rate_gamma_hebo\"\n",
    "# If true, this will look for the experiment history .csv in google drive and continue from there\n",
    "continue_from_checkpoint = False\n",
    "\n",
    "# Number of experiments per method\n",
    "n_experiments = 8\n",
    "\n",
    "# Number of iterations per experiment after the first random point being evaluated\n",
    "n_iterations = 25\n",
    "\n",
    "# Number of methods\n",
    "n_methods = 2\n",
    "\n",
    "# Index of Bayesian Optimization method\n",
    "bo_method = 0\n",
    "\n",
    "# Index of Random Search method\n",
    "rs_method = 1\n",
    "\n",
    "# Number of Hyperparameters\n",
    "n_hyperparameters = 2\n",
    "\n",
    "# Arrays containing the results and configurations of experiments\n",
    "experiment_results = np.zeros((n_methods, n_experiments, n_iterations+1))\n",
    "experiment_configurations = np.zeros((n_methods, n_experiments, n_iterations+1, n_hyperparameters))\n",
    "\n",
    "# Now load checkpoint if necessary\n",
    "if continue_from_checkpoint:\n",
    "  experiment_df, last_bo_experiment, last_rs_experiment, last_bo_iteration, last_rs_iteration, past_observations, past_rewards, best_init_y, best_observed_lr_rs, best_observed_gamma_rs, best_observed_result_rs, bo_done = load_checkpoint(experiment_name) \n",
    "  \n",
    "  hebo_seq = HEBO(hyperparams_space, \n",
    "                  model_name = 'gpy')\n",
    "  hebo_seq.X = past_observations\n",
    "  hebo_seq.y = past_rewards\n",
    "    \n",
    "else: \n",
    "  experiment_df = create_experiment_df()\n",
    "  \n",
    "  save_checkpoint(experiment_df, \n",
    "                  experiment_name)\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHTBIpJSQJVZ"
   },
   "source": [
    "First we try the bayesian optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jScxWwkmzEr",
    "outputId": "96830da6-8047-4f36-ad5d-d54acfde7d84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 0\n"
     ]
    }
   ],
   "source": [
    "if continue_from_checkpoint:\n",
    "    init_experiment = last_bo_experiment\n",
    "    init_iteration = last_bo_iteration\n",
    "else:\n",
    "    init_experiment = 0\n",
    "    init_iteration = 1\n",
    "    bo_done = False\n",
    "\n",
    "if not bo_done:\n",
    "    for e in range(init_experiment, n_experiments):\n",
    "        print(f\"EXPERIMENT {e}\")\n",
    "        if not (continue_from_checkpoint and init_experiment == e):\n",
    "            # Create our optimizer\n",
    "            hebo_seq = HEBO(hyperparams_space, \n",
    "                        model_name = 'gpy')\n",
    "\n",
    "\n",
    "            # Do the first observation\n",
    "            new_candidates = hebo_seq.suggest(n_suggestions=3)\n",
    "            hebo_seq.observe(new_candidates, target_function(new_candidates))\n",
    "     \n",
    "            # Initialize the observations\n",
    "            best_observed_result_bo = hebo_seq.y.min()\n",
    "            best_observed_lr_bo, best_observed_gamma_bo = get_hyp_values(new_candidates)\n",
    "\n",
    "            # Update the experiment history\n",
    "            experiment_df = update_experiment_history(bo_method,\n",
    "                                                  e,\n",
    "                                                  0,\n",
    "                                                  best_observed_lr_bo, # The learning rate selected for this iteration\n",
    "                                                  best_observed_gamma_bo, # The gamma selected for this iteration\n",
    "                                                  -best_observed_result_bo, # The reward lower bound of the model (the minus is added because we multiplied by -1 the reward in the target function in order to maximize it as HEBO uses LCB, so now to obtain the real reading we multiply by -1 again)\n",
    "                                                  best_observed_lr_bo, # The best learning rate\n",
    "                                                  best_observed_gamma_bo, # The best gamma\n",
    "                                                  -best_observed_result_bo, # The reward lower bound (the minus is added because we multiplied by -1 the reward in the target function in order to maximize it as HEBO uses LCB, so now to obtain the real reading we multiply by -1 again)\n",
    "                                                  experiment_df,\n",
    "                                                  experiment_name)\n",
    "\n",
    "        else:\n",
    "            # Initialize the observations\n",
    "            best_observed_result_bo = hebo_seq.y.min()\n",
    "            best_observed_lr_bo = experiment_df.iloc[-1][\"best_learning_rate\"]\n",
    "            best_observed_gamma_bo = experiment_df.iloc[-1][\"best_gamma\"]\n",
    "        \n",
    "    \n",
    "        for i in range(init_iteration, n_iterations+1):\n",
    "            # Obtain new candidates from HEBO\n",
    "            new_candidates = hebo_seq.suggest(n_suggestions=1)\n",
    "            candidate_lr_bo, candidate_gamma_bo = get_hyp_values(new_candidates)\n",
    "\n",
    "            # Evaluate new candidates\n",
    "            hebo_seq.observe(new_candidates, target_function(new_candidates))\n",
    "\n",
    "            if (hebo_seq.y.min() < best_observed_result_bo):\n",
    "                best_observed_result_bo = hebo_seq.y.min()\n",
    "                best_observed_lr_bo, best_observed_gamma_bo = get_hyp_values(new_candidates)\n",
    "\n",
    "            # Show iteration info\n",
    "            print(f\"Number of iteration: {i}\")\n",
    "            print(f\"  - Unnormalized learning rate: {candidate_lr_bo}\")\n",
    "            print(f\"  - Unnormalized gamma: {candidate_gamma_bo}\")\n",
    "            print(f\"  - Unstandardized lower reward bound: {-hebo_seq.y[-1][0]}\")\n",
    "            print(f\"Best point performs this way: {-best_observed_result_bo}\")\n",
    "\n",
    "\n",
    "            experiment_df = update_experiment_history(bo_method, \n",
    "                                                      e, \n",
    "                                                      i,\n",
    "                                                      candidate_lr_bo, # The lr selected for this iteration\n",
    "                                                      candidate_gamma_bo, # The gamma selected for this iteration\n",
    "                                                      -hebo_seq.y[-1][0], # The reward lower bound of the model (the minus is added because we multiplied by -1 the reward in the target function in order to maximize it as HEBO uses LCB, so now to obtain the real reading we multiply by -1 again)\n",
    "                                                      best_observed_lr_bo, # The best learning rate\n",
    "                                                      best_observed_gamma_bo, # The best gamma\n",
    "                                                      -best_observed_result_bo, # The reward lower bound (the minus is added because we multiplied by -1 the reward in the target function in order to maximize it as HEBO uses LCB, so now to obtain the real reading we multiply by -1 again) \n",
    "                                                      experiment_df,\n",
    "                                                      experiment_name)\n",
    "        \n",
    "        init_iteration = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "hebo-env",
   "language": "python",
   "name": "hebo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
