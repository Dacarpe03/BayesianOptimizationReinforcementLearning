{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR0o63R4Xa6F"
   },
   "source": [
    "# Step 1: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GEDX7xi_IE7",
    "outputId": "69a57850-ac64-4dad-90d3-af5717bf7b22"
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip3 install setuptools==65.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQXJ4h82Xt42",
    "outputId": "6cbe86cd-4e5d-4944-99fc-28c1b8f8a03e"
   },
   "outputs": [],
   "source": [
    "# !pip install gym[box2d]\n",
    "# !pip install stable-baselines3[extra] pyglet==1.5.27\n",
    "# !pip install botorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kpv4u1hZX2Vv"
   },
   "source": [
    "# Step 2: Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riwFQZMNtU_4"
   },
   "source": [
    "Libraries used for the Bayesian Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4mw25s5cYPuv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 10:56:17.193273: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-09 10:56:17.985642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/daniel/miniconda3/envs/bo-drl-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import gym\n",
    "\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import botorch\n",
    "from botorch.utils.transforms import standardize, normalize, unnormalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSAValUHtY_h"
   },
   "source": [
    "Libraries used to save checkpoints in GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8DoSb8utUAO",
    "outputId": "7fd49b1a-d92a-469a-d5f5-a6510f86eb5c"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "import pandas as pd\n",
    "\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xdYx-gMYyeV"
   },
   "source": [
    "# Step 3: Define objetive function\n",
    "This will be the lower bound of the mean reward of a trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XNRXK99lY_T9"
   },
   "outputs": [],
   "source": [
    "def get_hyp_values(hyperparams_tensor):\n",
    "  '''\n",
    "  Returns a tuple of values from a tensor containing a hyperparameter configuration\n",
    "\n",
    "          Parameters:\n",
    "                  hyperparams_tensor (torch.DoubleTensor): A tensor of size 1xn (1 row, n columns) with n being the number of hyperparameters to tune\n",
    "          \n",
    "          Returns:\n",
    "                  hyperparams_tuple (tuple): A tuple with the unpacked values of the hyperparams_tensor \n",
    "\n",
    "  '''\n",
    "  hyperparams_list = [hyperparams_tensor[0][i].item() for i in range(len(hyperparams_tensor[0]))]\n",
    "  hyperparams_tuple = tuple(hyperparams_list)\n",
    "  return tuple(hyperparams_list)\n",
    "\n",
    "\n",
    "def create_model(hyp,\n",
    "                 policy='MlpPolicy',\n",
    "                 env_name='CartPole-v1'):\n",
    "  '''\n",
    "  Returns a PPO model given a policy, environment, and hyperparameters of PPO\n",
    "\n",
    "          Parameters:\n",
    "                  hyp (float): The value of the hyperparameter to train the model with\n",
    "                  policy (str): The NN to train with PPO in the environment. Default is 'MlpPolicy'\n",
    "                  env (stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv): Specifies the gym environment to use for the training\n",
    "\n",
    "          Returns:\n",
    "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
    "  '''\n",
    "  # lr,  = get_hyp_values(hyperparams)\n",
    "  env = make_vec_env(env_name, n_envs=1)\n",
    "  model = PPO(policy = policy,\n",
    "              env = env,\n",
    "              learning_rate = 0.04,\n",
    "              n_steps = 1024,\n",
    "              batch_size = 64,\n",
    "              n_epochs = 4,\n",
    "              gamma = hyp,\n",
    "              gae_lambda = 0.98,\n",
    "              ent_coef = 0.01,\n",
    "              verbose=0)\n",
    "  \n",
    "  return model\n",
    "\n",
    "\n",
    "def train_model(model, timesteps=10000):\n",
    "  '''\n",
    "  Trains a PPO model during a number of timesteps\n",
    "          \n",
    "          Parameters:\n",
    "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
    "                  timesteps (int): The number of timesteps used to train the model\n",
    "\n",
    "          Returns:\n",
    "                  None\n",
    "  '''\n",
    "  model.learn(total_timesteps=timesteps)\n",
    "  return\n",
    "\n",
    "\n",
    "def evaluate_model(model, \n",
    "                   rl_env_name='CartPole-v1', \n",
    "                   n_eval_episodes=25):\n",
    "  '''\n",
    "  Evaluates the model for a number of episodes in a specified environment, this environment MUST be the same as the one the model has been trained in.\n",
    "\n",
    "          Parameters:\n",
    "                  model (stable_baselines3.ppo.ppo.PPO): The model to train\n",
    "                  rl_env_name (str): The name of the gym environment where the model has been trained\n",
    "                  n_eval_episodes (int): The number of episodes for which the model will be evaluated to obtain a mean and standard deviation\n",
    "\n",
    "          Returns:\n",
    "                  lower_mean_reward (float): A tensor of size 1x1 (1 row, 1 column) containing the mean_reward\n",
    "  '''\n",
    "  eval_env = gym.make(rl_env_name)\n",
    "  mean_reward, std_reward = evaluate_policy(model, \n",
    "                                            eval_env, \n",
    "                                            n_eval_episodes=n_eval_episodes, \n",
    "                                            deterministic=True)\n",
    "  \n",
    "  print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n",
    "  lower_mean_reward = mean_reward - std_reward\n",
    "  return lower_mean_reward\n",
    "\n",
    "\n",
    "def target_function(hyperparams, \n",
    "                    timesteps=10000,\n",
    "                    rl_env_name='CartPole-v1'):\n",
    "  '''\n",
    "  Given a hyperparameter configuration, evaluates their performance\n",
    "          Parameters:\n",
    "                  hyperparams (float): The value of the learning_rate to train the model with\n",
    "                  timesteps (int): timesteps (int): The number of timesteps used to train the model\n",
    "                  rl_env_name (str): The name of the gym environment where the model has been trained\n",
    "\n",
    "          Returns:\n",
    "                  lower_mean_reward (float): A tensor of size 1x1 (1 row, 1 column) containing the mean_reward\n",
    "  '''\n",
    "  model = create_model(hyperparams, env_name=rl_env_name)\n",
    "  \n",
    "  train_model(model, \n",
    "              timesteps)\n",
    "  \n",
    "  lower_mean_reward = evaluate_model(model, \n",
    "                                     rl_env_name)\n",
    "  \n",
    "  return lower_mean_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjdeNdB_jJPj"
   },
   "source": [
    "# Step 4: Define hyperparameters to tune\n",
    "First define the bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WXwDQp5oilS4"
   },
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "# Define here the list of parameters to tune\n",
    "hyperparams_list = [lr]\n",
    "# Define the lower bounds of the parameters\n",
    "lower_bounds = [0.0001]\n",
    "# Define the upper bounds of the parameters\n",
    "upper_bounds = [0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ILw3cqAjkbg"
   },
   "source": [
    "Then convert lists to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PnU1DMG8jnuc"
   },
   "outputs": [],
   "source": [
    "# Create tensors with the hyperparameters configurations and bounds for BOTorch to use\n",
    "hyperparams_tensor = torch.DoubleTensor([hyperparams_list])\n",
    "bounds_tensor = torch.DoubleTensor([lower_bounds, upper_bounds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igatCqADkIjz"
   },
   "source": [
    "# Step 5: Define functions needed for the Bayesian Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VYL5gMk_kK7m"
   },
   "outputs": [],
   "source": [
    "def generate_initial_data(upper_bound, \n",
    "                          lower_bound, \n",
    "                          n=3):\n",
    "  '''\n",
    "  Gets n values of the hyperparameter's bounded space and evaluates them\n",
    "          Parameters:\n",
    "                upper_bound (float): The upper bound of the hyperparameter value\n",
    "                lower_bound (float): The lower bound of the hyperparameter value\n",
    "                n (int): The number of initial points to get. Default is 3\n",
    "          \n",
    "          Returns:\n",
    "                train_x (torch.DoubleTensor): A tensor of size (n, 1) (n rows and 1 column) with the initial points\n",
    "                exact_obj (torch.DoubleTensor): A tensor of size (n, 1) (n rows and 1 column) containing the evaluation of the model with the sampled hyperparameters values\n",
    "                best_observed_vale: The best evaluation of the hyperparameters\n",
    "  '''\n",
    "  # Create our initial hyperparameter values\n",
    "  train_x = torch.rand(n, 1, dtype=torch.double) * (upper_bound - lower_bound) + lower_bound\n",
    "\n",
    "  # Evaluate them and store them in a torch.Tensor\n",
    "  exact_obj = torch.tensor([[target_function(float(hyp))] for hyp in train_x])\n",
    "\n",
    "  # Get the best observed value\n",
    "  best_observed_value = exact_obj.max().item()\n",
    "  \n",
    "  return train_x, exact_obj, best_observed_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1MUKzEZkmAxw"
   },
   "outputs": [],
   "source": [
    "from botorch.acquisition.analytic import ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.utils.transforms import standardize, normalize, unnormalize\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "from botorch import fit_gpytorch_model\n",
    "\n",
    "def compute_acquisition_function(single_model, \n",
    "                                 best_init_y,\n",
    "                                 l_bound=-2.,\n",
    "                                 h_bound=10.,\n",
    "                                 resolution=1000):\n",
    "  \n",
    "  '''\n",
    "  Evaluates the acquisition function in the discretized space of the bounded space\n",
    "          Parameters:\n",
    "                  single_model (botorch.models.gp_regression): A Gaussian Process regression model\n",
    "                  best_init_y (float): The best lower_bound_reward obtained until the moment\n",
    "                  l_bound (float): The lower bound of the hyperparameter value\n",
    "                  h_bound (float): The upper bound of the hyperparameter value\n",
    "                  resolution (int): The number of discretized points of the hyperparameter value bounded space\n",
    "\n",
    "          Returns:\n",
    "                  result_tensor (torch.Tensor): A tensor of shape 1xresolution (1 row, {resolution} colums) containing the evaluations of the acquisition function\n",
    "  '''\n",
    "  # Discretize the bounded hyperparameter value space\n",
    "  linspace = torch.linspace(l_bound, \n",
    "                            h_bound,\n",
    "                            steps=resolution)\n",
    "  x_test = torch.tensor([linspace[0]]).unsqueeze(-1)\n",
    "\n",
    "  # Compute our acquistion function\n",
    "  EI = ExpectedImprovement(model=single_model, \n",
    "                           best_f=best_init_y, \n",
    "                           maximize=True)\n",
    "  result = []\n",
    "\n",
    "  # Evaluate the acquisition function in the discretized space\n",
    "  for x in linspace:\n",
    "    x_test = torch.tensor([x]).unsqueeze(-1)\n",
    "    result.append(EI(x_test))\n",
    "  \n",
    "  # Convert results list in a tensor\n",
    "  result_tensor = torch.tensor(result)\n",
    "  return result_tensor\n",
    "\n",
    "\n",
    "def print_acquisition_function(acq_fun, \n",
    "                               iteration,\n",
    "                               l_bound=-2.,\n",
    "                               h_bound=10.,\n",
    "                               resolution=1000, \n",
    "                               suggested=None):\n",
    "  '''\n",
    "  Plots the acquistion function given a series of evaluations previously computed\n",
    "          Parameters:\n",
    "                  acq_fun (torch.Tensor): A tensor of shape 1xresolution with the evaluations of the acquisition function\n",
    "                  iteration (int): The iteration number of the Gaussian Process\n",
    "                  l_bound (float): The lower bound of the hyperparameter value\n",
    "                  h_bound (float): The upper bound of the hyperparameter value\n",
    "                  resolution (int): The number of discretized points of the hyperparameter value bounded space\n",
    "                  suggested (float): The suggested value of the hyperparameter\n",
    "\n",
    "          Returns:\n",
    "                  None\n",
    "  '''\n",
    "\n",
    "  # Discretize the hyperparameter value space\n",
    "  x = torch.linspace(l_bound, h_bound, steps=resolution).detach().numpy()\n",
    "  x_new = x.reshape((resolution,-1))\n",
    "  z = acq_fun\n",
    "\n",
    "  # Get the value that maximizes the acquisition value\n",
    "  max_acq_fun = x[((acq_fun == acq_fun.max().item()).nonzero(as_tuple=True)[0])]\n",
    "\n",
    "  # Plot our data\n",
    "  data = go.Scatter(x=x, y=z, line_color=\"yellow\")\n",
    "\n",
    "  # Axis\n",
    "  fig = go.Figure(data=data)\n",
    "  fig.update_layout(title=\"Expected Improvement acquisition function. Iteration \" + str(iteration), xaxis_title=\"input\", yaxis_title=\"output\")\n",
    "\n",
    "  # If we have suggested a point, draw a red vertical line on it, otherwise draw a red vertical line on the value that maximizes the acq function\n",
    "  if(suggested==None):\n",
    "    fig.add_vline(x=max_acq_fun, line_width=3, line_color=\"red\")\n",
    "  else:\n",
    "    fig.add_vline(x=float(suggested[0][0]), line_width=3, line_color=\"red\")\n",
    "  fig.show()\n",
    "\n",
    "\n",
    "def compute_predictive_distribution(single_model,\n",
    "                                    best_init_y,\n",
    "                                    l_bound=-2.,\n",
    "                                    h_bound=10., \n",
    "                                    resolution=1000):\n",
    "  '''\n",
    "  Computes the predictive distribution of the functions given a Gaussian Process Regresion model\n",
    "          Parameters:\n",
    "                  single_model (botorch.models.gp_regression): A Gaussian Process regression model\n",
    "                  best_init_y (float): The best lower_bound_reward obtained until the moment\n",
    "                  l_bound (float): The lower bound of the hyperparameter value\n",
    "                  h_bound (float): The upper bound of the hyperparameter value\n",
    "                  resolution (int): The number of discretized points of the hyperparameter value bounded space\n",
    "\n",
    "          Returns:\n",
    "                  means_tensor (torch.Tensor): A tensor of shape 1xresolution (1 row, {resolution} colums) containing the means of the discretized points\n",
    "                  variances_tensor (torch.Tensor): A tensor of shape 1xresolution (1 row, {resolution} colums) containing the variances of the discretized points\n",
    "  '''\n",
    "  # Discretize the hyperparameter value bounded space\n",
    "  linspace = torch.linspace(l_bound, h_bound, steps=resolution)\n",
    "  x_test = torch.tensor([linspace[0]]).unsqueeze(-1)\n",
    "\n",
    "  # Initialize our lists, result for the means\n",
    "  result = []\n",
    "  variances = []\n",
    "\n",
    "  # Evaluate means and variances given the GP model\n",
    "  for x in linspace:\n",
    "    x_test = torch.tensor([x]).unsqueeze(-1)\n",
    "    result.append(single_model.posterior(x_test).mean)\n",
    "    variances.append(single_model.posterior(x_test).variance)\n",
    "  \n",
    "  # Convert results into tensors\n",
    "  means_tensor = torch.tensor(result)\n",
    "  variances_tensor = torch.tensor(variances)\n",
    "\n",
    "  return torch.tensor(result), torch.tensor(variances)\n",
    "\n",
    "\n",
    "def print_predictive_mean(predictive_mean, \n",
    "                          predictive_variance,\n",
    "                          iteration, \n",
    "                          l_bound=-2.,\n",
    "                          h_bound=10.,\n",
    "                          resolution=1000,\n",
    "                          suggested=None,\n",
    "                          old_obs=[],\n",
    "                          old_values=[]):\n",
    "  '''\n",
    "  Plots the function distribution obtained by the Gaussian Process\n",
    "          Parameters:\n",
    "                  predictive_mean (torch.Tensor): A tensor of shape 1xresolution (1 row, {resolution} colums) containing the means of the discretized points\n",
    "                  predictive_variance (torch.Tensor): A tensor of shape 1xresolution (1 row, {resolution} colums) containing the variances of the discretized points\n",
    "                  iteration (int): The iteration number of the Gaussian Process\n",
    "                  l_bound (float): The lower bound of the hyperparameter value\n",
    "                  h_bound (float): The upper bound of the hyperparameter value\n",
    "                  resolution (int): The number of discretized points of the hyperparameter value bounded space\n",
    "                  suggested (float): The suggested value of the hyperparameter\n",
    "                  old_obs (list): A list with previous candidates selected by the optimization of the acquisition function\n",
    "                  old_values (list): A list with previous lower bound reward of the hyperparameter candidates selected previously by the optimization of the acquisition function\n",
    "\n",
    "          Returns:\n",
    "                  None\n",
    "  '''\n",
    "\n",
    "  # Discretize the space\n",
    "  x = torch.linspace(l_bound, h_bound, steps=resolution).detach().numpy()\n",
    "  x_new = x.reshape((resolution,-1))\n",
    "  z = predictive_mean\n",
    "\n",
    "  # Get the value that maximizes the predictive mean\n",
    "  max_predictive_mean = x[((predictive_mean == predictive_mean.max().item()).nonzero(as_tuple=True)[0])]\n",
    "\n",
    "  # Create figure\n",
    "  fig = go.Figure()\n",
    "\n",
    "  # Plot upper bound of the expected reward (predictive mean + predictive variance)\n",
    "  fig.add_trace(go.Scatter(x=x, \n",
    "                           y= predictive_mean + np.sqrt(predictive_variance),\n",
    "                           mode='lines',\n",
    "                           line=dict(color=\"#19D3F3\",width =0.1),\n",
    "                           name='upper bound'))\n",
    "  \n",
    "  # Plot predictive mean of each point's expected reward\n",
    "  fig.add_trace(go.Scatter(x=x, \n",
    "                           y= predictive_mean,\n",
    "                           mode='lines',\n",
    "                           line=dict(color=\"blue\"),\n",
    "                           fill='tonexty',\n",
    "                           name='predictive mean'))\n",
    "  \n",
    "  # Plot lower bound of the expected reward (predictive mean - predictive variance)\n",
    "  fig.add_trace(go.Scatter(x=x, y= predictive_mean - np.sqrt(predictive_variance),\n",
    "                         mode='lines',\n",
    "                         line=dict(color=\"blue\", width =0.1),\n",
    "                         fill='tonexty',\n",
    "                         name='lower bound'))\n",
    "  \n",
    "  \n",
    "  # Axis\n",
    "  fig.update_layout(title=\"GP Predictive distribution. Iteration \" + str(iteration), xaxis_title=\"Gamma\", yaxis_title=\"Expected reward\", showlegend=False)\n",
    "\n",
    "  # Add a vertical line in the point that maximizes the expected reward or in the suggested point if provided in the parameters\n",
    "  if(suggested==None):\n",
    "    fig.add_vline(x=max_predictive_mean, line_width=3, line_color=\"red\")\n",
    "  else:\n",
    "    fig.add_vline(x=float(suggested[0][0]), line_width=3, line_color=\"red\")  \n",
    "\n",
    "  # Plot old values\n",
    "  if(len(old_obs)>0):\n",
    "    fig.add_trace(go.Scatter(x=old_obs, y=old_values, mode = 'markers', marker_color=\"black\", marker_size=10))\n",
    "\n",
    "  fig.show()\n",
    "\n",
    "\n",
    "def visualize_functions(single_model,\n",
    "                        best_init_y,\n",
    "                        best_candidate,\n",
    "                        candidate_acq_fun,\n",
    "                        iteration,\n",
    "                        previous_observations,\n",
    "                        previous_values,\n",
    "                        bounds,\n",
    "                        best_candidate_normalized):\n",
    "  '''\n",
    "  Function that visualizes the acquisition function and gaussian process\n",
    "          Parameters:\n",
    "                  single_model (botorch.models.gp_regression): A Gaussian Process regression model\n",
    "                  best_init_y (float): The best lower_bound_reward obtained until the moment\n",
    "                  best_candidate (float): The normalized best candidate until this iteration of the optimization process \n",
    "                  candidate_acq_fun (float): The selected unnormalized candidate in this iteration of the optimization process\n",
    "                  iteration (int): The iteration number of the optimization process\n",
    "                  previous_observations (list): A list with previous candidates selected by the optimization of the acquisition function\n",
    "                  previous_values (list): A list with previous lower bound reward of the hyperparameter candidates selected previously by the optimization of the acquisition function\n",
    "                  bounds (torch.Tensor): Normalized bounds of the hyperparameter values in the form of tensors of shape 2x1 (2 rows, 1 column), first row containing lower bound, second containing upper bound\n",
    "\n",
    "          Returns:\n",
    "                  None\n",
    "\n",
    "  '''\n",
    "\n",
    "  # Compute the mean and variance of the function distribution given by the gaussian process (our single model)\n",
    "  predictive_mean, predictive_variance = compute_predictive_distribution(single_model, \n",
    "                                                                         best_init_y,\n",
    "                                                                         l_bound=0,\n",
    "                                                                         h_bound=1)\n",
    "  # Plot the distribution\n",
    "  print_predictive_mean(predictive_mean, \n",
    "                        predictive_variance, \n",
    "                        iteration,\n",
    "                        suggested=candidate_acq_fun, \n",
    "                        old_obs=previous_observations,\n",
    "                        old_values=previous_values,\n",
    "                        l_bound=bounds[0][0],\n",
    "                        h_bound=bounds[1][0])\n",
    "  \n",
    "  # Compute the acquisition function \n",
    "  acq_fun = compute_acquisition_function(single_model, \n",
    "                                         best_init_y, \n",
    "                                         l_bound=0,\n",
    "                                         h_bound=1)\n",
    "\n",
    "  # Plot the acquisition function\n",
    "  print_acquisition_function(acq_fun,\n",
    "                             iteration,\n",
    "                             suggested=candidate_acq_fun, \n",
    "                             l_bound=bounds[0][0],\n",
    "                             h_bound=bounds[1][0])\n",
    "  \n",
    "\n",
    "def get_next_points_and_visualize_norm(init_x,\n",
    "                                       init_y, \n",
    "                                       best_init_y, \n",
    "                                       normalized_bounds, \n",
    "                                       iteration, \n",
    "                                       previous_observations,\n",
    "                                       previous_values,\n",
    "                                       bounds,\n",
    "                                       n_points=1,\n",
    "                                       visualize=False):\n",
    "  '''\n",
    "  Function that computes the next point to add to the Gaussian Process and visualizes the acquisition function and function distribution\n",
    "          Parameters:\n",
    "                  init_x (torch.Tensor): A tensor of shape {iteration}x1 containing the previous hyperparameters\n",
    "                  init_y (torch.Tensor): A tensor of shape {iteration}x1 containing the previous rewards of the models trained with x_init hyperparameters values\n",
    "                  best_init_y (float): Best reward obtained until the moment\n",
    "                  normalized_bounds (torch.Tensor): Normalized bounds of the hyperparameter values in the form of tensors of shape 2x1 (2 rows, 1 column), first row containing lower bound, second containing upper bound\n",
    "                  iteration (int): The iteration number of the Bayesian Optimization process\n",
    "                  previous_observations (list): A list with previous hyperparameter values\n",
    "                  previous_values (list): A list with previous rewards obtained\n",
    "                  bounds (torch.Tensor): Actual bounds of the hyperparameter values in the form of tensors of shape 2x1 (2 rows, 1 column), first row containing lower bound, second containing upper bound\n",
    "                  n_points (int): Number of candidates to obtain for the next iteration. Default is 1\n",
    "                  visualize (bool): If True, then visualize the GP. Default is True\n",
    "\n",
    "          Returns:\n",
    "                  candidates (torch.Tensor): A tensor of shape 1x1 containing the value of the hyperparameter that optimizes the acquisition function\n",
    "  '''\n",
    "  # Create our model with the points\n",
    "  single_model = SingleTaskGP(init_x, init_y)\n",
    "\n",
    "  mll = ExactMarginalLogLikelihood(single_model.likelihood, single_model)\n",
    "  fit_gpytorch_model(mll)\n",
    "\n",
    "  # Instantiaet the acquisition function given our model\n",
    "  EI = ExpectedImprovement(model=single_model, best_f=best_init_y, maximize=True)\n",
    "  \n",
    "  # Optimize the acquisition function\n",
    "  candidates, _ = optimize_acqf(acq_function=EI, \n",
    "                                bounds=normalized_bounds, \n",
    "                                q=n_points,\n",
    "                                num_restarts=200,\n",
    "                                raw_samples=512, \n",
    "                                options={\"batch_limit\": 5, \"maxiter\": 200})\n",
    "  \n",
    "  # Get the best candidate unnormalized\n",
    "  best_candidate = unnormalize(init_x[((init_y == best_init_y).nonzero(as_tuple=True)[0])][0][0], bounds=normalized_bounds)\n",
    "  # Get our best candidate normalized for the GP to use\n",
    "  best_candidate_normalized = init_x[((init_y == best_init_y).nonzero(as_tuple=True)[0])][0][0]\n",
    "\n",
    "  # Visualize acquisition functions and GP regression\n",
    "  if visualize:\n",
    "    visualize_functions(single_model, \n",
    "                        best_init_y,\n",
    "                        best_candidate,\n",
    "                        unnormalize(candidates, bounds=bounds),\n",
    "                        iteration, previous_observations,\n",
    "                        previous_values, \n",
    "                        bounds, \n",
    "                        best_candidate_normalized)\n",
    "\n",
    "  return candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E038wSN_MFoQ"
   },
   "source": [
    "# Step 6: Set experiments' configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWEEF07XuQPj"
   },
   "source": [
    "First let us define functions to save and load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8sde3AjauPiS"
   },
   "outputs": [],
   "source": [
    "def create_experiment_df():\n",
    "  '''\n",
    "  Creates an empty dataframe to save checkpoints\n",
    "          Parameters:\n",
    "\n",
    "          Returns:\n",
    "                  experiment_df (pandas.DataFrame): An empty Dataframe with columns specified below that will be used to save the experiment history\n",
    "  '''\n",
    "  columns = [\"method\",\n",
    "             \"experiment\",\n",
    "             \"iteration\",\n",
    "             \"gamma\",\n",
    "             \"reward_lower_bound\",\n",
    "             \"best_gamma\",\n",
    "             \"best_reward_lower_bound\"\n",
    "             ]\n",
    "\n",
    "  experiment_df = pd.DataFrame(columns=columns)\n",
    "  return experiment_df\n",
    "\n",
    "\n",
    "def get_filepath(experiment_name):\n",
    "  '''\n",
    "  Returns the path of the csv of the experiment specified.\n",
    "          Parameters:\n",
    "                  experiment_name (string): The name of the experiment\n",
    "\n",
    "          Returns:\n",
    "                  filepath (string): The path to the .csv file that has the data of the experiment\n",
    "  '''\n",
    "  filepath = f\"./{experiment_name}.csv\"\n",
    "  return filepath\n",
    "\n",
    "\n",
    "def update_experiment_history(method, \n",
    "                              experiment_number, \n",
    "                              iteration,\n",
    "                              hyp,\n",
    "                              reward_lower_bound,\n",
    "                              best_hyp,\n",
    "                              best_reward_lower_bound,\n",
    "                              experiment_df,\n",
    "                              experiment_name):\n",
    "  '''\n",
    "  Updates the experiment dataframe and saves it in GDrive\n",
    "          Parameters:\n",
    "                  method (int): 0 if Bayesian Optimization, 1 if Random Search\n",
    "                  experiment_number (int): The id of the experiment\n",
    "                  iteration (int): The current iteration of the experiment\n",
    "                  hype (float): The hyperparameter value selected for this iteration\n",
    "                  reward_lower_bound (float): The reward's lower bound obtained by the model trained this iteration\n",
    "                  best_hyp (float): The hyperparameter value that has induced the best reward lower bound\n",
    "                  best_reward_lower_bound (float): The best reward lower bound obtained until this iteration\n",
    "                  experiment_df (pandas.DataFrame): The dataframe containing the experiments history\n",
    "                  experiment_name (string): The name of the experiment\n",
    "\n",
    "          Returns:\n",
    "                  concatenated_df (pandas.DataFrame): The updated experiments history dataframe\n",
    "  '''\n",
    "  # Define the columns of the dataframe\n",
    "  columns = [\"method\",\n",
    "             \"experiment\",\n",
    "             \"iteration\",\n",
    "             \"gamma\",\n",
    "             \"reward_lower_bound\",\n",
    "             \"best_gamma\",\n",
    "             \"best_reward_lower_bound\"]\n",
    "\n",
    "  # Set the values of the columns given by the iteration configuration and result\n",
    "  iteration_results = [[method,\n",
    "                        experiment_number,\n",
    "                        iteration,\n",
    "                        hyp,\n",
    "                        reward_lower_bound,\n",
    "                        best_hyp,\n",
    "                        best_reward_lower_bound]]\n",
    "\n",
    "  # Create a one row dataframe for this experiment\n",
    "  new_iteration_df = pd.DataFrame(iteration_results, columns=columns)\n",
    "\n",
    "  # Concatenate the experiments history with this experiment's results\n",
    "  concatenated_df = pd.concat([experiment_df, new_iteration_df], ignore_index=True)\n",
    "\n",
    "  # Save the updated history in google drive\n",
    "  save_checkpoint(concatenated_df,\n",
    "                  experiment_name)\n",
    "  \n",
    "  # Return the concatenated dataframe representing the updated experiment history\n",
    "  return concatenated_df\n",
    "\n",
    "\n",
    "def save_checkpoint(experiment_df,\n",
    "                    experiment_name):\n",
    "  '''\n",
    "  Saves the experiments history dataframe in google drive\n",
    "          Parameters:\n",
    "                  experiment_df (pandas.DataFrame): The dataframe containing the experiments history\n",
    "                  experiment_name (string): The name of the experiment\n",
    "          \n",
    "          Returns:\n",
    "                  None\n",
    "  '''\n",
    "\n",
    "  filepath = get_filepath(experiment_name)\n",
    "  experiment_df.to_csv(filepath, index=False)\n",
    "\n",
    "\n",
    "def load_checkpoint(experiment_name,\n",
    "                    experiment_results,\n",
    "                    experiment_configurations):\n",
    "  '''\n",
    "  Loads a checkpoint of an experiment given its name\n",
    " \n",
    "          Parameters:\n",
    "                  experiment_name (string): The name of the experiment\n",
    "                  experiment_results (numpy.array): A numpy array of three dimensions (method, iteration, best_result)\n",
    "                  expeirment_configuration (numpy.array): A numpy array of three dimensions (method, iteration, best_learning rate)\n",
    "\n",
    "          Returns:\n",
    "                  experiment_df (pandas.DataFrame): A dataframe with the experiment history\n",
    "  '''\n",
    "  # First we retrieve the dataframe from GDrive\n",
    "  filepath = get_filepath(experiment_name)\n",
    "  experiment_df = pd.read_csv(filepath)\n",
    "\n",
    "  # Now we iterate through the rows of the dataframe to update the experiment history numpy arrays that we will use later to compare the methods and plot results\n",
    "  for index, row in experiment_df.iterrows():\n",
    "      # Unpack the columns\n",
    "      method, exp, iter, hyp, rlb, best_hyp, best_rlb = row.values\n",
    "      # Add them to the experiments arrays\n",
    "      experiment_results[int(method)][int(exp)][int(iter)] = best_rlb\n",
    "      experiment_configurations[int(method)][int(exp)][int(iter)] = best_hyp\n",
    "\n",
    "  method, exp, iter, hyp, rlb, best_hyp, best_rlb = experiment_df.iloc[-1]\n",
    "  print(experiment_df.iloc[-1])\n",
    "  if method == 0:\n",
    "    bo_done = False\n",
    "    last_bo_experiment = int(exp)\n",
    "    last_rs_experiment = 0\n",
    "\n",
    "    # Plus one because we want to start in the next one\n",
    "    last_bo_iteration = int(iter)+1\n",
    "    last_rs_iteration = 1\n",
    "\n",
    "  else:\n",
    "    bo_done = True\n",
    "    last_bo_experiment = experiment_configurations.shape[1]-1\n",
    "    last_rs_experiment = int(exp)\n",
    "\n",
    "    # Plus one because we want to start in the next one\n",
    "    last_bo_iteration = experiment_configurations.shape[2]-1\n",
    "    last_rs_iteration = int(iter)+1\n",
    "\n",
    "  # Now lets get the initial data\n",
    "  bo_experiment_df = experiment_df[(experiment_df[\"method\"]==0) & (experiment_df[\"experiment\"]==int(exp))]\n",
    "  init_x = torch.DoubleTensor([[float(lr)] for lr in bo_experiment_df.gamma.values])\n",
    "  init_y = torch.DoubleTensor([[float(reward)] for reward in bo_experiment_df.reward_lower_bound.values])\n",
    "  best_init_y = init_y.max().item()\n",
    "\n",
    "  rs_experiment_df = experiment_df[experiment_df[\"method\"]==0]\n",
    "  if rs_experiment_df.empty:\n",
    "      best_rs_hyp = 0\n",
    "      best_rs_r = 0\n",
    "  else:\n",
    "      best_rs_hyp = rs_experiment_df.iloc[-1][\"best_gamma\"]\n",
    "      best_rs_r = rs_experiment_df.iloc[-1][\"best_reward_lower_bound\"]\n",
    "             \n",
    "  return experiment_df, last_bo_experiment, last_rs_experiment, last_bo_iteration, last_rs_iteration, init_x, init_y, best_init_y, best_rs_hyp, best_rs_r, bo_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AChh4uxu6lk"
   },
   "source": [
    "Now let's set the configuration for the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FF5-qYTvN2zZ",
    "outputId": "c74b90d8-8b5f-4421-e9aa-4f5369c7a43f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# The name of the file (WITHOUT EXTENSION) where the history of experiments will be saved\n",
    "experiment_name = \"cartpole_gamma_low_fidelity\"\n",
    "# If true, this will look for the experiment history .csv in google drive and continue from there\n",
    "continue_from_checkpoint = False\n",
    "\n",
    "# Number of experiments per method\n",
    "n_experiments = 25\n",
    "\n",
    "# Number of iterations per experiment after the first random point being evaluated\n",
    "n_iterations = 30\n",
    "\n",
    "# Number of methods\n",
    "n_methods = 2\n",
    "\n",
    "# Index of Bayesian Optimization method\n",
    "bo_method = 0\n",
    "\n",
    "# Index of Random Search method\n",
    "rs_method = 1\n",
    "\n",
    "# Arrays containing the results and configurations of experiments\n",
    "experiment_results = np.zeros((n_methods, n_experiments, n_iterations+1))\n",
    "experiment_configurations = np.zeros((n_methods, n_experiments, n_iterations+1))\n",
    "\n",
    "# Now load checkpoint if necessary\n",
    "if continue_from_checkpoint:\n",
    "  experiment_df, last_bo_experiment, last_rs_experiment, last_bo_iteration, last_rs_iteration, init_x, init_y, best_init_y, best_observed_candidate_rs, best_observed_result_rs, bo_done = load_checkpoint(experiment_name,\n",
    "                                            experiment_results,\n",
    "                                            experiment_configurations) \n",
    "else: \n",
    "  experiment_df = create_experiment_df()\n",
    "  \n",
    "  save_checkpoint(experiment_df, \n",
    "                  experiment_name)\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHTBIpJSQJVZ"
   },
   "source": [
    "First we try the bayesian optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6jScxWwkmzEr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEXPERIMENT \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (continue_from_checkpoint \u001b[38;5;129;01mand\u001b[39;00m init_experiment \u001b[38;5;241m==\u001b[39m e):\n\u001b[1;32m     13\u001b[0m   \u001b[38;5;66;03m# Sample initial hyperparameter values and evaluate the models obtained with them\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m   init_x, init_y, best_init_y \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_initial_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupper_bounds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mlower_bounds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# We normalize the bounds of the hyperparameters as BOTorch assumes this\u001b[39;00m\n\u001b[1;32m     19\u001b[0m normalized_bounds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0.0\u001b[39m], [\u001b[38;5;241m1.0\u001b[39m]])\n",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m, in \u001b[0;36mgenerate_initial_data\u001b[0;34m(upper_bound, lower_bound, n)\u001b[0m\n\u001b[1;32m     17\u001b[0m train_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(n, \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble) \u001b[38;5;241m*\u001b[39m (upper_bound \u001b[38;5;241m-\u001b[39m lower_bound) \u001b[38;5;241m+\u001b[39m lower_bound\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Evaluate them and store them in a torch.Tensor\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m exact_obj \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[target_function(\u001b[38;5;28mfloat\u001b[39m(hyp))] \u001b[38;5;28;01mfor\u001b[39;00m hyp \u001b[38;5;129;01min\u001b[39;00m train_x])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Get the best observed value\u001b[39;00m\n\u001b[1;32m     23\u001b[0m best_observed_value \u001b[38;5;241m=\u001b[39m exact_obj\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m train_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(n, \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble) \u001b[38;5;241m*\u001b[39m (upper_bound \u001b[38;5;241m-\u001b[39m lower_bound) \u001b[38;5;241m+\u001b[39m lower_bound\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Evaluate them and store them in a torch.Tensor\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m exact_obj \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[43mtarget_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;28;01mfor\u001b[39;00m hyp \u001b[38;5;129;01min\u001b[39;00m train_x])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Get the best observed value\u001b[39;00m\n\u001b[1;32m     23\u001b[0m best_observed_value \u001b[38;5;241m=\u001b[39m exact_obj\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[5], line 102\u001b[0m, in \u001b[0;36mtarget_function\u001b[0;34m(hyperparams, timesteps, rl_env_name)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03mGiven a hyperparameter configuration, evaluates their performance\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m        Parameters:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m                lower_mean_reward (float): A tensor of size 1x1 (1 row, 1 column) containing the mean_reward\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    100\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(hyperparams, env_name\u001b[38;5;241m=\u001b[39mrl_env_name)\n\u001b[0;32m--> 102\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m lower_mean_reward \u001b[38;5;241m=\u001b[39m evaluate_model(model, \n\u001b[1;32m    106\u001b[0m                                    rl_env_name)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lower_mean_reward\n",
      "Cell \u001b[0;32mIn[5], line 58\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, timesteps)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m):\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m  Trains a PPO model during a number of timesteps\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m          \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m                  None\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m  '''\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m   \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-env/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-env/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:250\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    247\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 250\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-env/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:169\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 169\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-env/lib/python3.9/site-packages/stable_baselines3/common/policies.py:624\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    622\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[1;32m    623\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n\u001b[0;32m--> 624\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n\u001b[1;32m    626\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-env/lib/python3.9/site-packages/stable_baselines3/common/distributions.py:89\u001b[0m, in \u001b[0;36mDistribution.get_actions\u001b[0;34m(self, deterministic)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode()\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-env/lib/python3.9/site-packages/stable_baselines3/common/distributions.py:298\u001b[0m, in \u001b[0;36mCategoricalDistribution.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bo-drl-env/lib/python3.9/site-packages/torch/distributions/categorical.py:118\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    116\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[1;32m    117\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[0;32m--> 118\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if continue_from_checkpoint:\n",
    "  init_experiment = last_bo_experiment\n",
    "  init_iteration = last_bo_iteration\n",
    "else:\n",
    "  init_experiment = 0\n",
    "  init_iteration = 1\n",
    "  bo_done = False\n",
    "\n",
    "if not bo_done:\n",
    "  for e in range(init_experiment, n_experiments):\n",
    "    print(f\"EXPERIMENT {e}\")\n",
    "    if not (continue_from_checkpoint and init_experiment == e):\n",
    "      # Sample initial hyperparameter values and evaluate the models obtained with them\n",
    "      init_x, init_y, best_init_y = generate_initial_data(upper_bounds[0],\n",
    "                                                          lower_bounds[0],\n",
    "                                                          1)\n",
    "\n",
    "    # We normalize the bounds of the hyperparameters as BOTorch assumes this\n",
    "    normalized_bounds = torch.tensor([[0.0], [1.0]])\n",
    "\n",
    "    # Normalize the hyperparameter as BOTorch assumes this\n",
    "    init_x_normalized = normalize(init_x,\n",
    "                                  bounds=bounds_tensor)\n",
    "\n",
    "    # Standardize the objective as BOTorch assumes this\n",
    "    init_y_standardized = standardize(init_y)\n",
    "\n",
    "    # Obtain the best result among the initial random experiments\n",
    "    best_init_y_standardized = init_y_standardized.max().item()\n",
    "\n",
    "    candidates=[]\n",
    "    results=[]\n",
    "\n",
    "    best_observed_result_bo = best_init_y\n",
    "    best_observed_candidate_bo = init_x[0][0].item()\n",
    "\n",
    "    if not (continue_from_checkpoint and init_experiment == e):\n",
    "      experiment_df = update_experiment_history(bo_method, \n",
    "                                                e, \n",
    "                                                0,\n",
    "                                                best_observed_candidate_bo, # The gamma selected for this iteration\n",
    "                                                best_observed_result_bo, # The reward lower bound of the model\n",
    "                                                best_observed_candidate_bo, # The best_gamma\n",
    "                                                best_observed_result_bo, # The reward lower bound\n",
    "                                                experiment_df,\n",
    "                                                experiment_name)\n",
    "      \n",
    "      experiment_configurations[rs_method,e,0] = best_observed_candidate_bo\n",
    "      experiment_results[rs_method,e,0] = best_observed_result_bo\n",
    "\n",
    "    for i in range(init_iteration, n_iterations+1):\n",
    "      # Get the next points given our actual queries\n",
    "      normalized_new_candidates = get_next_points_and_visualize_norm(init_x_normalized,\n",
    "                                                                    init_y_standardized, \n",
    "                                                                    best_init_y_standardized, \n",
    "                                                                    normalized_bounds,\n",
    "                                                                    i, \n",
    "                                                                    init_x,\n",
    "                                                                    init_y,\n",
    "                                                                    bounds_tensor,\n",
    "                                                                    1,\n",
    "                                                                    True)\n",
    "    \n",
    "      # Unnormalize the candidate hyperparameter value\n",
    "      new_candidates = unnormalize(normalized_new_candidates,\n",
    "                                  bounds=bounds_tensor)\n",
    "      \n",
    "      # Compute the performance of the model\n",
    "      new_results = torch.tensor([[target_function(float(new_candidates))]])\n",
    "\n",
    "      # Update our hyperparameters and rewards history\n",
    "      init_x = torch.cat([init_x, new_candidates])\n",
    "      init_y = torch.cat([init_y, new_results])\n",
    "\n",
    "      # Normalize our updated hyperparameters and rewards history\n",
    "      init_x_normalized = normalize(init_x, bounds=bounds_tensor)\n",
    "      init_y_standardized = standardize(init_y)\n",
    "\n",
    "      # Update the best reward\n",
    "      best_init_y = init_y.max().item()\n",
    "      best_init_y_standardized = init_y_standardized.max().item()\n",
    "      \n",
    "      # Show iteration info\n",
    "      \n",
    "      print(f\"Number of iteration: {i}\")\n",
    "      print(f\"  - Unnormalized gamma: {new_candidates.item()}\")\n",
    "      print(f\"  - Normalized gamma: {normalized_new_candidates.item()}\")\n",
    "      print(f\"  - Unstandardized lower reward bound: {new_results.item()}\")\n",
    "      print(f\"  - Standardized lower reward bound: {init_y_standardized[-1].item()}\")\n",
    "      print(f\"Best point performs this way: {best_init_y}\")\n",
    "      candidates.append(float(normalized_new_candidates[0][0]))\n",
    "      results.append(float(standardize(new_results[0][0])))\n",
    "\n",
    "      if best_observed_result_bo < new_results[0][0]:\n",
    "        best_observed_result_bo = new_results[0][0].item()\n",
    "        best_observed_candidate_bo = new_candidates[0][0].item()\n",
    "\n",
    "      experiment_df = update_experiment_history(bo_method, \n",
    "                                                e, \n",
    "                                                i,\n",
    "                                                new_candidates[0][0].item(), # The gamma selected for this iteration\n",
    "                                                new_results[0][0].item(), # The reward lower bound of the model\n",
    "                                                best_observed_candidate_bo, # The best_gamma\n",
    "                                                best_observed_result_bo, # The reward lower bound\n",
    "                                                experiment_df,\n",
    "                                                experiment_name)\n",
    "\n",
    "      experiment_configurations[bo_method,e,i] = best_observed_candidate_bo\n",
    "      experiment_results[bo_method,e,i] = best_observed_result_bo\n",
    "      print('----------------------')\n",
    "    init_iteration = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kbrnek5JW9f"
   },
   "source": [
    "Now we perform a random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8vpuRYDRJWmf",
    "outputId": "82f20218-361f-4f50-bd1d-234e0fdd516b"
   },
   "outputs": [],
   "source": [
    "if continue_from_checkpoint:\n",
    "  init_experiment = last_rs_experiment\n",
    "  init_iteration = last_rs_iteration\n",
    "else:\n",
    "  init_experiment = 0\n",
    "  init_iteration = 1\n",
    "\n",
    "for e in range(init_experiment, n_experiments):\n",
    "  if not (continue_from_checkpoint and init_experiment == e) or (init_experiment==0 and init_iteration==1):\n",
    "    # Initiate with a random value\n",
    "    random_value = np.random.random() * (upper_bounds[0] - lower_bounds[0]) + lower_bounds[0]\n",
    "    best_observed_result_rs = target_function(random_value)\n",
    "    best_observed_candidate_rs = random_value\n",
    "    # Update our experiments histories\n",
    "    experiment_df = update_experiment_history(rs_method, \n",
    "                                              e, \n",
    "                                              0,\n",
    "                                              random_value, # The gamma selected for this iteration\n",
    "                                              best_observed_result_rs, # The reward lower bound of the model\n",
    "                                              best_observed_candidate_rs, # The best_gamma\n",
    "                                              best_observed_result_rs, # The reward lower bound\n",
    "                                              experiment_df,\n",
    "                                              experiment_name)\n",
    "  \n",
    "  # Iterate with random search\n",
    "  for i in range(init_iteration, n_iterations+1):\n",
    "    # Get a new random value for the hyperparameter\n",
    "    random_value = np.random.random() * (upper_bounds[0] - lower_bounds[0]) + lower_bounds[0]\n",
    "    # Evaluate the model with that hyperparameter value\n",
    "    rs_obj_fun_result = target_function(random_value)\n",
    "\n",
    "    # Update best reward and candidate found if necessary\n",
    "    if best_observed_result_rs < rs_obj_fun_result:\n",
    "      best_observed_result_rs = rs_obj_fun_result\n",
    "      best_observed_candidate_rs = random_value\n",
    "    \n",
    "    # Update our experiments histories\n",
    "    experiment_df = update_experiment_history(rs_method, \n",
    "                                              e, \n",
    "                                              i,\n",
    "                                              random_value, # The gamma selected for this iteration\n",
    "                                              rs_obj_fun_result, # The reward lower bound of the model\n",
    "                                              best_observed_candidate_rs, # The best_gamma\n",
    "                                              best_observed_result_rs, # The reward lower bound\n",
    "                                              experiment_df,\n",
    "                                              experiment_name)\n",
    "    experiment_configurations[rs_method,e,i] = best_observed_candidate_rs\n",
    "    experiment_results[rs_method,e,i] = best_observed_result_rs\n",
    "\n",
    "  init_iteration = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukMdAAapSR2c"
   },
   "source": [
    "# Step 7: Compare the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeN8EWoOSVxC"
   },
   "source": [
    "First we give the recommendation as the best observed result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nlf2cyT-SZf2",
    "outputId": "751e185a-ef94-43d1-fb97-02f5a5717e94"
   },
   "outputs": [],
   "source": [
    "best_observed_result = np.max(experiment_results)\n",
    "index_set = np.where(experiment_results==best_observed_result)\n",
    "print(\"The best observed result is: \" + str(best_observed_result))\n",
    "print(\"The best observed result belong to the : \" + str(index_set[0][0]) + \" method. Its value is \" + str(experiment_configurations[index_set][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jZ5oFK6Sf3Q"
   },
   "source": [
    "And now we plot the results to compare both methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "lgo_wr7VSj7k",
    "outputId": "5c4e2d3a-31b3-4217-d9b4-f9b6ac80aa8d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(1, n_iterations, n_iterations).astype(int)\n",
    "mean_bo = np.mean(experiment_results[0,:,:], axis=0)\n",
    "mean_rs = np.mean(experiment_results[1,:,:], axis=0)\n",
    "std_bo = np.std(experiment_results[0,:,:], axis=0) * 0.25\n",
    "std_rs = np.std(experiment_results[1,:,:], axis=0) * 0.25\n",
    "bo_ub_results = go.Scatter(x=x, y=mean_bo + std_bo, mode='lines', name=\"\", line_color=\"green\", line_width=0.1)\n",
    "bo_results = go.Scatter(x=x, y=mean_bo, mode='lines', fill='tonexty', line_color=\"green\", name=\"Bayesian Optimization\")\n",
    "bo_lb_results = go.Scatter(x=x, y=mean_bo - std_bo, mode='lines', fill='tonexty', name=\"\", line_color=\"green\", line_width=0.1)\n",
    "\n",
    "rs_ub_results = go.Scatter(x=x, y=mean_rs + std_rs, mode='lines', name=\"\", line_color=\"red\", line_width=0.1)\n",
    "rs_results = go.Scatter(x=x, y=mean_rs, mode='lines', fill='tonexty', line_color=\"red\", name=\"Random Search\")\n",
    "rs_lb_results = go.Scatter(x=x, y=mean_rs - std_rs, mode='lines', fill='tonexty', name=\"\", line_color=\"red\", line_width=0.1)\n",
    "  \n",
    "fig = go.Figure()\n",
    "fig.add_trace(bo_ub_results)\n",
    "fig.add_trace(bo_results)\n",
    "fig.add_trace(bo_lb_results)\n",
    "fig.add_trace(rs_ub_results)\n",
    "fig.add_trace(rs_results)\n",
    "fig.add_trace(rs_lb_results)\n",
    "fig.update_layout(title=\"Performance comparison between BO and RS\", xaxis_title=\"Iterations\", yaxis_title=\"Reward lower bound\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "bo-drl-env",
   "language": "python",
   "name": "bo-drl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
